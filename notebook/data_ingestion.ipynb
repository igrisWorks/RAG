{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8457d57f",
   "metadata": {},
   "source": [
    "### RAG pipeline - data ingestion and Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24434c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import  TextLoader,DirectoryLoader, PyMuPDFLoader, PyPDFLoader\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d378ffb",
   "metadata": {},
   "source": [
    "### read all the pdf's in the directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1eace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_all_pdfs(pdf_directory):\n",
    "\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files in {pdf_directory}\"\n",
    "        )\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "    ### adding the source information for the metadata\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "\n",
    "                all_documents.extend(documents)\n",
    "                print(f\"Loaded{len(documents)} pages\")\n",
    "        except Exception as e:\n",
    "            print(\" Error:{e}\")\n",
    "    \n",
    "    print(f\"Total docuents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "  \n",
    "      \n",
    "### process all the pdf in data directory\n",
    "all_pdf_files = read_all_pdfs(\"../data/pdf\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "427baf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Matrix Profile I: All Pairs Similarity Joins for Time Series: \\nA Unifying View that Includes Motifs, Discords and Shapelets \\nChin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,  \\nHoang Anh Dau, ‚Ä†Diego Furtado Silva, ‚Ä°Abdullah Mueen, and Eamonn Keogh \\nUniversity of California, Riverside, ‚Ä†Universidade de S√£o Paulo, ‚Ä°University of New Mexico \\n{myeh003, yzhu015, lulan001, nbegu001, yding007, hdau001}@ucr.edu, diegofsilva@icmc.usp.br, mueen@unm.edu, eamonn@cs.ucr.edu \\n \\nAbstract‚Äî The all-pairs-similarity-search (or similarity join) \\nproblem has been extensively studied for text and a handful of \\nother datatypes. However, surprisingly little progress has been  \\nmade on similarity joins for time series subsequences. The lack of  \\nprogress probably stems from the daunting nature of the \\nproblem. For even modest sized data sets the obvious nested -loop \\nalgorithm can take months, and the typical speed -up techniques \\nin this domain (i.e., indexing, lower -bounding, triangular -\\ninequality pruning and early abandoning) at best produce one or \\ntwo orders of magnitude speedup. In this work we introduce a \\nnovel scalable algorithm for time series subsequence all -pairs-\\nsimilarity-search. For exceptionally large datasets, the algorithm \\ncan be trivially cast as an anytime algorithm and produce high -\\nquality approximate solutions in reasonable  time. The exact \\nsimilarity join algorithm computes the answer to the time series \\nmotif and time series discord  problem as a side -effect, and our \\nalgorithm incidentally provides the fastest known algorithm for \\nboth these  extensively-studied problems. We de monstrate the \\nutility of our ideas for many time series data mining problems, \\nincluding motif discovery, novelty discovery,  shapelet discovery,  \\nsemantic segmentation, density estimation, and contrast set \\nmining.    \\nKeywords‚ÄîTime Series; Similarity Joins; Motif Discovery \\nI. INTRODUCTION \\nThe all-pairs-similarity-search (also known as similarity \\njoin) problem comes in several variants. The basic task is this: \\nGiven a collection of data objects, retrieve the nearest neighbor \\nfor each object . In the text domain the  algorithm has \\napplications in a host of problems, including community \\ndiscovery, duplicate detection, collaborative filtering, \\nclustering, and query refinement [1]. While virtually all text \\nprocessing algorithms have analogues in time series data \\nmining, there has been surprisingly little progress on Time \\nSeries subsequences All-Pairs-Similarity-Search (TSAPSS). \\nWe believe that this lack of progress stems not from a lack \\nof interest in this useful primitive, but from the daunting nature \\nof the problem. Consider the following example that reflects the \\nneeds of an industrial collaborator. A boiler at a che mical \\nrefinery reports pressure once a minute. After a year, we have a \\ntime series of length 525,600. A plant manager may wish to do \\na similarity self-join on this data with week -long subsequences \\n(10,080) to discover operating regimes (summer vs. winter o r \\nlight distillate vs. heavy distillate etc.) The obvious nested loop \\nalgorithm requires 132,880,692,960 Euclidean distance \\ncomputations. If we assume each one takes 0.0001 seconds, \\nthen the join will take 153.8 days. The core contribution of this \\nwork is to show that we can reduce this time to 6.3 hours, using \\nan off-the-shelf desktop computer. Moreover, we show that this \\njoin can be computed and/or updated incrementally. Thus we \\ncould maintain this join essentially forever on a standard \\ndesktop, even if the data arrival frequency was much faster than \\nonce a minute. \\nOur algorithm uses an ultra -fast similarity search algorithm \\nunder z -normalized Euclidean distance as a subroutine, \\nexploiting the overlap between subsequences using the classic \\nFast Fourier Transform (FFT) algorithm.  \\nOur method has the following advantages/features: \\n\\uf0b7 It is exact, providing no false positives or false dismissals. \\n\\uf0b7 It is simple and parameter -free. In contrast, the more \\ngeneral metric space APSS algorithms require building and \\ntuning spatial access methods and/or hash functions.  \\n\\uf0b7 Our algorithm requires an inconsequential space overhead, \\njust O(n) with a small constant factor. \\n\\uf0b7 While our exact algorithm is extremely scalable, for \\nextremely large datasets we can compute the results in an \\nanytime fashion, allowing ultra-fast approximate solutions.  \\n\\uf0b7 Having computed the similarity join for a dataset, we can \\nincrementally upd ate it very efficiently. In many domains \\nthis means we can effectively maintain exact joins on \\nstreaming data forever.  \\n\\uf0b7 Our method provides full joins, eliminating the need to \\nspecify a similarity threshold, which as we will show, is a \\nnear impossible task in this domain.  \\n\\uf0b7 Our algorithm is embarrassingly parallelizable, both on \\nmulticore processors and in distributed systems. \\nGiven all these features, our algorithm has implications for \\nmany time series data mining tasks [5][18][28].  \\nThe rest of the paper is organized as follows. Section II \\nreviews related work an d introduces the necessary background \\nmaterials and definitions. In Section III we introduce our \\nalgorithm and its anytime and incremental variants. Section IV \\nsees a detailed empirical evaluation of our algorithm and shows \\nits implications for many data mining tasks. Finally, in Section \\nV we offer conclusions and directions for future work. \\nII. RELATED WORK AND BACKGROUND \\nThe basic variant of  similarity join problem we are \\ninterested in is as follows : Given a collection of data objects, \\nretrieve the nearest neighbor for every object.   \\nOther common variants include retrieving the top-K nearest \\nneighbors or the nearest neighbor for each object if that \\nneighbor is within a user-supplied threshold, œÑ. (Such variations \\nare trivial generalizations of our proposed algorithm, so we \\nomit them from further discussion). The latter variant results in \\na much easier problem, provided that the threshold is small. For \\nexample, [1] notes that virtually all research efforts ‚Äú exploit a \\nsimilarity threshold more aggressively in order to limit the set'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='of candidate pairs that are considered..  [or] ... to reduce the \\namount of information indexed in the first place.‚Äù \\nThis critical dependence on œÑ is a major issue for text joins, \\nas it is known that ‚Äú join size can change dramatically \\ndepending on the input similarity threshold ‚Äù [10]. However, \\nthis issue is even more critical for time series for two reasons. \\nFirst, unlike similarity (which is bounded between zero and \\none), the Euclidean distance is effectively unbounded, and \\ngenerally not intuitive. For example, if two heartbeats have a \\nEuclidean distance of 17.1, are they similar? Even for a domain \\nexpert that knows the sampling rate and the noise level of the \\ndata, this is not obvious. Second, a single threshold can produce \\nradically different output sizes, even for datasets that are very \\nsimilar.  Consider Figure 1 which shows the output size vs. \\nthreshold setting for the first and s econd halves of a ten -day \\nperiod monitoring data center chillers [21]. For the first five \\ndays a threshold of 0.6 would return zero items, but for the \\nsecond five days the same setting would return 108 items.  This \\nshows the difficulty in selecting  an appropriate threshold. Our \\nsolution is to have no threshold, and do a full join. \\n \\nFigure 1.  Output size vs. threshold for data center chillers [21]. Values beyond \\n2.0 are truncated for clarity (but archived at [24]).  \\nA handful of efforts have considered joins on time series, \\nachieving speedup by (in addition to the use of MapReduce) \\nconverting the data to lower -dimensional representations such \\nas PAA [11] or SAX [12] and exploiting lower bounds and/o r \\nLocality Sensitive Hashing (LSH) to prune some calculations. \\nHowever, the methods are very complex, with many (10 -plus) \\nparameters to adjust. As [11] acknowledges with admirable \\ncandor, ‚ÄúReasoning about the optimal settings is not trivial.‚Äù In \\ncontrast, our proposed algorithm has zero parameters to set. \\nA very recent research effort [28] has tackled the scalability \\nissue by converting the real -valued time series into discrete \\n‚Äúfingerprints‚Äù before using a LSH approach, much like the text \\nretrieval community [1]. They produced impressive speedup, \\nbut they also experienced false negatives. Moreover, the \\napproach has several parameters that need to be set; for \\nexample, they need to set the threshold to a very precise 0.818.  \\nAs we shall show, our algorithm allows both anytime and \\nincremental (i.e. streaming) versions. While a streaming join \\nalgorithm for text was recently introduced [15], we are not \\naware of any such algorithms for time series data or general \\nmetric spaces. More generally, there is a large amount of \\nliterature on joins for text processing [1]. Such work is \\ninteresting, but of little utility given our constraints, data type \\nand problem setting. We require full joins, not threshold joins, \\nand we are unwilling to allow the possibility of false negatives. \\nA. Definitions and Notation \\nWe begin by defining the data type of interest, time series: \\nDefinition 1:  A time series T is a sequence of real -valued \\nnumbers ti: T = t1, t2, ..., tn where n is the length of T. \\nWe are not interested in the global properties of time series, \\nbut in the similarity between local subsequences:  \\nDefinition 2:  A subsequence Ti,m of a T is a continuous \\nsubset of the values from T of length m starting from \\nposition i.   Ti,m = ti, ti+1,‚Ä¶, ti+m-1, where 1 ‚â§  i ‚â§  n-m+1.  \\nWe can take any subsequence from a time series and \\ncompute its distance to all sequences. We call an ordered vector \\nof such distances a distance profile: \\nDefinition 3:  A distance profile D is a vector of the \\nEuclidean distances between a given query and each \\nsubsequence in an all-subsequences set (see Definition 4).  \\nNote that we are assuming that the distance is measured \\nusing the Euclidean distance between the z -normalized \\nsubsequences [8]. The distance profile can be considered a meta \\ntime series that annotates the time series T that was used to \\ngenerate it. The first three definitions are illustrated in Figure 2. \\n \\nFigure 2. A subsequence Q extracted from a time series T is used as a query to \\nevery subsequence in T. The vector of all distances is a distance profile. \\nNote that if the query and all-subsequences set belong to the \\nsame time series, the distance profile must be zero at the \\nlocation of the query, and close to zero just before and just \\nafter. Such matches are c alled trivial matches in the literature \\n[18], and are avoided by ignoring an exclusion zone (shown as \\na gray region) of m/2 before and after the location of the query. \\nWe are interested in similarity join of all subsequences of a \\ngiven time series. We define an all-subsequences set of a given \\ntime series as a set that contains all possible subsequences from \\nthe time series. The notion of all-subsequences set is purely for \\nnotational purposes. In our implementation, we do not actually \\nextract the subsequences in this form as it would require \\nsignificant time and space overhead. \\nDefinition 4: An all-subsequences set A of a time series T \\nis an ordered set of all possible subsequences of T obtained \\nby sliding a window of length m across T: A ={T1,m,, \\nT2,m,‚Ä¶, Tn-m+1,m}, where m is a user -defined subsequence \\nlength. We use A[i] to denote Ti,m. \\nWe are interested in the nearest neighbor (i.e., 1NN) relation \\nbetween subsequences; therefore, we define a 1NN-join \\nfunction which indicates the nearest neighbor relation between \\nthe two input subsequences. \\nDefinition 5:  1NN-join function :  given two all -\\nsubsequences sets A and B and two subsequences A[i] and \\nB[j], a 1NN-join function  Œ∏1nn (A[i], B[j]) is a Boolean \\nfunction which returns ‚Äútrue‚Äù only if B[j] is the nearest \\nneighbor of A[i] in the set B. \\nWith the defined join function, a similarity join set  can be \\ngenerated by applying the similarity join operator on two input \\nall-subsequences sets. \\nDefinition 6: Similarity join set : given all -subsequences \\nsets A and B, a similarity join set  JAB of A and B is a set \\ncontaining pairs of each subsequence in A with its nearest \\nneighbor in B: JAB={‚å© A[i], B[j] ‚å™ |Œ∏1nn (A[i], B[j])}.  We \\ndenote this formally as JAB = A‚ãà\\uf0711nnB. \\n0\\n400\\n800\\n1200\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\\nFirst 5 Days\\nSecond 5 Days\\nData Center Chillers\\nT, a snippet of an energy \\nconsumption\\n2,0000 m/2m/2\\nQ, query of length m\\nNote that |D| = |T|-|Q|+1D, a distance profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='We measure the Euclidean distance between each pair \\nwithin a similarity join set and store the resultants into an \\nordered vector. We call the result vector the matrix profile. \\nDefinition 7:  A matrix profile (or just profile) PAB is a \\nvector of the Euclidean distances between each pair in JAB. \\nWe call this vector the matrix profile because one \\n(inefficient) way to compute it would be to compute the full \\ndistance matrix of all the subsequences in one time series with \\nall the subsequence in another time series and extract the \\nsmallest value in each row (the smallest non-diagonal value for \\nthe self-join case).  In Figure 3 we show the matrix profile of \\nour running example. \\n \\nFigure 3. A time series T, and its self-join matrix profile P.  \\nLike the distance profile, the matrix profile can be \\nconsidered a meta time series annotating the time series T if the \\nmatrix profile is generated by joining T with itself. The profile \\nhas a host of interesting and exploitable properties. For \\nexample, the highest point on the profile corresponds to the \\ntime series discord [5], th e ( tied) lowest points correspond to \\nthe locations of the best time series motif pair [18], and the \\nvariance can be seen as a measure of the T‚Äôs com plexity. \\nMoreover, the histogram of the values in the matrix profile is \\nthe exact answer to the time series density estimation [4]. \\nWe name this special case of the similarity join set \\n(Definition 6) as self-similarity join set, and the corresponding \\nprofile as self-similarity join profile. \\nDefinition 8:  A self-similarity join  set JAA is a result of \\nsimilarity join of the set  A with itself . We denote this \\nformally as JAA = A ‚ãà\\uf0711nn A. We denote the corresponding \\nmatrix profile or self-similarity join profile as PAA. \\nNote that we exclude trivial matches when self -similarity \\njoin is performed, i.e., if A[i] and A[j] are subsequences from \\nthe same all -subsequences set A, Œ∏1nn (A[i], B[j]) is ‚Äúfalse‚Äù \\nwhen A[i] and A[j] are a trivially matched pair. \\nThe ith element in the matrix profile tells us the Euclidean \\ndistance to the nearest neighbor of the subsequence of T, \\nstarting at i.  However, it does not tell us where that neighbor is \\nlocated. This information is recorded in matrix profile index. \\nDefinition 9: A matrix profile index IAB of a similarity join \\nset JAB is a vector of integers where IAB[i] = j if {A[i], B[j]} \\n‚àà JAB. \\nBy storing the neighboring information this way, we can \\nefficiently retrieve the nearest neighbor of A[i] by accessing the \\nith element in the matrix profile index. \\nNote that the function which computes the similarity join set \\nof two input time series  is not symmetric; therefore, JAB ‚â† JBA,  \\nPAB ‚â† PBA, and IAB ‚â† IBA. \\nFor ease of presentation, we have confined this work to the \\nsingle dimensional case; however, nothing intrinsically \\nprecludes generalizations to multidimensional data. \\nSummary of the Previous Section \\nThe previous section was rather dense, so before moving on \\nwe summarize the main takeaway points. We can create two \\nmeta time series, the matrix profile and the matrix profile index, \\nto annotate a time series TA with the distance and location of all \\nits subsequences nearest neighbors in itself or another time \\nseries TB. These two data objects explicitly or implicitly contain \\nthe answers to many time series data mining tasks. However, \\nthey appear to be too expensive to compute to be practica l. In \\nthe next section we will show an algorithm that can compute \\nthese efficiently. \\nIII. ALGORITHMS \\nWe are finally in a position to explain our algorithm s. We \\nbegin by stating the fundamental intuition, which stems from \\nthe relationship between distance profiles and the matrix \\nprofile. As Figure 2 and Figure 3 visually suggest, all distance \\nprofiles (excluding the trivial match region) are upper bound \\napproximations to the matrix profile. More critically, if we \\ncompute all the distance profiles, an d take the minimum value \\nat each location, the result is the matrix profile! \\nThis tells us that if we have a fast way to compute the \\ndistance profiles, then we also have a fast way to compute the \\nmatrix profile. As we shall show in the next section, we have an \\nultra-fast way to compute the distance profiles. \\nA. Mueen‚Äôs ultra-fast Algorithm for Similarity Search (MASS) \\nWe begin by introducing a novel Euclidean distance \\nsimilarity search algorithm for time series data. The algorithm \\ndoes not just find the nearest neighbor to a query and return its \\ndistance; it returns the distance to every subsequence. In \\nparticular, it computes the distance profile, as shown in Figure \\n2. The algorithm requires just O( nlogn) time by exploiting the \\nFFT to calculate  the dot products between the query and all \\nsubsequences of the time series.  \\nWe need to carefully qualify the claim of ‚Äúultra-fast‚Äù. There \\nare dozens of algorithms for time series similarity search that \\nutilize index structures to efficiently locate neighbors [8]. While \\nsuch algorithms can be faster in the  best case , all these \\nalgorithms degenerate to brute force search in the worst case 1 \\n(actually, much worse than brute force search due to the \\noverhead of the index). Likewise, there are index -free methods \\nthat achieve speed -up using various early abandoning tricks \\n[22], but they too degrade to brute force search in the worst \\ncase. In contrast, the performance of the algorithms outlined in \\nTABLE I and TABLE II is completely independent of the data. \\nTABLE I. CALCULATION OF SLIDING DOT PRODUCTS \\nProcedure SlidingDotProduct(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: The dot product between Q and all subsequences in T \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nn ‚Üê Length(T), m ‚Üê Length(Q) \\nTa ‚Üê Append T with n zeros   \\nQr ‚Üê Reverse(Q)  \\nQra ‚Üê Append Qr with 2n-m zeros \\nQraf ‚Üê FFT(Qra), Taf ‚Üê FFT(Ta) \\nQT ‚Üê InverseFFT(ElementwiseMultiplication(Qraf, Taf)) \\nreturn QT \\n \\n1 There are many such worse case scenarios, including high levels of noise blurring \\nthe distinction between closest and furthest neighbors, and rendering triangular -\\ninequality pruning and early abandoning worthless. \\n2,0000\\nP, a matrix profile\\nT, a snippet of an energy \\nconsumption\\nNote that |P| = |T|-|Q|+1'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Line 1 determines the length of both the time series T and \\nthe query Q. In line 2, we use that information to append T with \\nan equal number of zeros. In line 3, we obtain the mirror image \\nof the original query. This reversing ensures that a convolution \\n(i.e. ‚Äúcrisscrossed‚Äù multiplication) essentially produces in-order \\nalignment. Because we require both vectors to be the s ame \\nlength, in line 4 we append enough zeros to the (now reversed) \\nquery so that, like Ta, it is also of length 2 n. In line 5, the \\nalgorithm calculates Fourier transforms of the appended -\\nreversed query (Qra) and the appended time series Ta. Note that \\nwe use FFT algorithm which is an O(nlogn) algorithm. The Qraf \\nand the Taf produced in line 5 are vectors of complex numbers \\nrepresenting frequency components of the two time series. The \\nalgorithm calculates the element-wise multiplication of the two \\ncomplex vectors and performs inverse FFT on the product. \\nLines 5-6 are the class ic convolution operation on two vectors \\n[7]. Figure 4 shows a toy example of the sliding dot product \\nfunction in work. The algorithm time complexity does not \\ndepend on the length of the query (m). \\n \\nFigure 4. A toy example of convolution operation being used to calculate  \\nsliding dot products for time series data. Note the reverse and append \\noperation on T and q in the input. Fifteen dot products are calculated for every \\nslide. The cells m = 2 to n = 4 from left ( red/bold arrows) contain valid \\nproducts. TABLE II takes this subroutine and uses it to create a distance \\nprofile (see Definition 3). \\nIn line 1 of TABLE II, we invoke the dot products code \\noutlined in TABLE I. The formula to calculate the z-normalized \\nEuclidean distance D[i] between two time series subsequence Q \\nand Ti,m using their dot product, QT[i] is (see [24] for \\nderivation): \\nùê∑[ùëñ] = ‚àö2ùëö(1‚àíùëÑùëá[ùëñ]‚àíùëöùúáùëÑùëÄùëá[ùëñ]\\nùëöùúéùëÑùõ¥ùëá[ùëñ] ) \\nwhere m is the subsequence length, ŒºQ is the mean of Q, \\nMT[i] is the mean of Ti,m, œÉQ is the standard deviation of Q, and  \\nŒ£T[i] is the standard deviation of Ti,m. Normally, it takes O( m) \\ntime to calculate the mean and standard deviation for every \\nsubsequence of a long time series. However, here we exploit a \\ntechnique noted in [22] in a different context. We cache \\ncumulative sums of the values and square of the values in the \\ntime series. At any stage the two cumulative sum vectors are \\nsufficient to calculate the mean an d the standard deviation of \\nany subsequence of any length.  See [14] for an elaborate \\ndescription and variations of MASS. \\nTABLE II. MUEEN‚ÄôS ALGORITHM FOR SIMILARITY SEARCH (MASS) \\nProcedure MASS(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: A distance profile D of the query Q  \\n1 \\n2 \\n3 \\n4 \\nQT ‚Üê SlidingDotProducts(Q, T) \\nŒºQ, œÉQ, ŒúT, Œ£T  ‚Üê ComputeMeanStd(Q, T)    // see [22] \\nD ‚Üê CalculateDistanceProfile(Q, T, QT, ŒºQ, œÉQ, ŒúT, Œ£T) \\nreturn D \\nUnlike the dozens of time series KNN search algorithms in \\nthe literature [8], this algorithm calculates the distance to every \\nsubsequence, i.e. the distance profile  of time series T. \\nAlternatively, in join nomenclature, the algorithm produces one \\nfull row of the all -pair similarity matrix. Thus, as we show in \\nthe next section, our join algorithm is simply a loop that \\ncomputes each full row of the all -pair similarity matrix and \\nupdates the current ‚Äúbest-so-far‚Äù matrix profile when needed. \\nB. The STAMP Algorithm \\nWe call our join algorithm STAMP, Scalable Time series \\nAnytime Matrix Profile. The algorithm is outlined in TABLE \\nIII. In line 1, we extract the length of TB. In line 2, we allocate \\nmemory and initial matrix profile PAB and matrix profile index \\nIAB. From lines 3 to line 6, we calculate the distance profiles D \\nusing each subsequence B[idx] in the time series TB and the \\ntime series TA. Then, we perform pairwise minimum for each \\nelement in D with the paired element in PAB (i.e., min( D[i], \\nPAB[i]) for i = 0 to length(D) - 1.) We also update IAB[i] with idx \\nwhen D[i] ‚â§ PAB[i] as we perform the  pairwise minimum \\noperation. Finally, we return the result PAB and IAB in line 7.  \\nNote that the algorithm presented in TABLE III computes \\nthe matrix profile for the general similarity join. To modify the \\ncurrent algorithm to compute  the self -similarity join matrix \\nprofile of a time series TA, we simply replace TB in line 1 with \\nTA, replace B with A in line 4, and ignore trivial match in D \\nwhen performing ElementWiseMin in line 5. \\nTABLE III. THE STAMP ALGORITHM \\nProcedure STAMP(TA, TB, m) \\nInput: Two user provided time series, TA and TB, interested \\nsubsequence length m \\nOutput: A matrix profile PAB and associated matrix profile index IAB of \\nTA join TB, JAB = A‚ãà\\uf0711nnB \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nnB ‚Üê Length(TB) \\nPAB ‚Üê infs, IAB ‚Üê zeros, idxes ‚Üê 1:nB-m+1 \\nfor each idx in idxes    // In any order, but random for anytime algorithm \\n          D ‚Üê MASS(B[idx], TA) \\n          PAB, IAB ‚Üê ElementWiseMin(PAB, IAB, D, idx) \\nend for \\nreturn PAB, IAB \\nTo parallelize the STAMP algorithm for multicore \\nmachines, we simply distribute the indexes to secondary \\nprocess run in each core, and the secondary processes use the \\nindexes they received to update their own PAB and IAB. Once the \\nmain process returns from  all secondary processes, we use \\nElementWiseMin to merge the received PAB and IAB.  \\nC. An Anytime Algorithm for TSAPSS \\nWhile the exact algorithm introduced in the previous section \\nis extremely scalable, there will always be datasets for which \\ntime needed for an exact solution is untenable. We can mitigate \\nthis by computing the results in an anytime fashion, allowing \\nfast approximate solutions [30]. To a dd the anytime nature to \\nthe STAMP algorithm, we simply ensure a randomized search \\norder in line 2 of TABLE III.  \\nWe can compute a (post-hoc) measurement of the quality of \\nan anytime solution by measuring the Root -Mean-Squared-\\nError (RMSE) between the true matrix profile and the current \\nbest-so-far mat rix profile. As Figure 5 suggests, with an \\nexperiment on random walk data, the algorithm converges very \\nquickly. \\nQ2T1 0 00 00 00 00 0Q1T4Q2T2+Q1T1 Q2T3+Q1T2 Q2T4+Q1T3\\nOutput\\nInputT2T1 T4T3 00 00 Q1Q2 00 00 00'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Figure 5. main) The decre ase in RMSE as the STAMP algorithm updates \\nmatrix profile with the distance profile calculated at each iteration.  inset) The \\napproximate matrix profile at the 10% mark is visually indistinguishable from \\nthe final matrix profile. \\nZilberstein [30] gives a number of desirable properties of \\nanytime algorithms, including Low Overhead , Interruptibility, \\nMonotonicity, Recognizable Quality, Diminishing Returns and \\nPreemptability (these properties are mostly obvious from their \\nnames, but full definitions are at [30]). \\nBecause each subsequence‚Äôs distance profile is bounded \\nbelow by the exact matrix profile, updating an approximate \\nmatrix profile with a distance profile with pairwise minimum \\noperation either drives the approximate solution closer the exact \\nsolution or retains the current approximate solution. Thus , we \\nhave guaranteed Monotonicity. From Figure 5, the approximate \\nmatrix profile converges to the exact matrix profile \\nsuperlinearly; therefore, we have strong Diminishing Returns. \\nWe can easily achieve Interruptibility and Preemptability by \\nsimply inserting a few lines of code between lines 5 and 6 of \\nTABLE III that read: \\n5new \\n6new \\n7new \\nif CheckForUserInterrupt  = TRUE \\n          Report({PAB, IAB}, ‚ÄòHere is an approximate answer.‚Äô) \\nif GetUserChoice = ‚Äòfurther refine‚Äô, CONTINUE, else BREAK \\nThe space and time overhead for the anytime property is \\neffectively zero, thus we have Low Overhead. This leaves only \\nthe property of Recognizable Quality. Here we must resort to a \\nprobabilistic argument.  The convergence curve shown in  \\nFigure 5 is very typical, so we could use past convergence \\ncurves to predict the quality of solution when interrupted on \\nsimilar data.  \\nD. Time and Space Complexity \\nThe overall complexity of the proposed algorithm is \\nO(n2logn) where n is the length of the time series. However, our \\nexperiments (see Section 4.1) empirically suggest that the \\nruntime of  STAMP‚Äôs growth rate is roughly O( n2) instead of \\nO(n2logn). One possible e xplanation for this is that the nlogn \\nfactor comes from the FFT subroutine. Because FFT is so \\nimportant in many applications, it is extraordinarily well \\noptimized. Thus, the empirical runtime is very close to linear. \\nIn contrast to the above, the brute for ce nested loop \\napproach has a time complexity of O(n2m). Recall the industrial \\nexample in the introduction section. We have  m = 10,080, but \\nlog(n) = 5.7, so we would expect our approach to be about \\n1,768 times faster. In fact, we are empirically even faster. The \\ncomplexity analysis downplays the details of important constant \\nfactors. The nested loop algorithm must also z -normalize the \\nsubsequences. This either requires O( nm) time, but with an \\nuntenable O(nm) space overhead, or an O( n2m) time overhead. \\nAnd r ecall that this is before a single Euclidean distance \\ncalculation is performed.  \\nFinally, we mention one quirk of our algorithm which we \\ninherit from using the highly optimized FFT subroutine. Our \\nalgorithm is fastest when n is an integer power of two, slower \\nfor non -power of two but composite numbers, and slowest \\nwhen n is prime. The difference (for otherwise similar values of \\nn) can approach a factor of 1.6x. Thus, where possible, it is \\nworth contriving the best case by tru ncation or zero-padding to \\nthe nearest power of two. \\nE. Incrementally Maintaining TSAPSS \\nUp to this point we have discussed the batch version of \\nTSAPSS. By batch, we mean that the STAMP algorithm needs \\nto see the entire time series TA and TB (or just TA if we are \\ncalculating the self -similarity join matrix profile) before \\ncreating the matrix profile. However, it would be advantageous \\nif we could build the matrix profile incrementally. Given that \\nwe have performed a batch construction of matrix profile, i f \\nnew data arrives, it would clearly be preferable to incrementally \\nadjust the current profile, rather than start from scratch.  \\nBecause the matrix profile solves both the times series motif \\nand the time series discord problems, an incremental version of \\nSTAMP would automatically provide the first incremental \\nversions of both algorithms. We call such an algorithm the \\nSTAMPI (STAMP Incremental) algorithm. \\nWe will demonstrate our ability to incrementally maintain \\nthe matrix profile in this section. For simpli city and brevity \\nTABLE IV only shows the algorithm to maintain the self -\\nsimilarity join. The generalizations are obvious. \\nTABLE IV. THE STAMPI ALGORITHM \\nProcedure STAMPI(TA, t, PAA, IAA) \\nInput: The original time series TA, a new data point t following TA, the \\nmatrix profile PAA and its associated matrix profile index IAA of TA.  \\nOutput: The incrementally updated matrix profile PAA,new and its matrix \\nprofile index IAA,new of the current time series TA,new= TA, t. \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nTA,new = [TA, t] \\nS ‚Üê last subsequence in TA,new, idx ‚Üê index of S in TA,new \\nD ‚Üê MASS (S, TA) \\nPAA, IAA ‚Üê ElementWiseMin(PAA, IAA, D, idx) \\npAA,last, iAA,last ‚Üê FindMin(D) \\nPAA,new ‚Üê [PAA, pAA,last], IAA,new ‚Üê [IAA, iAA,last] \\nreturn PAA,new, IAA,new \\nFor clarity, we denote the updated time series as TA,new, the \\nupdated matrix profile as PAA,new and the associated matrix \\nprofile index as IAA,new. As each additional data point t arrives, \\nthe size of the time series TA increases by one, and a new \\nsubsequence S is generated at the end of TA,new. In line 3 we \\nobtain the distance profile of S with regard to TA. Then, as in the \\noriginal STAMP algorithm,  in line 4 we perform  a pairwise \\ncomparison between every element in D with the corresponding \\nelement in PAA to see if the corresponding element in PAA needs \\nto be updated. In line 5, we find the nearest neighbor of S and \\nthe associated index by evaluating the minimum value of D. \\nFinally, in line 6, we obtain the new matrix profile and \\nassociated matrix profile index by concatenating the results in \\nline 4 and line 5.  \\nThe time complexity of the STAMP I algorithm is O(nlogn) \\nwhere n is the length of size of th e current time series TA. Note \\nthat as we maintain the profile, each incremental call of \\nSTAMPI requires invoking the FFT subroutine with a slightly \\nlonger time series ( n becomes n+1 ). Thus it gets very slightly \\nslower at each time step. Therefore, the best way to measure the \\nperformance is to ask for the Maximum Time Horizon (MTH), \\nin essence the answer to this question: ‚Äú Given this arrival rate, \\nhow long can we maintain the profile before we can no longer \\nupdate fast enough?‚Äù \\nNote that the subsequence length m is not considered in the \\nMTH evaluation. Recall that overall time complexity of the \\n10,0000\\nRMSE\\niteration1,000\\napproximate matrix \\nprofile at 1,000 iterations.\\nexact matrix profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='algorithm is determined by the efficiency of the FFT \\nsubroutine, which is independent of m. We have computed the \\nMTH for two common scenarios of interest to the community. \\n\\uf0b7 House Electrical Demand  [19]: This dataset is updated \\nevery eight seconds. By iteratively calling the STAMP I \\nalgorithm, we can maintain the profile for 5.8 years.  \\n\\uf0b7 Oil Refinery :  Most telemetry in oil refineries is sampled \\nat once a minute [26]. The relatively low sampling rate \\nreflects the ‚Äúinertia‚Äù of massive boilers/condensers. Even if \\nwe maintain the profile for 40 years, the update time is only \\naround 6.78 seconds. Moreover, th e raw data, matrix \\nprofile and index would only require 0.5 gigabytes of main \\nmemory. Thus the MTH here is forty plus years. Given \\nprojected improvements in hardware, this effectively \\nmeans we can maintain the matrix profile forever. \\nAs impressive as these  numbers are, they are actually quite \\npessimistic. For simplicity we assume that every value in the \\nmatrix profile index will be updated at each time step.  \\nHowever, empirically, much less than 0.1% of them need to be \\nupdated. If it is possible to prove an upper bound on the number \\nof changes to the matrix profile index per update, then we could \\ngreatly extend the MTH  or handle much faster sampling rates. \\nWe leave such considerations for future work. \\nIV. EXPERIMENTAL EVALUATION \\nWe begin by stating our experimen tal philosophy. We have \\ndesigned all experiments such that they are easily reproducible. \\nTo this end, we have built a webpage [24] which contains al l \\ndatasets and code used in this work, together with spreadsheets \\nwhich contain the raw numbers and some supporting videos.  \\nGiven page limits, and the utility of our algorithm for a host \\nof existing (and several new) data mining tasks, we have chosen \\nto c onduct exceptionally broad  but shallow experiments. We \\nhave conducted such deep detailed experiments and placed \\nthem at [24]. Unless otherwise state d we measure wall clock \\ntime on an Intel i7@4GHz with 4 cores. \\nA. Scalability of Profile-Based Self-Join \\nBecause the time performance of STAMP is independent of \\nthe data quality or any user inputs (there are none except the \\nchoice of m, which does not affect the speed), our scalability \\nexperiments are unusually brief.  In TABLE V we show the \\ntime required for a self -join with m fixed to 256, for \\nincreasingly long time series.  \\nTABLE V.  TIME REQUIRED FOR A SELF-JOIN WITH M = 256, VARYING N \\nValue of n 217 218 219 220 221 \\nTime Required 15.1 min 70.4 min 5.4 hours 24.4 hours 4.2 days \\nIn TABLE VI, we show the time required for a self -join \\nwith n fixed to 2 17, for increasing long m. Again recall that \\nunlike virtually all other time series data mining algorithms in \\nthe literature whose performance degrades for longer \\nsubsequences [8][18], the running time of STAMP does not \\ndepend on m. \\nTABLE VI. TIME REQUIRED FOR A SELF-JOIN WITH N= 217, VARYING M \\nValue of m 64 128 256 512 1,024 \\nTime Required 15.1 min 15.1 min 15.1 min 15.0 min 14.5 min \\nFinally, we further exploit the simple parallelizability of the \\nalgorithm by using four 16 -core virtual machines on Microsoft \\nAzure to redo the two -million join ( n = 2 21 and m = 256) \\nexperiment. By scaling up the computational power, we have \\nreduced the running time from 4.2 days to just 14.1 hours. This \\nuse of cloud computing required writing just few dozen lines of \\nsimple additional code [24]. \\nIt is difficult to find good baselines to compare to. We \\nbelieve STAMP is the only algorithm that does full, exact joins \\non time series subsequences. Most algorithms do only threshold \\njoins and/or do approximate joins, and are not specialized for \\ntime series subsequences. However, we searched for the best \\nbaselines and made the following concessions to  the rival \\nalgorithms to allow comparisons. \\n\\uf0b7 TSFRDAA [11]: While STAMP returns all nearest \\nneighbors, we adjust the threshold (the selectively) of \\nTSFRDAA such that only needs to return the top 1% nearest \\nneighbors, a much easier task. \\n\\uf0b7 HDSJI-SAX [12]: This method  allows false negatives ; we \\nignore this. It needs time to build indexes ; we do not count \\nthis time, and as above, we allow it to return  the only the \\ntop 1% nearest neighbors ( as bef ore, not the 100% the \\nSTAMP returns, and therefore a much easier task.). \\n\\uf0b7 Optimized Nested Loop (ONL): Here we use a nested -loop \\njoin optimized in the following manner. We use a state -of-\\nthe-art DFT indexing technique to do the search in the \\ninner loop, and we do not count the time needed to build \\nthe indexes [8]. We carefully adjust  parameters for best \\nperformance.  \\nWe consider the first 2 18 data points of the ECG dataset \\nused in [22], with a query length of 256, about one heartbeat ; \\nTABLE VII shows the results.  \\nTABLE VII. TIME FOR A SELF-JOIN WITH M = 256, VARYING ALGORITHMS \\nAlgorithm TSFRDAA HDSJI-SAX ONL STAMP \\nTime Required 51.7 hours 19.6 min 28.1 hours 1.17 hours \\nAs these results show, even if we ignore the limitations of \\nthe baseline methods, STAMP is still significantly faster. \\nB. Profile-Based Self-Join \\nA recent paper notes that many fundamental problems in \\nseismology can be solved by joining seismometer telemetry \\n[28], including the disc overy of foreshocks, aftershocks, \\ntriggered earthquakes, volcanic activity and induced seismicity. \\nHowever, the paper notes a join with a query length of 200 on a \\ndata stream of length 604,781 requires 9.5 days. Their solution, \\na clever transformation of t he data to allow LSH based \\ntechniques, does achieve significant speedup, but at the cost of \\nfalse negatives and the need for significant parameter tuning.  \\nThe authors kindly shared their data, and, as we hint at in \\nFigure 6, confirmed that STAMP does not have false negatives. \\n \\nFigure 6. top) An excerpt of a seismic time series aligned with its matrix \\nprofile (bottom). The ground truth provided by the authors of [28] requires that \\nthe events occurring at time 4,050 and 7,800 match. \\nWe repeated the n = 604,781, m = 200 experime nt and \\nfound it took just 8.9 hours to finish. As impressive as this is, in \\nthe next section we show that we can do even better.   \\n4,000 5,000 6,000 7,000 8,000 9,000\\n0\\n5\\n10\\n15\\nSeismic time series (excerpt) \\nMatrix Profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='C. The Utility of Anytime STAMP \\nThe seismology dataset offers an excellent opportunity to \\ndemonstrate the utility of the anytime version of our algorithm. \\nThe authors of [28] revealed in their long -term ambition of \\nmining even larger datasets [3]. In Figure 7 we repeated the \\nexperiment with the snippet shown in Figure 6, this time \\nreporting the best-so-far matrix profile reported by the \\nalgorithm at various milestones. Even with just 0.25% of the \\ndistances computed (that is to say, 400 times faster) the correct \\nanswer has emerged. Thus, we can provide the correct answers \\nto the seismologists in just minutes, rather than the 9.5 days.  \\n \\nFigure 7. top) An excerpt of the seismic data that is also shown in Figure 6. \\ntop-to-bottom) The approximations of the matrix profile for increasing \\ninterrupt times. By the time we have computed just 0.25% of the calculations \\nrequired, the minimum of the matrix profile points to the ground truth. \\nTo show the generality of this anytime feature of STAMP, \\nwe consider a very different dataset. As shown in Figure \\n8.inset), it is possible to convert DNA to a time ser ies [22]. We \\nconverted the Y-chromosome of the Chimpanzee this way. The \\nresulting time series is  little over one -million in length. We \\nperformed a self-join with m = 60,000.  Figure 8.bottom shows \\nthe best motif is so well conserved (ignoring the first 20%), that \\nit must correspond to a recent (in evolutionary tim e) gene \\nduplication event. In fact, in a subsequent analysis we \\ndiscovered that ‚Äúmuch of the Y (Chimp chromosome) consists of \\nlengthy, highly similar repeat units, or ‚Äòamplicons‚Äô‚Äù [9]. \\n \\nFigure 8. top) The Y -chromosome of the Chimp in time series space with its \\nmatrix profile. bottom) A zoom -in of the top motif discovered using anytime \\nSTAMP, we believe it to be an amplicon [9]. \\nThis demanding join would take just over a day of CPU \\ntime (see TABLE V). However, using anytime STAMP we \\nhave the result shown above after doing just 0.021% of the \\ncomputations, in about 18 seconds. At [24] we have videos that \\nshow the rapid convergence of the anytime variant of STAMP. \\nD. Profile-Based Similarity Join Set \\nIn this section we show two uses of similarity join set. The \\nfirst use is more familiar to APSS users, quantifying what is \\nsimilar between two time series. The second example, \\nquantifying what is different between two time series, is novel  \\nand can only be supported by threshold -free algorithms that \\nreport the nearest neighbor for all objects. \\nTime Series Set Similarity \\nGiven two (or more) time series collected under different \\nconditions or treatments, a data analyst may wish to know what \\npatterns (if any) are conserved between the two time series. To \\ndemonstrate the utility of automating this, we consider a simple \\nbut in tuitive example. Figure 9 shows the raw audio of two \\npopular songs converted to Mel Frequency Cepstral \\nCoefficients (MFCCs). Specifically, the songs used in this \\nexample are ‚ÄúUnder Pressure‚Äù by Queen an d David Bowie and \\n‚ÄúIce Ice Baby ‚Äù by the American rapper Vanilla Ice. Normally, \\nthere are 13 MFCCs; here, we consider just one for simplicity. \\n \\n \\nFigure 9. Two songs represented by just the 2 nd MFCC at 100Hz. We \\nrecognize that it is difficult to see any structure in these times series; however, \\nthis difficulty is the motivation for this experiment. \\nEven for these two relatively short time series, visual \\ninspection does not offer immediate answers. The problem here \\nis compounded by the size reproduction, but is not significantly \\neasier with large-scale [24] or interactive graphic tools. We ran \\nJAB (Queen-Bowie, Vanilla Ice) on these datasets with m = 500 \\n(five seconds), the best match, corresponding the minimum \\nvalue of the matrix profile PAB, is shown in Figure 10. \\n \\nFigure 10. The result of JAB (Queen-Bowie (red/bold), Vanilla-Ice (green/fine)) \\nproduces a strongly conserved five second region. \\nReaders may know the cause for this highly conserved \\nsubsequence. It corresponds to the famous baseline of ‚ÄúUnder \\nPressure,‚Äù which was sampled (plagiarized) by Vanilla Ice in \\nhis song. The join took 21.9 seconds. The ability to find \\nconserved structure in apparently disparate time series could \\nopen many avenues of research in medicine and industry. \\nTime Series Difference \\nWe introduce the Time Series Diff (TSD) operator, which \\ninformally asks ‚ÄúWhat happens in time series T A, that does not \\nhappen in time series TB?‚Äù Here TA/TB could be an ECG before \\na drug is administered/after a drug is administered, telemetry \\nbefore a successful launch/before a catastrophic launch etc. The \\nTSD is simply the subsequence referred to by the maximum \\nvalue of the JAB join‚Äôs profile PAB. \\nWe begin with a simple intuitive example. The UK and US \\nversions of the Harry Potter audiobook series are performed by \\ndifferent narrators, and have a handful of differences in the text. \\nFor example, the UK version contains: \\nHarry was passionate about Quidditch. He had played as Seeker on the Gryffindor \\nhouse Quidditch team ever since his first year at Hogwarts and owned a Firebolt, one \\nof the best racing brooms in the world... \\nBut the corresponding USA version has: \\nHarry had been on the Gryffindor House Quidditch team ever since his first year at \\nHogwarts and owned one of the best racing brooms in the world, a Firebolt. \\nAs shown in Figure 11, we can convert the audio \\ncorresponding to these snippets into MFCCs and invoke a JAB \\njoin set to produce a matrix profile PAB that represents the \\ndifferences between them. As Figure 11.left shows, the low \\nvalues of this profile correspond to identical spoken phrases \\n(despite having two different narrators). However here we are \\n4,000 5,000 6,000 7,000 8,000 9,000\\ndata\\n0.25%\\n1%\\n100%\\n0 1,000,0000\\n100\\n200\\nPan troglodytes Y-chromosome \\n0 60,000\\n12,749,475 to 14,249,474 bp\\n622,725 to 2,122,724 bp\\nT1 = 0;\\nfor i = 1 to length(chromosome) \\nif chromosomei = A, then Ti+1 = Ti + 2 \\nif chromosomei = G, then Ti+1 = Ti + 1\\nif chromosomei = C, then Ti+1 = Ti - 1 \\nif chromosomei = T, then Ti+1 = Ti - 2 \\nend\\n0\\nQueen-Bowie\\n1,000 2,000\\n-10\\n0\\n10\\nVanilla Ice\\n0 250 500-3\\n-2\\n-1\\n0\\n1\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='interested in the differences, the maximum value of the profile. \\nAs we can see in Figure 11.right, here the profile corresponds \\nto a phrase unique to the USA edition.  \\n \\nFigure 11. The 2 nd MFCC of snippets from the USA ( pink/bold) and UK \\n(green/fine) Harry Potter audiobooks.  The JAB join of the two longer sections \\nin the main text produce s mostly small values in the profile correspond to the \\nsame phrase (left), the largest value in  the profile corresponds to a phrase \\nunique to the USA edition (right). \\nThe time required to do this is just 0.067 seconds, much \\nfaster than real time. While this demonstration is trivial, in [24] \\nwe show an example applied to ECG telemetry.  \\nE. Profile-Based Motif Discovery \\nSince their introduction in 2003, time series motifs have \\nbecome one of the most frequently used primitives in time  \\nseries data mining, with applications in dozens of domains [2]. \\nThere are several proposed definitions for time series motifs, \\nbut in [18] it is argued that if you can solve the most basic \\nvariant, the closest (non -trivial) pair of subsequences, then all \\nother variants only require some minor additional calculations. \\nNote that the locations of the two (tying) minimum values of \\nthe matrix profile are exactly the locations of the 1st motif pair. \\nThe fastest known exact algorithm for computing time \\nseries motifs is the MK algorithm [18]. Note, however, that this \\nalgorithm‚Äôs time performance depends on the time series itself. \\nIn contrast, the Profile -Based Motif Discovery (PBMD) takes \\ntime independent of the data. To see this, we compared the two \\napproaches on an electrocardiogram of length 65,536. In Figure \\n12.left we ask what happens as we search for lon ger and longer \\nmotifs. In Figure 12.right we ask what happens if the motif \\nlength is fixed to m = 512, but the data becomes increasing \\nnoisy.  \\n \\nFigure 12. The time required to find the top -motif pairs in a time series of \\nlength 216 for increasingly long motif lengths ( left), and for a length fixed to \\n512, but in the face of increasing noise levels (right). \\nThese results show that  even in the best case for MK, \\nPBMD is competitive, but as we have longer queries and/or \\nnoisier data, its advantage becomes unassailable. Moreover, \\nPBMD inherits STAMP‚Äôs anytime and incremental \\ncomputability, and is easily parallelizable.  \\nF. Profile-Based Discord Discovery \\nA time series discord  is the subsequence that has the \\nmaximum distance to its nearest neighbor. While this is a \\nsimple definition, time series discords are known to be very \\ncompetitive as novelty/anomaly detectors  [5]. Note that as \\nshown in Figure 13, the time series discord is encoded as the \\nmaximum value in a matrix profile. \\n \\nFigure 13. top) An excerpt from an ECG incorporating a premature ventricular \\ncontraction (red/bold). bottom) The time series profile peaks exactly at the \\nbeginning of the PVC. \\nThe time taken to compute the discord  is obviously just the \\ntime needed to compute the matrix profile ( here, 0.9 seconds). \\nThere are a few dozen discord discovery algorithms in the \\nliterature. Some of them may be competitive in the best case, \\nbut just like motif -discovery algorithms they all degenerate to \\nbrute force search in the worst case, and none allow the anytime \\nproperties that we inherit from using STAMP.  \\nG. Incrementally Maintaining Motifs and Discords \\nWe have demonstrated the ability to detect time series \\nmotifs and discords using the matrix profile in the previous two \\nsections. However, we assumed that the entire time series was \\navailable beforehand. Here we remove this assumption and \\nshow how STAMP I allows us to incrementa lly maintain time \\nseries motifs/ discords in an online fashion. There are other \\nattempts at one [20][2] or both [25] of these tasks, but they are \\nall approximate and allow false dismissals.   \\nIn Section III.E, we introduced the STAMPI algorithm. The \\nability to incrementally maintain the matrix profile implies the \\nability to exactly maintain the time series motif [18] and/or time \\nseries discord [5] in streaming data. We simply need to keep \\ntrack of the extreme values of the incrementally-growing matrix \\nprofile, report a new pair of motifs when a new minimum value \\nis detected, and report a new discord when we see a new \\nmaximum value. \\nWe demonstrate the utility of these ideas on the AMPds \\ndataset [13]. Here the kitchen fridge and the heat pump are both \\nplugged into a single metered power supply. For the first week, \\nonly the refrigerator is running. At the end of the week, the \\nweather gets cold and the heat pump is turned on. The sampling \\nrate is one sample/m inute, and th e subsequence length is 100. \\nWe apply the STAMP algorithm to the first three days of data, \\nthen invoke STAMP I to handle newly arriving data , report an \\nevent when we detect a new extreme value. \\nOur first event occurs at the 9,864 th minute (6 da y 20 hour \\n24 minute). As shown in Figure 14, a new minimum value is \\ndetected, which indicates a new time series motif. The just -\\narrived 100 -minute-long pattern looks v ery similar to another \\npattern that occurred five hours earlier. While there is a lot of \\nregularity in the fridge data in general, the exceptional \\nsimilarity observed here suggested some underlying physical \\nmechanism that caused such a perfectly -conserved pattern, \\nperhaps a mechanical ice-making ‚Äúsubroutine.‚Äù \\n \\nFigure 14. top) The matrix profile of the first 9,864 minutes of data. bottom) \\nThe minimum value of the matrix profile corresponds to a pair of time series \\nmotifs in the power usage data. right) The time series motif detected. \\nOur second event occurs at the 10,473 th minute (7 day 6 \\nhour 33 minute). As shown in Figure 15, a new maximum value \\n‚Ä¶indor house Quidditch team ever since his first ye‚Ä¶\\nHarry had been on the Gryffindor House Quidditch te..\\nsince his first year at Hogwarts and owned a Fire..\\nsince his first year at Hogwarts and owned on..\\nED = 2.9 \\nClosest Match \\n0 100(1.6 seconds) 0 100\\nED = 10.4\\n(1.6 seconds)\\nFurthest Match (Time Series Difference) \\n256 512 1,024 2,048 4,096\\n0\\n40\\n80\\n120\\n160\\n200\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nSubsequence Length  (m)\\nTime Taken (min)\\n80 40 0\\n0\\n70\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nNoise Added (dB)  \\n0 1000 2000 3000\\nECG qtdb\\nSel102\\n(excerpt)\\nPremature Ventricular \\nContraction\\nTime Series Profile\\n0 5,000 10,000\\n0 100\\nnew minimum value\\nnew motifMatrix Profile\\nPower Usage Data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='is detected, which indicates a new time series discord. The time \\nseries discord corresponds to the first occurrence of a heat \\npump pattern in the power usage data.  \\n \\nFigure 15. top) The matrix profile for the first 10,473 minutes. bottom) The \\nmaximum value of the matrix profile corresponds to a time series discord. \\nright) The time series discord detected is the first  heat pump pattern \\noccurrence in the dataset. \\nThe maximum time needed to process a single data point \\nwith STAMP I in this dataset is 0.005 second s, which is less \\nthan 0.01% of the data sampling rate. Thus, on this dataset we \\ncould continue monitoring with the STAMP I algorithm for \\nseveral decades before running out of time or memory. \\nH. Profile-Based Shapelet Discovery \\nShapelets are time series subsequences which are in some \\nsense the maximally representative of a class [23][27]. \\nShapelets can be used to classify time series (essentially, the \\nnearest shapelet algorithm), offering the benefits of speed, \\nintuitiveness and at least on some domains, significantly \\nimproved accuracy [23]. However, these advantages come at \\nthe cost of a very expensive training phase, with O( n2m4) time \\ncomplexity, where m is the length of the longest time series \\nobject in the dataset, and n is the number of objects in the \\ntraining set. In order to mitigate this high ti me complexity, \\nresearchers have proposed various distance pruning techniques \\nand candidate ranking approaches for both the admissible [27] \\nand appro ximate [23] shapelet discovery. Nevertheless, \\nscalability remains the bottleneck.  \\nBecause shapelets are essentially supervised motifs, and we \\nhave shown that STAMP can find motifs very quickly, it is \\nnatural to ask if STAMP has implications for shapelet \\ndiscovery. While space limitations prohibit a detailed \\nconsideration of this question, we briefly sketch out and test \\nthis possibility as follows. \\nAs shown in Figure 16, we can use matrix profile to \\nheuristically ‚Äúsuggest‚Äù candidate shapelets. We consider two \\ntime series TA (green/bold) and TB (pink/light) with class 1 and \\nclass 0 being their corresponding class label, and we take  JAB, \\nJAA, JBA and JBB. Our claim is the differences in the heights of \\nPAB, PAA (or PBA, PBB) are strong indicators of good candidate \\nshapelets. The intuition is that if a discriminative pattern is \\npresent in say, class 1, but not in class 0, then we expect to see a \\n‚Äúbump‚Äù in the PAB (the intuition holds if the order is reversed). \\nA significant difference (quantified by a threshold shown in \\ndashed line) between the heights of PAA and PAB curves \\ntherefore indicates the occurrence of good candidate shapelets, \\npatterns that only occur in one of the two classes. \\n \\nFigure 16. top.left) Two time series TA and TB formed by concatenating \\ninstances of class 1 and 0 respectively of ArrowHead [6]. bottom) The height \\ndifference between PAB (or PBA) and PAA (or PBB) are suggestive of  good \\nshapelets. top.right) An example of good shapelet extracted from class 1. \\nThe time taken to compute  all four matrix profiles is 1.0 \\nseconds and the time to further evaluate the two twelve \\ncandidates selected takes 2.7 seconds. On the same machine, \\nthe brute force shapelet classifier takes 4.2 minutes with 2,364 \\ncandidates. Note that, in this toy demonst ration, the speedup is \\n68X, however, for larger datasets, the speedup is greater [24]. \\nI. Profile-Based Semantic Segmentation \\nThe goal of time series semantic segmentation is to partition \\na dataset containing multiple activities/regimes into atomic \\nbehaviors or conditions. For example, for human activity data \\nthe regimes might later be mapped to { eating, working, \\ncommuting,...}[29]. As this example suggests, most work in this \\narea is highly domain -dependent. In contrast, here we show \\nhow the matrix profile index can be emp loyed for domain \\nagnostic time series segmentation.  \\nThe intuition of our approach is as follows. Within a single \\nregime we might expect that most subsequences will have a \\nnearest neighbor close by (in time). For example, consider the \\ntoy problem shown in Figure 17 which shows two obvious \\nregimes. We would expect that the nearest neighbor to the first \\nrun gait cycle is the second or third or fourth run cycle, but it \\nwill almost certainly not be one of the walk cycles. In general, \\nthis tendency for nearest neighbor pointers not to cross the \\nboundaries corresponding to regime changes may be sufficient \\nto discover these boundaries, and of course, this is precisely the \\ninformation that is encoded in the matrix profile index. \\nThus, for each point we count how many ‚Äúarcs‚Äù connecting \\ntwo nearest neighbors cross it if we connect each subsequence \\nto its nearest neighbor as shown in Figure 17. \\n \\nFigure 17. top) A (toy) time series ( red) and nearest neighbor locations for \\neach subsequence. bottom) The number of arcs crossing above each \\nsubsequence. \\nWe applied the procedure described above to a heavily \\nstudied activity segmentation problem [29], which was derived \\nfrom the CMU Motion Capture data base [16]. The recordings \\nare represented as multi -dimensional time series, and most \\nresearch efforts carefully select the best subset for the \\nsegmentation task. For example, [29] states that ‚Äú we only \\nconsider the 14 most informative joints out of 29 .‚Äù In contrast, \\nwe attempt this with a single dimension. The only parameter we \\nneed to set is sliding window size . In Figure 18 we show the \\nsegmenting results obtained using our approach , with  \\nannotations taken from [29] for context. \\nMuch of the evaluation in this community lacks formal \\nmetrics, preferring instead visual sanity tests like the one in \\nFigure 18. Given this, we can say that our approach is very \\ncompetitive on this dataset, in spite of the fact that we \\nhandicapped ourselves to only consider one dimension.  \\n0 100\\n0 10,0005,000\\nMatrix Profile\\nPower Usage Data\\nnew maximum value\\nnew discord\\n0\\n7\\n0 1,400\\nPABPAA\\n0 1,600\\nTA\\nTB\\n0 300\\nAn example of good shapelet\\n0 1,400\\nPBA\\nPBB\\n8\\n0\\n0 1,400\\nDiff(PAB ,PAA) Threshold\\n-1\\n4\\n-2\\n4\\n0 1,400\\nDiff(PBA ,PBB) Threshold\\n0\\n1\\n2\\n1 2 0\\nwalking slow    walking slow     run       run run run\\nNumber of arcs intersecting each point'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content=\"Figure 18. top) A matrix profile ( blue) obtained for the time series ( red) and \\nnumber of arcs crossing each point ( green). Low values of this green curve \\ncoorespond to candidate split points. bottom) Human annotations of the \\nactivities: w ‚Äì walk, s ‚Äì stretch, p ‚Äì punch, c ‚Äì chop, t ‚Äì turn, d ‚Äì drink. \\nV. CONCLUSION \\nWe have introduced a scalable algorithm for creating time \\nseries subsequences joins. Our algorithm is simple, fast, \\nparallelizable and parameter -free, and can be in crementally \\nupdated for moderately fast data arrival rates. We have shown \\nthat our algorithm has implications for many existing tasks, \\nsuch as motif discovery, discord discovery, shapelet discovery \\nand semantic segmentation, and may open up new avenues for  \\nresearch, including computing various definitions of time series \\nset difference. Our code is freely available for the community to \\nconfirm, extend and exploit our ideas. \\nREFERENCES \\n[1] R. J. Bayardo, Y. Ma and R. Srikant, ‚ÄúScaling up all pairs similarity \\nsearch,‚Äù WWW 2007, pp 131-140. \\n[2] N. Begum and E. Keogh, ‚ÄúRare time series motif discovery from \\nunbounded streams,‚Äù PVLDB 8(2): 149-160, 2014. \\n[3] G. Beroza, ‚ÄúPersonal Correspondence,‚Äù Jan 21th, 2016. \\n[4] T. Bouezmarni and J. Rombouts, ‚ÄúNonparametric density estimation for \\npositive time series,‚Äù CSDA, 54, 245-261, 2010.  \\n[5] V. Chandola, D. Cheboli and V. Kumar, ‚ÄúDetecting anomalies in a time \\nseries database,‚Äù UMN TR09-004. \\n[6] T. Chen  et al ., ‚ÄúThe UCR time series classification archive,‚Äù \\nhttp://www.cs.ucr.edu/~eamonn/time_series_data/. \\n[7] ‚ÄúConvolution - Wikipedia, the free encyclopedia,‚Äù  \\nhttps://en.wikipedia.org/wiki/Convolution, Accessed: 2016-01-19. \\n[8] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang and E. J. Keogh, \\n‚ÄúQuerying and mining of time series data: experimental comparison of \\nrepresentations and distance measures,‚Äù PVLDB 1(2): 1542-1552. 2008. \\n[9] J. Hughes et al. , ‚ÄúChimpanzee and human Y chromosomes are \\nremarkably divergent in structure‚Äù Nature 463, (2010). \\n[10] H. Lee, R. Ng and K. Shim, ‚ÄúSimilarity join size estimation using \\nLocality sensitive hashing,‚Äù PVLDB, 4(6):338‚Äì349, 2011. \\n[11] W. Luo, H. Tan, H. Mao  and L. M.  Ni, ‚ÄúEfficient similarity joins on \\nmassive high-dimensional datasets using mapreduce,‚Äù In MDM'12, IEEE \\nComputer Society, pp. 1-10. \\n[12] Y. Ma, X. Meng and S. Wang, ‚ÄúParallel similarity joins on massive high-\\ndimensional data using MapReduce ,‚Äù Concurrency and Computation , \\nVolume 28, Issue 1 Jan 2016. Pages 166‚Äì183. \\n[13] S. V. Makonin, ‚ÄúAMPds: a public dataset for load disaggregation and \\neco-feedback research,‚Äù EPEC 2013, pp 1-6. \\n[14] MASS: http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html \\n[15] G. D. F. Morales and A. Gionis, ‚Äústreaming similarity self-join,‚Äù Proc. \\nVLDB Endow, 2016. \\n[16] Motion Capture Database, http://mocap.cs.cmu.edu/ \\n[17] A. Mueen, H. Hamooni and T. Estrada, ‚ÄúTime series join on subsequence \\ncorrelation,‚Äù IEEE ICDM 2014, pp. 450-459. \\n[18] A. Mueen, E. Keogh, Q. Zhu, S. Cash and B. Westover, ‚ÄúExact discovery \\nof time series motif,‚Äù SDM 2009. \\n[19] D. Murray et al. , ‚ÄúA data management platform for personalised real-\\ntime energy feedback,‚Äù In EEDAL 2015. \\n[20] V. Niennattrakul et al, ‚ÄúData editing techniques to allow the application \\nof distance-based outlier detection to streams,‚Äù ICDM 2010: 947-952. \\n[21] D. Patnaik, et al, ‚ÄúSustainable operation and management of data center \\nchillers using temporal data mining,‚Äù KDD 2009. \\n[22] T. Rakthanmanon et al., ‚ÄúSearching and Mining Trillions of Time Series \\nSubsequences under Dynamic Time Warping,‚Äù In KDD 2012, 262-270. \\n[23] T. Rakthanmanon and E. Keogh, ‚ÄúFast shapelets: a scalable algorithm for \\ndiscovering time series shapelets,‚Äù SDM, 2013. \\n[24] Supporting page. http://www.cs.ucr.edu/~eamonn/MatrixProfile.html \\n[25] C. D. Truong and D. T.  Anh, ‚ÄúAn Efficient Method for Motif and \\nAnomaly Detection in Time Series‚Äù IJBIDM, Vol. 10, No. 4, 2015. \\n[26] A. Tucker and X. Liu, ‚ÄúA Bayesian  Network Approach to Explaining \\nTime Series with Changing Structure,‚Äù Intell Data Anal, 8 (5) (2004). \\n[27] L. Ye and E. Keogh, ‚ÄúTime series shapelets: a new primitive for data \\nmining,‚Äù ACM SIGKDD, 2009, pp 947-56. \\n[28] C. Yoon, O. O‚ÄôReilly, K. Bergen and G. Beroza, ‚ÄúEarthquake detection \\nthrough computationally efficient similarity search,‚Äù Sci. Adv. 2015. \\n[29] F. Zhou, F. Torre and J.  Hodgins ‚Äú Aligned Cluster Analysis for \\nTemporal Segmentation of Human Motion,‚Äù IEEE FG'2008. \\n[30] S. Zilberstein and S. Russell, ‚ÄúApproximate Reasoning Using Anytime \\nAlgorithms,‚Äù In Imprecise and Approximate Computation , Kluwer \\nAcademic Publishers, 1995. \\n \\n0 5,000 10,000\\nMatrix  Profile\\nSplit Points   \\nPrediction\\nOne-dimension of multi-d time series: Subject 86, recording 4, dimension 30\\nW S P N      W C T           D            P         W\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Matrix Profile I: All Pairs Similarity Joins for Time Series: \\nA Unifying View that Includes Motifs, Discords and Shapelets \\nChin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,  \\nHoang Anh Dau, ‚Ä†Diego Furtado Silva, ‚Ä°Abdullah Mueen, and Eamonn Keogh \\nUniversity of California, Riverside, ‚Ä†Universidade de S√£o Paulo, ‚Ä°University of New Mexico \\n{myeh003, yzhu015, lulan001, nbegu001, yding007, hdau001}@ucr.edu, diegofsilva@icmc.usp.br, mueen@unm.edu, eamonn@cs.ucr.edu \\n \\nAbstract‚Äî The all-pairs-similarity-search (or similarity join) \\nproblem has been extensively studied for text and a handful of \\nother datatypes. However, surprisingly little progress has been  \\nmade on similarity joins for time series subsequences. The lack of  \\nprogress probably stems from the daunting nature of the \\nproblem. For even modest sized data sets the obvious nested -loop \\nalgorithm can take months, and the typical speed -up techniques \\nin this domain (i.e., indexing, lower -bounding, triangular -\\ninequality pruning and early abandoning) at best produce one or \\ntwo orders of magnitude speedup. In this work we introduce a \\nnovel scalable algorithm for time series subsequence all -pairs-\\nsimilarity-search. For exceptionally large datasets, the algorithm \\ncan be trivially cast as an anytime algorithm and produce high -\\nquality approximate solutions in reasonable  time. The exact \\nsimilarity join algorithm computes the answer to the time series \\nmotif and time series discord  problem as a side -effect, and our \\nalgorithm incidentally provides the fastest known algorithm for \\nboth these  extensively-studied problems. We de monstrate the \\nutility of our ideas for many time series data mining problems, \\nincluding motif discovery, novelty discovery,  shapelet discovery,  \\nsemantic segmentation, density estimation, and contrast set \\nmining.    \\nKeywords‚ÄîTime Series; Similarity Joins; Motif Discovery \\nI. INTRODUCTION \\nThe all-pairs-similarity-search (also known as similarity \\njoin) problem comes in several variants. The basic task is this: \\nGiven a collection of data objects, retrieve the nearest neighbor \\nfor each object . In the text domain the  algorithm has \\napplications in a host of problems, including community \\ndiscovery, duplicate detection, collaborative filtering, \\nclustering, and query refinement [1]. While virtually all text \\nprocessing algorithms have analogues in time series data \\nmining, there has been surprisingly little progress on Time \\nSeries subsequences All-Pairs-Similarity-Search (TSAPSS). \\nWe believe that this lack of progress stems not from a lack \\nof interest in this useful primitive, but from the daunting nature \\nof the problem. Consider the following example that reflects the \\nneeds of an industrial collaborator. A boiler at a che mical \\nrefinery reports pressure once a minute. After a year, we have a \\ntime series of length 525,600. A plant manager may wish to do \\na similarity self-join on this data with week -long subsequences \\n(10,080) to discover operating regimes (summer vs. winter o r \\nlight distillate vs. heavy distillate etc.) The obvious nested loop \\nalgorithm requires 132,880,692,960 Euclidean distance \\ncomputations. If we assume each one takes 0.0001 seconds, \\nthen the join will take 153.8 days. The core contribution of this \\nwork is to show that we can reduce this time to 6.3 hours, using \\nan off-the-shelf desktop computer. Moreover, we show that this \\njoin can be computed and/or updated incrementally. Thus we \\ncould maintain this join essentially forever on a standard \\ndesktop, even if the data arrival frequency was much faster than \\nonce a minute. \\nOur algorithm uses an ultra -fast similarity search algorithm \\nunder z -normalized Euclidean distance as a subroutine, \\nexploiting the overlap between subsequences using the classic \\nFast Fourier Transform (FFT) algorithm.  \\nOur method has the following advantages/features: \\n\\uf0b7 It is exact, providing no false positives or false dismissals. \\n\\uf0b7 It is simple and parameter -free. In contrast, the more \\ngeneral metric space APSS algorithms require building and \\ntuning spatial access methods and/or hash functions.  \\n\\uf0b7 Our algorithm requires an inconsequential space overhead, \\njust O(n) with a small constant factor. \\n\\uf0b7 While our exact algorithm is extremely scalable, for \\nextremely large datasets we can compute the results in an \\nanytime fashion, allowing ultra-fast approximate solutions.  \\n\\uf0b7 Having computed the similarity join for a dataset, we can \\nincrementally upd ate it very efficiently. In many domains \\nthis means we can effectively maintain exact joins on \\nstreaming data forever.  \\n\\uf0b7 Our method provides full joins, eliminating the need to \\nspecify a similarity threshold, which as we will show, is a \\nnear impossible task in this domain.  \\n\\uf0b7 Our algorithm is embarrassingly parallelizable, both on \\nmulticore processors and in distributed systems. \\nGiven all these features, our algorithm has implications for \\nmany time series data mining tasks [5][18][28].  \\nThe rest of the paper is organized as follows. Section II \\nreviews related work an d introduces the necessary background \\nmaterials and definitions. In Section III we introduce our \\nalgorithm and its anytime and incremental variants. Section IV \\nsees a detailed empirical evaluation of our algorithm and shows \\nits implications for many data mining tasks. Finally, in Section \\nV we offer conclusions and directions for future work. \\nII. RELATED WORK AND BACKGROUND \\nThe basic variant of  similarity join problem we are \\ninterested in is as follows : Given a collection of data objects, \\nretrieve the nearest neighbor for every object.   \\nOther common variants include retrieving the top-K nearest \\nneighbors or the nearest neighbor for each object if that \\nneighbor is within a user-supplied threshold, œÑ. (Such variations \\nare trivial generalizations of our proposed algorithm, so we \\nomit them from further discussion). The latter variant results in \\na much easier problem, provided that the threshold is small. For \\nexample, [1] notes that virtually all research efforts ‚Äú exploit a \\nsimilarity threshold more aggressively in order to limit the set'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='of candidate pairs that are considered..  [or] ... to reduce the \\namount of information indexed in the first place.‚Äù \\nThis critical dependence on œÑ is a major issue for text joins, \\nas it is known that ‚Äú join size can change dramatically \\ndepending on the input similarity threshold ‚Äù [10]. However, \\nthis issue is even more critical for time series for two reasons. \\nFirst, unlike similarity (which is bounded between zero and \\none), the Euclidean distance is effectively unbounded, and \\ngenerally not intuitive. For example, if two heartbeats have a \\nEuclidean distance of 17.1, are they similar? Even for a domain \\nexpert that knows the sampling rate and the noise level of the \\ndata, this is not obvious. Second, a single threshold can produce \\nradically different output sizes, even for datasets that are very \\nsimilar.  Consider Figure 1 which shows the output size vs. \\nthreshold setting for the first and s econd halves of a ten -day \\nperiod monitoring data center chillers [21]. For the first five \\ndays a threshold of 0.6 would return zero items, but for the \\nsecond five days the same setting would return 108 items.  This \\nshows the difficulty in selecting  an appropriate threshold. Our \\nsolution is to have no threshold, and do a full join. \\n \\nFigure 1.  Output size vs. threshold for data center chillers [21]. Values beyond \\n2.0 are truncated for clarity (but archived at [24]).  \\nA handful of efforts have considered joins on time series, \\nachieving speedup by (in addition to the use of MapReduce) \\nconverting the data to lower -dimensional representations such \\nas PAA [11] or SAX [12] and exploiting lower bounds and/o r \\nLocality Sensitive Hashing (LSH) to prune some calculations. \\nHowever, the methods are very complex, with many (10 -plus) \\nparameters to adjust. As [11] acknowledges with admirable \\ncandor, ‚ÄúReasoning about the optimal settings is not trivial.‚Äù In \\ncontrast, our proposed algorithm has zero parameters to set. \\nA very recent research effort [28] has tackled the scalability \\nissue by converting the real -valued time series into discrete \\n‚Äúfingerprints‚Äù before using a LSH approach, much like the text \\nretrieval community [1]. They produced impressive speedup, \\nbut they also experienced false negatives. Moreover, the \\napproach has several parameters that need to be set; for \\nexample, they need to set the threshold to a very precise 0.818.  \\nAs we shall show, our algorithm allows both anytime and \\nincremental (i.e. streaming) versions. While a streaming join \\nalgorithm for text was recently introduced [15], we are not \\naware of any such algorithms for time series data or general \\nmetric spaces. More generally, there is a large amount of \\nliterature on joins for text processing [1]. Such work is \\ninteresting, but of little utility given our constraints, data type \\nand problem setting. We require full joins, not threshold joins, \\nand we are unwilling to allow the possibility of false negatives. \\nA. Definitions and Notation \\nWe begin by defining the data type of interest, time series: \\nDefinition 1:  A time series T is a sequence of real -valued \\nnumbers ti: T = t1, t2, ..., tn where n is the length of T. \\nWe are not interested in the global properties of time series, \\nbut in the similarity between local subsequences:  \\nDefinition 2:  A subsequence Ti,m of a T is a continuous \\nsubset of the values from T of length m starting from \\nposition i.   Ti,m = ti, ti+1,‚Ä¶, ti+m-1, where 1 ‚â§  i ‚â§  n-m+1.  \\nWe can take any subsequence from a time series and \\ncompute its distance to all sequences. We call an ordered vector \\nof such distances a distance profile: \\nDefinition 3:  A distance profile D is a vector of the \\nEuclidean distances between a given query and each \\nsubsequence in an all-subsequences set (see Definition 4).  \\nNote that we are assuming that the distance is measured \\nusing the Euclidean distance between the z -normalized \\nsubsequences [8]. The distance profile can be considered a meta \\ntime series that annotates the time series T that was used to \\ngenerate it. The first three definitions are illustrated in Figure 2. \\n \\nFigure 2. A subsequence Q extracted from a time series T is used as a query to \\nevery subsequence in T. The vector of all distances is a distance profile. \\nNote that if the query and all-subsequences set belong to the \\nsame time series, the distance profile must be zero at the \\nlocation of the query, and close to zero just before and just \\nafter. Such matches are c alled trivial matches in the literature \\n[18], and are avoided by ignoring an exclusion zone (shown as \\na gray region) of m/2 before and after the location of the query. \\nWe are interested in similarity join of all subsequences of a \\ngiven time series. We define an all-subsequences set of a given \\ntime series as a set that contains all possible subsequences from \\nthe time series. The notion of all-subsequences set is purely for \\nnotational purposes. In our implementation, we do not actually \\nextract the subsequences in this form as it would require \\nsignificant time and space overhead. \\nDefinition 4: An all-subsequences set A of a time series T \\nis an ordered set of all possible subsequences of T obtained \\nby sliding a window of length m across T: A ={T1,m,, \\nT2,m,‚Ä¶, Tn-m+1,m}, where m is a user -defined subsequence \\nlength. We use A[i] to denote Ti,m. \\nWe are interested in the nearest neighbor (i.e., 1NN) relation \\nbetween subsequences; therefore, we define a 1NN-join \\nfunction which indicates the nearest neighbor relation between \\nthe two input subsequences. \\nDefinition 5:  1NN-join function :  given two all -\\nsubsequences sets A and B and two subsequences A[i] and \\nB[j], a 1NN-join function  Œ∏1nn (A[i], B[j]) is a Boolean \\nfunction which returns ‚Äútrue‚Äù only if B[j] is the nearest \\nneighbor of A[i] in the set B. \\nWith the defined join function, a similarity join set  can be \\ngenerated by applying the similarity join operator on two input \\nall-subsequences sets. \\nDefinition 6: Similarity join set : given all -subsequences \\nsets A and B, a similarity join set  JAB of A and B is a set \\ncontaining pairs of each subsequence in A with its nearest \\nneighbor in B: JAB={‚å© A[i], B[j] ‚å™ |Œ∏1nn (A[i], B[j])}.  We \\ndenote this formally as JAB = A‚ãà\\uf0711nnB. \\n0\\n400\\n800\\n1200\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\\nFirst 5 Days\\nSecond 5 Days\\nData Center Chillers\\nT, a snippet of an energy \\nconsumption\\n2,0000 m/2m/2\\nQ, query of length m\\nNote that |D| = |T|-|Q|+1D, a distance profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='We measure the Euclidean distance between each pair \\nwithin a similarity join set and store the resultants into an \\nordered vector. We call the result vector the matrix profile. \\nDefinition 7:  A matrix profile (or just profile) PAB is a \\nvector of the Euclidean distances between each pair in JAB. \\nWe call this vector the matrix profile because one \\n(inefficient) way to compute it would be to compute the full \\ndistance matrix of all the subsequences in one time series with \\nall the subsequence in another time series and extract the \\nsmallest value in each row (the smallest non-diagonal value for \\nthe self-join case).  In Figure 3 we show the matrix profile of \\nour running example. \\n \\nFigure 3. A time series T, and its self-join matrix profile P.  \\nLike the distance profile, the matrix profile can be \\nconsidered a meta time series annotating the time series T if the \\nmatrix profile is generated by joining T with itself. The profile \\nhas a host of interesting and exploitable properties. For \\nexample, the highest point on the profile corresponds to the \\ntime series discord [5], th e ( tied) lowest points correspond to \\nthe locations of the best time series motif pair [18], and the \\nvariance can be seen as a measure of the T‚Äôs com plexity. \\nMoreover, the histogram of the values in the matrix profile is \\nthe exact answer to the time series density estimation [4]. \\nWe name this special case of the similarity join set \\n(Definition 6) as self-similarity join set, and the corresponding \\nprofile as self-similarity join profile. \\nDefinition 8:  A self-similarity join  set JAA is a result of \\nsimilarity join of the set  A with itself . We denote this \\nformally as JAA = A ‚ãà\\uf0711nn A. We denote the corresponding \\nmatrix profile or self-similarity join profile as PAA. \\nNote that we exclude trivial matches when self -similarity \\njoin is performed, i.e., if A[i] and A[j] are subsequences from \\nthe same all -subsequences set A, Œ∏1nn (A[i], B[j]) is ‚Äúfalse‚Äù \\nwhen A[i] and A[j] are a trivially matched pair. \\nThe ith element in the matrix profile tells us the Euclidean \\ndistance to the nearest neighbor of the subsequence of T, \\nstarting at i.  However, it does not tell us where that neighbor is \\nlocated. This information is recorded in matrix profile index. \\nDefinition 9: A matrix profile index IAB of a similarity join \\nset JAB is a vector of integers where IAB[i] = j if {A[i], B[j]} \\n‚àà JAB. \\nBy storing the neighboring information this way, we can \\nefficiently retrieve the nearest neighbor of A[i] by accessing the \\nith element in the matrix profile index. \\nNote that the function which computes the similarity join set \\nof two input time series  is not symmetric; therefore, JAB ‚â† JBA,  \\nPAB ‚â† PBA, and IAB ‚â† IBA. \\nFor ease of presentation, we have confined this work to the \\nsingle dimensional case; however, nothing intrinsically \\nprecludes generalizations to multidimensional data. \\nSummary of the Previous Section \\nThe previous section was rather dense, so before moving on \\nwe summarize the main takeaway points. We can create two \\nmeta time series, the matrix profile and the matrix profile index, \\nto annotate a time series TA with the distance and location of all \\nits subsequences nearest neighbors in itself or another time \\nseries TB. These two data objects explicitly or implicitly contain \\nthe answers to many time series data mining tasks. However, \\nthey appear to be too expensive to compute to be practica l. In \\nthe next section we will show an algorithm that can compute \\nthese efficiently. \\nIII. ALGORITHMS \\nWe are finally in a position to explain our algorithm s. We \\nbegin by stating the fundamental intuition, which stems from \\nthe relationship between distance profiles and the matrix \\nprofile. As Figure 2 and Figure 3 visually suggest, all distance \\nprofiles (excluding the trivial match region) are upper bound \\napproximations to the matrix profile. More critically, if we \\ncompute all the distance profiles, an d take the minimum value \\nat each location, the result is the matrix profile! \\nThis tells us that if we have a fast way to compute the \\ndistance profiles, then we also have a fast way to compute the \\nmatrix profile. As we shall show in the next section, we have an \\nultra-fast way to compute the distance profiles. \\nA. Mueen‚Äôs ultra-fast Algorithm for Similarity Search (MASS) \\nWe begin by introducing a novel Euclidean distance \\nsimilarity search algorithm for time series data. The algorithm \\ndoes not just find the nearest neighbor to a query and return its \\ndistance; it returns the distance to every subsequence. In \\nparticular, it computes the distance profile, as shown in Figure \\n2. The algorithm requires just O( nlogn) time by exploiting the \\nFFT to calculate  the dot products between the query and all \\nsubsequences of the time series.  \\nWe need to carefully qualify the claim of ‚Äúultra-fast‚Äù. There \\nare dozens of algorithms for time series similarity search that \\nutilize index structures to efficiently locate neighbors [8]. While \\nsuch algorithms can be faster in the  best case , all these \\nalgorithms degenerate to brute force search in the worst case 1 \\n(actually, much worse than brute force search due to the \\noverhead of the index). Likewise, there are index -free methods \\nthat achieve speed -up using various early abandoning tricks \\n[22], but they too degrade to brute force search in the worst \\ncase. In contrast, the performance of the algorithms outlined in \\nTABLE I and TABLE II is completely independent of the data. \\nTABLE I. CALCULATION OF SLIDING DOT PRODUCTS \\nProcedure SlidingDotProduct(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: The dot product between Q and all subsequences in T \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nn ‚Üê Length(T), m ‚Üê Length(Q) \\nTa ‚Üê Append T with n zeros   \\nQr ‚Üê Reverse(Q)  \\nQra ‚Üê Append Qr with 2n-m zeros \\nQraf ‚Üê FFT(Qra), Taf ‚Üê FFT(Ta) \\nQT ‚Üê InverseFFT(ElementwiseMultiplication(Qraf, Taf)) \\nreturn QT \\n \\n1 There are many such worse case scenarios, including high levels of noise blurring \\nthe distinction between closest and furthest neighbors, and rendering triangular -\\ninequality pruning and early abandoning worthless. \\n2,0000\\nP, a matrix profile\\nT, a snippet of an energy \\nconsumption\\nNote that |P| = |T|-|Q|+1'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Line 1 determines the length of both the time series T and \\nthe query Q. In line 2, we use that information to append T with \\nan equal number of zeros. In line 3, we obtain the mirror image \\nof the original query. This reversing ensures that a convolution \\n(i.e. ‚Äúcrisscrossed‚Äù multiplication) essentially produces in-order \\nalignment. Because we require both vectors to be the s ame \\nlength, in line 4 we append enough zeros to the (now reversed) \\nquery so that, like Ta, it is also of length 2 n. In line 5, the \\nalgorithm calculates Fourier transforms of the appended -\\nreversed query (Qra) and the appended time series Ta. Note that \\nwe use FFT algorithm which is an O(nlogn) algorithm. The Qraf \\nand the Taf produced in line 5 are vectors of complex numbers \\nrepresenting frequency components of the two time series. The \\nalgorithm calculates the element-wise multiplication of the two \\ncomplex vectors and performs inverse FFT on the product. \\nLines 5-6 are the class ic convolution operation on two vectors \\n[7]. Figure 4 shows a toy example of the sliding dot product \\nfunction in work. The algorithm time complexity does not \\ndepend on the length of the query (m). \\n \\nFigure 4. A toy example of convolution operation being used to calculate  \\nsliding dot products for time series data. Note the reverse and append \\noperation on T and q in the input. Fifteen dot products are calculated for every \\nslide. The cells m = 2 to n = 4 from left ( red/bold arrows) contain valid \\nproducts. TABLE II takes this subroutine and uses it to create a distance \\nprofile (see Definition 3). \\nIn line 1 of TABLE II, we invoke the dot products code \\noutlined in TABLE I. The formula to calculate the z-normalized \\nEuclidean distance D[i] between two time series subsequence Q \\nand Ti,m using their dot product, QT[i] is (see [24] for \\nderivation): \\nùê∑[ùëñ] = ‚àö2ùëö(1‚àíùëÑùëá[ùëñ]‚àíùëöùúáùëÑùëÄùëá[ùëñ]\\nùëöùúéùëÑùõ¥ùëá[ùëñ] ) \\nwhere m is the subsequence length, ŒºQ is the mean of Q, \\nMT[i] is the mean of Ti,m, œÉQ is the standard deviation of Q, and  \\nŒ£T[i] is the standard deviation of Ti,m. Normally, it takes O( m) \\ntime to calculate the mean and standard deviation for every \\nsubsequence of a long time series. However, here we exploit a \\ntechnique noted in [22] in a different context. We cache \\ncumulative sums of the values and square of the values in the \\ntime series. At any stage the two cumulative sum vectors are \\nsufficient to calculate the mean an d the standard deviation of \\nany subsequence of any length.  See [14] for an elaborate \\ndescription and variations of MASS. \\nTABLE II. MUEEN‚ÄôS ALGORITHM FOR SIMILARITY SEARCH (MASS) \\nProcedure MASS(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: A distance profile D of the query Q  \\n1 \\n2 \\n3 \\n4 \\nQT ‚Üê SlidingDotProducts(Q, T) \\nŒºQ, œÉQ, ŒúT, Œ£T  ‚Üê ComputeMeanStd(Q, T)    // see [22] \\nD ‚Üê CalculateDistanceProfile(Q, T, QT, ŒºQ, œÉQ, ŒúT, Œ£T) \\nreturn D \\nUnlike the dozens of time series KNN search algorithms in \\nthe literature [8], this algorithm calculates the distance to every \\nsubsequence, i.e. the distance profile  of time series T. \\nAlternatively, in join nomenclature, the algorithm produces one \\nfull row of the all -pair similarity matrix. Thus, as we show in \\nthe next section, our join algorithm is simply a loop that \\ncomputes each full row of the all -pair similarity matrix and \\nupdates the current ‚Äúbest-so-far‚Äù matrix profile when needed. \\nB. The STAMP Algorithm \\nWe call our join algorithm STAMP, Scalable Time series \\nAnytime Matrix Profile. The algorithm is outlined in TABLE \\nIII. In line 1, we extract the length of TB. In line 2, we allocate \\nmemory and initial matrix profile PAB and matrix profile index \\nIAB. From lines 3 to line 6, we calculate the distance profiles D \\nusing each subsequence B[idx] in the time series TB and the \\ntime series TA. Then, we perform pairwise minimum for each \\nelement in D with the paired element in PAB (i.e., min( D[i], \\nPAB[i]) for i = 0 to length(D) - 1.) We also update IAB[i] with idx \\nwhen D[i] ‚â§ PAB[i] as we perform the  pairwise minimum \\noperation. Finally, we return the result PAB and IAB in line 7.  \\nNote that the algorithm presented in TABLE III computes \\nthe matrix profile for the general similarity join. To modify the \\ncurrent algorithm to compute  the self -similarity join matrix \\nprofile of a time series TA, we simply replace TB in line 1 with \\nTA, replace B with A in line 4, and ignore trivial match in D \\nwhen performing ElementWiseMin in line 5. \\nTABLE III. THE STAMP ALGORITHM \\nProcedure STAMP(TA, TB, m) \\nInput: Two user provided time series, TA and TB, interested \\nsubsequence length m \\nOutput: A matrix profile PAB and associated matrix profile index IAB of \\nTA join TB, JAB = A‚ãà\\uf0711nnB \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nnB ‚Üê Length(TB) \\nPAB ‚Üê infs, IAB ‚Üê zeros, idxes ‚Üê 1:nB-m+1 \\nfor each idx in idxes    // In any order, but random for anytime algorithm \\n          D ‚Üê MASS(B[idx], TA) \\n          PAB, IAB ‚Üê ElementWiseMin(PAB, IAB, D, idx) \\nend for \\nreturn PAB, IAB \\nTo parallelize the STAMP algorithm for multicore \\nmachines, we simply distribute the indexes to secondary \\nprocess run in each core, and the secondary processes use the \\nindexes they received to update their own PAB and IAB. Once the \\nmain process returns from  all secondary processes, we use \\nElementWiseMin to merge the received PAB and IAB.  \\nC. An Anytime Algorithm for TSAPSS \\nWhile the exact algorithm introduced in the previous section \\nis extremely scalable, there will always be datasets for which \\ntime needed for an exact solution is untenable. We can mitigate \\nthis by computing the results in an anytime fashion, allowing \\nfast approximate solutions [30]. To a dd the anytime nature to \\nthe STAMP algorithm, we simply ensure a randomized search \\norder in line 2 of TABLE III.  \\nWe can compute a (post-hoc) measurement of the quality of \\nan anytime solution by measuring the Root -Mean-Squared-\\nError (RMSE) between the true matrix profile and the current \\nbest-so-far mat rix profile. As Figure 5 suggests, with an \\nexperiment on random walk data, the algorithm converges very \\nquickly. \\nQ2T1 0 00 00 00 00 0Q1T4Q2T2+Q1T1 Q2T3+Q1T2 Q2T4+Q1T3\\nOutput\\nInputT2T1 T4T3 00 00 Q1Q2 00 00 00'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Figure 5. main) The decre ase in RMSE as the STAMP algorithm updates \\nmatrix profile with the distance profile calculated at each iteration.  inset) The \\napproximate matrix profile at the 10% mark is visually indistinguishable from \\nthe final matrix profile. \\nZilberstein [30] gives a number of desirable properties of \\nanytime algorithms, including Low Overhead , Interruptibility, \\nMonotonicity, Recognizable Quality, Diminishing Returns and \\nPreemptability (these properties are mostly obvious from their \\nnames, but full definitions are at [30]). \\nBecause each subsequence‚Äôs distance profile is bounded \\nbelow by the exact matrix profile, updating an approximate \\nmatrix profile with a distance profile with pairwise minimum \\noperation either drives the approximate solution closer the exact \\nsolution or retains the current approximate solution. Thus , we \\nhave guaranteed Monotonicity. From Figure 5, the approximate \\nmatrix profile converges to the exact matrix profile \\nsuperlinearly; therefore, we have strong Diminishing Returns. \\nWe can easily achieve Interruptibility and Preemptability by \\nsimply inserting a few lines of code between lines 5 and 6 of \\nTABLE III that read: \\n5new \\n6new \\n7new \\nif CheckForUserInterrupt  = TRUE \\n          Report({PAB, IAB}, ‚ÄòHere is an approximate answer.‚Äô) \\nif GetUserChoice = ‚Äòfurther refine‚Äô, CONTINUE, else BREAK \\nThe space and time overhead for the anytime property is \\neffectively zero, thus we have Low Overhead. This leaves only \\nthe property of Recognizable Quality. Here we must resort to a \\nprobabilistic argument.  The convergence curve shown in  \\nFigure 5 is very typical, so we could use past convergence \\ncurves to predict the quality of solution when interrupted on \\nsimilar data.  \\nD. Time and Space Complexity \\nThe overall complexity of the proposed algorithm is \\nO(n2logn) where n is the length of the time series. However, our \\nexperiments (see Section 4.1) empirically suggest that the \\nruntime of  STAMP‚Äôs growth rate is roughly O( n2) instead of \\nO(n2logn). One possible e xplanation for this is that the nlogn \\nfactor comes from the FFT subroutine. Because FFT is so \\nimportant in many applications, it is extraordinarily well \\noptimized. Thus, the empirical runtime is very close to linear. \\nIn contrast to the above, the brute for ce nested loop \\napproach has a time complexity of O(n2m). Recall the industrial \\nexample in the introduction section. We have  m = 10,080, but \\nlog(n) = 5.7, so we would expect our approach to be about \\n1,768 times faster. In fact, we are empirically even faster. The \\ncomplexity analysis downplays the details of important constant \\nfactors. The nested loop algorithm must also z -normalize the \\nsubsequences. This either requires O( nm) time, but with an \\nuntenable O(nm) space overhead, or an O( n2m) time overhead. \\nAnd r ecall that this is before a single Euclidean distance \\ncalculation is performed.  \\nFinally, we mention one quirk of our algorithm which we \\ninherit from using the highly optimized FFT subroutine. Our \\nalgorithm is fastest when n is an integer power of two, slower \\nfor non -power of two but composite numbers, and slowest \\nwhen n is prime. The difference (for otherwise similar values of \\nn) can approach a factor of 1.6x. Thus, where possible, it is \\nworth contriving the best case by tru ncation or zero-padding to \\nthe nearest power of two. \\nE. Incrementally Maintaining TSAPSS \\nUp to this point we have discussed the batch version of \\nTSAPSS. By batch, we mean that the STAMP algorithm needs \\nto see the entire time series TA and TB (or just TA if we are \\ncalculating the self -similarity join matrix profile) before \\ncreating the matrix profile. However, it would be advantageous \\nif we could build the matrix profile incrementally. Given that \\nwe have performed a batch construction of matrix profile, i f \\nnew data arrives, it would clearly be preferable to incrementally \\nadjust the current profile, rather than start from scratch.  \\nBecause the matrix profile solves both the times series motif \\nand the time series discord problems, an incremental version of \\nSTAMP would automatically provide the first incremental \\nversions of both algorithms. We call such an algorithm the \\nSTAMPI (STAMP Incremental) algorithm. \\nWe will demonstrate our ability to incrementally maintain \\nthe matrix profile in this section. For simpli city and brevity \\nTABLE IV only shows the algorithm to maintain the self -\\nsimilarity join. The generalizations are obvious. \\nTABLE IV. THE STAMPI ALGORITHM \\nProcedure STAMPI(TA, t, PAA, IAA) \\nInput: The original time series TA, a new data point t following TA, the \\nmatrix profile PAA and its associated matrix profile index IAA of TA.  \\nOutput: The incrementally updated matrix profile PAA,new and its matrix \\nprofile index IAA,new of the current time series TA,new= TA, t. \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nTA,new = [TA, t] \\nS ‚Üê last subsequence in TA,new, idx ‚Üê index of S in TA,new \\nD ‚Üê MASS (S, TA) \\nPAA, IAA ‚Üê ElementWiseMin(PAA, IAA, D, idx) \\npAA,last, iAA,last ‚Üê FindMin(D) \\nPAA,new ‚Üê [PAA, pAA,last], IAA,new ‚Üê [IAA, iAA,last] \\nreturn PAA,new, IAA,new \\nFor clarity, we denote the updated time series as TA,new, the \\nupdated matrix profile as PAA,new and the associated matrix \\nprofile index as IAA,new. As each additional data point t arrives, \\nthe size of the time series TA increases by one, and a new \\nsubsequence S is generated at the end of TA,new. In line 3 we \\nobtain the distance profile of S with regard to TA. Then, as in the \\noriginal STAMP algorithm,  in line 4 we perform  a pairwise \\ncomparison between every element in D with the corresponding \\nelement in PAA to see if the corresponding element in PAA needs \\nto be updated. In line 5, we find the nearest neighbor of S and \\nthe associated index by evaluating the minimum value of D. \\nFinally, in line 6, we obtain the new matrix profile and \\nassociated matrix profile index by concatenating the results in \\nline 4 and line 5.  \\nThe time complexity of the STAMP I algorithm is O(nlogn) \\nwhere n is the length of size of th e current time series TA. Note \\nthat as we maintain the profile, each incremental call of \\nSTAMPI requires invoking the FFT subroutine with a slightly \\nlonger time series ( n becomes n+1 ). Thus it gets very slightly \\nslower at each time step. Therefore, the best way to measure the \\nperformance is to ask for the Maximum Time Horizon (MTH), \\nin essence the answer to this question: ‚Äú Given this arrival rate, \\nhow long can we maintain the profile before we can no longer \\nupdate fast enough?‚Äù \\nNote that the subsequence length m is not considered in the \\nMTH evaluation. Recall that overall time complexity of the \\n10,0000\\nRMSE\\niteration1,000\\napproximate matrix \\nprofile at 1,000 iterations.\\nexact matrix profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='algorithm is determined by the efficiency of the FFT \\nsubroutine, which is independent of m. We have computed the \\nMTH for two common scenarios of interest to the community. \\n\\uf0b7 House Electrical Demand  [19]: This dataset is updated \\nevery eight seconds. By iteratively calling the STAMP I \\nalgorithm, we can maintain the profile for 5.8 years.  \\n\\uf0b7 Oil Refinery :  Most telemetry in oil refineries is sampled \\nat once a minute [26]. The relatively low sampling rate \\nreflects the ‚Äúinertia‚Äù of massive boilers/condensers. Even if \\nwe maintain the profile for 40 years, the update time is only \\naround 6.78 seconds. Moreover, th e raw data, matrix \\nprofile and index would only require 0.5 gigabytes of main \\nmemory. Thus the MTH here is forty plus years. Given \\nprojected improvements in hardware, this effectively \\nmeans we can maintain the matrix profile forever. \\nAs impressive as these  numbers are, they are actually quite \\npessimistic. For simplicity we assume that every value in the \\nmatrix profile index will be updated at each time step.  \\nHowever, empirically, much less than 0.1% of them need to be \\nupdated. If it is possible to prove an upper bound on the number \\nof changes to the matrix profile index per update, then we could \\ngreatly extend the MTH  or handle much faster sampling rates. \\nWe leave such considerations for future work. \\nIV. EXPERIMENTAL EVALUATION \\nWe begin by stating our experimen tal philosophy. We have \\ndesigned all experiments such that they are easily reproducible. \\nTo this end, we have built a webpage [24] which contains al l \\ndatasets and code used in this work, together with spreadsheets \\nwhich contain the raw numbers and some supporting videos.  \\nGiven page limits, and the utility of our algorithm for a host \\nof existing (and several new) data mining tasks, we have chosen \\nto c onduct exceptionally broad  but shallow experiments. We \\nhave conducted such deep detailed experiments and placed \\nthem at [24]. Unless otherwise state d we measure wall clock \\ntime on an Intel i7@4GHz with 4 cores. \\nA. Scalability of Profile-Based Self-Join \\nBecause the time performance of STAMP is independent of \\nthe data quality or any user inputs (there are none except the \\nchoice of m, which does not affect the speed), our scalability \\nexperiments are unusually brief.  In TABLE V we show the \\ntime required for a self -join with m fixed to 256, for \\nincreasingly long time series.  \\nTABLE V.  TIME REQUIRED FOR A SELF-JOIN WITH M = 256, VARYING N \\nValue of n 217 218 219 220 221 \\nTime Required 15.1 min 70.4 min 5.4 hours 24.4 hours 4.2 days \\nIn TABLE VI, we show the time required for a self -join \\nwith n fixed to 2 17, for increasing long m. Again recall that \\nunlike virtually all other time series data mining algorithms in \\nthe literature whose performance degrades for longer \\nsubsequences [8][18], the running time of STAMP does not \\ndepend on m. \\nTABLE VI. TIME REQUIRED FOR A SELF-JOIN WITH N= 217, VARYING M \\nValue of m 64 128 256 512 1,024 \\nTime Required 15.1 min 15.1 min 15.1 min 15.0 min 14.5 min \\nFinally, we further exploit the simple parallelizability of the \\nalgorithm by using four 16 -core virtual machines on Microsoft \\nAzure to redo the two -million join ( n = 2 21 and m = 256) \\nexperiment. By scaling up the computational power, we have \\nreduced the running time from 4.2 days to just 14.1 hours. This \\nuse of cloud computing required writing just few dozen lines of \\nsimple additional code [24]. \\nIt is difficult to find good baselines to compare to. We \\nbelieve STAMP is the only algorithm that does full, exact joins \\non time series subsequences. Most algorithms do only threshold \\njoins and/or do approximate joins, and are not specialized for \\ntime series subsequences. However, we searched for the best \\nbaselines and made the following concessions to  the rival \\nalgorithms to allow comparisons. \\n\\uf0b7 TSFRDAA [11]: While STAMP returns all nearest \\nneighbors, we adjust the threshold (the selectively) of \\nTSFRDAA such that only needs to return the top 1% nearest \\nneighbors, a much easier task. \\n\\uf0b7 HDSJI-SAX [12]: This method  allows false negatives ; we \\nignore this. It needs time to build indexes ; we do not count \\nthis time, and as above, we allow it to return  the only the \\ntop 1% nearest neighbors ( as bef ore, not the 100% the \\nSTAMP returns, and therefore a much easier task.). \\n\\uf0b7 Optimized Nested Loop (ONL): Here we use a nested -loop \\njoin optimized in the following manner. We use a state -of-\\nthe-art DFT indexing technique to do the search in the \\ninner loop, and we do not count the time needed to build \\nthe indexes [8]. We carefully adjust  parameters for best \\nperformance.  \\nWe consider the first 2 18 data points of the ECG dataset \\nused in [22], with a query length of 256, about one heartbeat ; \\nTABLE VII shows the results.  \\nTABLE VII. TIME FOR A SELF-JOIN WITH M = 256, VARYING ALGORITHMS \\nAlgorithm TSFRDAA HDSJI-SAX ONL STAMP \\nTime Required 51.7 hours 19.6 min 28.1 hours 1.17 hours \\nAs these results show, even if we ignore the limitations of \\nthe baseline methods, STAMP is still significantly faster. \\nB. Profile-Based Self-Join \\nA recent paper notes that many fundamental problems in \\nseismology can be solved by joining seismometer telemetry \\n[28], including the disc overy of foreshocks, aftershocks, \\ntriggered earthquakes, volcanic activity and induced seismicity. \\nHowever, the paper notes a join with a query length of 200 on a \\ndata stream of length 604,781 requires 9.5 days. Their solution, \\na clever transformation of t he data to allow LSH based \\ntechniques, does achieve significant speedup, but at the cost of \\nfalse negatives and the need for significant parameter tuning.  \\nThe authors kindly shared their data, and, as we hint at in \\nFigure 6, confirmed that STAMP does not have false negatives. \\n \\nFigure 6. top) An excerpt of a seismic time series aligned with its matrix \\nprofile (bottom). The ground truth provided by the authors of [28] requires that \\nthe events occurring at time 4,050 and 7,800 match. \\nWe repeated the n = 604,781, m = 200 experime nt and \\nfound it took just 8.9 hours to finish. As impressive as this is, in \\nthe next section we show that we can do even better.   \\n4,000 5,000 6,000 7,000 8,000 9,000\\n0\\n5\\n10\\n15\\nSeismic time series (excerpt) \\nMatrix Profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='C. The Utility of Anytime STAMP \\nThe seismology dataset offers an excellent opportunity to \\ndemonstrate the utility of the anytime version of our algorithm. \\nThe authors of [28] revealed in their long -term ambition of \\nmining even larger datasets [3]. In Figure 7 we repeated the \\nexperiment with the snippet shown in Figure 6, this time \\nreporting the best-so-far matrix profile reported by the \\nalgorithm at various milestones. Even with just 0.25% of the \\ndistances computed (that is to say, 400 times faster) the correct \\nanswer has emerged. Thus, we can provide the correct answers \\nto the seismologists in just minutes, rather than the 9.5 days.  \\n \\nFigure 7. top) An excerpt of the seismic data that is also shown in Figure 6. \\ntop-to-bottom) The approximations of the matrix profile for increasing \\ninterrupt times. By the time we have computed just 0.25% of the calculations \\nrequired, the minimum of the matrix profile points to the ground truth. \\nTo show the generality of this anytime feature of STAMP, \\nwe consider a very different dataset. As shown in Figure \\n8.inset), it is possible to convert DNA to a time ser ies [22]. We \\nconverted the Y-chromosome of the Chimpanzee this way. The \\nresulting time series is  little over one -million in length. We \\nperformed a self-join with m = 60,000.  Figure 8.bottom shows \\nthe best motif is so well conserved (ignoring the first 20%), that \\nit must correspond to a recent (in evolutionary tim e) gene \\nduplication event. In fact, in a subsequent analysis we \\ndiscovered that ‚Äúmuch of the Y (Chimp chromosome) consists of \\nlengthy, highly similar repeat units, or ‚Äòamplicons‚Äô‚Äù [9]. \\n \\nFigure 8. top) The Y -chromosome of the Chimp in time series space with its \\nmatrix profile. bottom) A zoom -in of the top motif discovered using anytime \\nSTAMP, we believe it to be an amplicon [9]. \\nThis demanding join would take just over a day of CPU \\ntime (see TABLE V). However, using anytime STAMP we \\nhave the result shown above after doing just 0.021% of the \\ncomputations, in about 18 seconds. At [24] we have videos that \\nshow the rapid convergence of the anytime variant of STAMP. \\nD. Profile-Based Similarity Join Set \\nIn this section we show two uses of similarity join set. The \\nfirst use is more familiar to APSS users, quantifying what is \\nsimilar between two time series. The second example, \\nquantifying what is different between two time series, is novel  \\nand can only be supported by threshold -free algorithms that \\nreport the nearest neighbor for all objects. \\nTime Series Set Similarity \\nGiven two (or more) time series collected under different \\nconditions or treatments, a data analyst may wish to know what \\npatterns (if any) are conserved between the two time series. To \\ndemonstrate the utility of automating this, we consider a simple \\nbut in tuitive example. Figure 9 shows the raw audio of two \\npopular songs converted to Mel Frequency Cepstral \\nCoefficients (MFCCs). Specifically, the songs used in this \\nexample are ‚ÄúUnder Pressure‚Äù by Queen an d David Bowie and \\n‚ÄúIce Ice Baby ‚Äù by the American rapper Vanilla Ice. Normally, \\nthere are 13 MFCCs; here, we consider just one for simplicity. \\n \\n \\nFigure 9. Two songs represented by just the 2 nd MFCC at 100Hz. We \\nrecognize that it is difficult to see any structure in these times series; however, \\nthis difficulty is the motivation for this experiment. \\nEven for these two relatively short time series, visual \\ninspection does not offer immediate answers. The problem here \\nis compounded by the size reproduction, but is not significantly \\neasier with large-scale [24] or interactive graphic tools. We ran \\nJAB (Queen-Bowie, Vanilla Ice) on these datasets with m = 500 \\n(five seconds), the best match, corresponding the minimum \\nvalue of the matrix profile PAB, is shown in Figure 10. \\n \\nFigure 10. The result of JAB (Queen-Bowie (red/bold), Vanilla-Ice (green/fine)) \\nproduces a strongly conserved five second region. \\nReaders may know the cause for this highly conserved \\nsubsequence. It corresponds to the famous baseline of ‚ÄúUnder \\nPressure,‚Äù which was sampled (plagiarized) by Vanilla Ice in \\nhis song. The join took 21.9 seconds. The ability to find \\nconserved structure in apparently disparate time series could \\nopen many avenues of research in medicine and industry. \\nTime Series Difference \\nWe introduce the Time Series Diff (TSD) operator, which \\ninformally asks ‚ÄúWhat happens in time series T A, that does not \\nhappen in time series TB?‚Äù Here TA/TB could be an ECG before \\na drug is administered/after a drug is administered, telemetry \\nbefore a successful launch/before a catastrophic launch etc. The \\nTSD is simply the subsequence referred to by the maximum \\nvalue of the JAB join‚Äôs profile PAB. \\nWe begin with a simple intuitive example. The UK and US \\nversions of the Harry Potter audiobook series are performed by \\ndifferent narrators, and have a handful of differences in the text. \\nFor example, the UK version contains: \\nHarry was passionate about Quidditch. He had played as Seeker on the Gryffindor \\nhouse Quidditch team ever since his first year at Hogwarts and owned a Firebolt, one \\nof the best racing brooms in the world... \\nBut the corresponding USA version has: \\nHarry had been on the Gryffindor House Quidditch team ever since his first year at \\nHogwarts and owned one of the best racing brooms in the world, a Firebolt. \\nAs shown in Figure 11, we can convert the audio \\ncorresponding to these snippets into MFCCs and invoke a JAB \\njoin set to produce a matrix profile PAB that represents the \\ndifferences between them. As Figure 11.left shows, the low \\nvalues of this profile correspond to identical spoken phrases \\n(despite having two different narrators). However here we are \\n4,000 5,000 6,000 7,000 8,000 9,000\\ndata\\n0.25%\\n1%\\n100%\\n0 1,000,0000\\n100\\n200\\nPan troglodytes Y-chromosome \\n0 60,000\\n12,749,475 to 14,249,474 bp\\n622,725 to 2,122,724 bp\\nT1 = 0;\\nfor i = 1 to length(chromosome) \\nif chromosomei = A, then Ti+1 = Ti + 2 \\nif chromosomei = G, then Ti+1 = Ti + 1\\nif chromosomei = C, then Ti+1 = Ti - 1 \\nif chromosomei = T, then Ti+1 = Ti - 2 \\nend\\n0\\nQueen-Bowie\\n1,000 2,000\\n-10\\n0\\n10\\nVanilla Ice\\n0 250 500-3\\n-2\\n-1\\n0\\n1\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='interested in the differences, the maximum value of the profile. \\nAs we can see in Figure 11.right, here the profile corresponds \\nto a phrase unique to the USA edition.  \\n \\nFigure 11. The 2 nd MFCC of snippets from the USA ( pink/bold) and UK \\n(green/fine) Harry Potter audiobooks.  The JAB join of the two longer sections \\nin the main text produce s mostly small values in the profile correspond to the \\nsame phrase (left), the largest value in  the profile corresponds to a phrase \\nunique to the USA edition (right). \\nThe time required to do this is just 0.067 seconds, much \\nfaster than real time. While this demonstration is trivial, in [24] \\nwe show an example applied to ECG telemetry.  \\nE. Profile-Based Motif Discovery \\nSince their introduction in 2003, time series motifs have \\nbecome one of the most frequently used primitives in time  \\nseries data mining, with applications in dozens of domains [2]. \\nThere are several proposed definitions for time series motifs, \\nbut in [18] it is argued that if you can solve the most basic \\nvariant, the closest (non -trivial) pair of subsequences, then all \\nother variants only require some minor additional calculations. \\nNote that the locations of the two (tying) minimum values of \\nthe matrix profile are exactly the locations of the 1st motif pair. \\nThe fastest known exact algorithm for computing time \\nseries motifs is the MK algorithm [18]. Note, however, that this \\nalgorithm‚Äôs time performance depends on the time series itself. \\nIn contrast, the Profile -Based Motif Discovery (PBMD) takes \\ntime independent of the data. To see this, we compared the two \\napproaches on an electrocardiogram of length 65,536. In Figure \\n12.left we ask what happens as we search for lon ger and longer \\nmotifs. In Figure 12.right we ask what happens if the motif \\nlength is fixed to m = 512, but the data becomes increasing \\nnoisy.  \\n \\nFigure 12. The time required to find the top -motif pairs in a time series of \\nlength 216 for increasingly long motif lengths ( left), and for a length fixed to \\n512, but in the face of increasing noise levels (right). \\nThese results show that  even in the best case for MK, \\nPBMD is competitive, but as we have longer queries and/or \\nnoisier data, its advantage becomes unassailable. Moreover, \\nPBMD inherits STAMP‚Äôs anytime and incremental \\ncomputability, and is easily parallelizable.  \\nF. Profile-Based Discord Discovery \\nA time series discord  is the subsequence that has the \\nmaximum distance to its nearest neighbor. While this is a \\nsimple definition, time series discords are known to be very \\ncompetitive as novelty/anomaly detectors  [5]. Note that as \\nshown in Figure 13, the time series discord is encoded as the \\nmaximum value in a matrix profile. \\n \\nFigure 13. top) An excerpt from an ECG incorporating a premature ventricular \\ncontraction (red/bold). bottom) The time series profile peaks exactly at the \\nbeginning of the PVC. \\nThe time taken to compute the discord  is obviously just the \\ntime needed to compute the matrix profile ( here, 0.9 seconds). \\nThere are a few dozen discord discovery algorithms in the \\nliterature. Some of them may be competitive in the best case, \\nbut just like motif -discovery algorithms they all degenerate to \\nbrute force search in the worst case, and none allow the anytime \\nproperties that we inherit from using STAMP.  \\nG. Incrementally Maintaining Motifs and Discords \\nWe have demonstrated the ability to detect time series \\nmotifs and discords using the matrix profile in the previous two \\nsections. However, we assumed that the entire time series was \\navailable beforehand. Here we remove this assumption and \\nshow how STAMP I allows us to incrementa lly maintain time \\nseries motifs/ discords in an online fashion. There are other \\nattempts at one [20][2] or both [25] of these tasks, but they are \\nall approximate and allow false dismissals.   \\nIn Section III.E, we introduced the STAMPI algorithm. The \\nability to incrementally maintain the matrix profile implies the \\nability to exactly maintain the time series motif [18] and/or time \\nseries discord [5] in streaming data. We simply need to keep \\ntrack of the extreme values of the incrementally-growing matrix \\nprofile, report a new pair of motifs when a new minimum value \\nis detected, and report a new discord when we see a new \\nmaximum value. \\nWe demonstrate the utility of these ideas on the AMPds \\ndataset [13]. Here the kitchen fridge and the heat pump are both \\nplugged into a single metered power supply. For the first week, \\nonly the refrigerator is running. At the end of the week, the \\nweather gets cold and the heat pump is turned on. The sampling \\nrate is one sample/m inute, and th e subsequence length is 100. \\nWe apply the STAMP algorithm to the first three days of data, \\nthen invoke STAMP I to handle newly arriving data , report an \\nevent when we detect a new extreme value. \\nOur first event occurs at the 9,864 th minute (6 da y 20 hour \\n24 minute). As shown in Figure 14, a new minimum value is \\ndetected, which indicates a new time series motif. The just -\\narrived 100 -minute-long pattern looks v ery similar to another \\npattern that occurred five hours earlier. While there is a lot of \\nregularity in the fridge data in general, the exceptional \\nsimilarity observed here suggested some underlying physical \\nmechanism that caused such a perfectly -conserved pattern, \\nperhaps a mechanical ice-making ‚Äúsubroutine.‚Äù \\n \\nFigure 14. top) The matrix profile of the first 9,864 minutes of data. bottom) \\nThe minimum value of the matrix profile corresponds to a pair of time series \\nmotifs in the power usage data. right) The time series motif detected. \\nOur second event occurs at the 10,473 th minute (7 day 6 \\nhour 33 minute). As shown in Figure 15, a new maximum value \\n‚Ä¶indor house Quidditch team ever since his first ye‚Ä¶\\nHarry had been on the Gryffindor House Quidditch te..\\nsince his first year at Hogwarts and owned a Fire..\\nsince his first year at Hogwarts and owned on..\\nED = 2.9 \\nClosest Match \\n0 100(1.6 seconds) 0 100\\nED = 10.4\\n(1.6 seconds)\\nFurthest Match (Time Series Difference) \\n256 512 1,024 2,048 4,096\\n0\\n40\\n80\\n120\\n160\\n200\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nSubsequence Length  (m)\\nTime Taken (min)\\n80 40 0\\n0\\n70\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nNoise Added (dB)  \\n0 1000 2000 3000\\nECG qtdb\\nSel102\\n(excerpt)\\nPremature Ventricular \\nContraction\\nTime Series Profile\\n0 5,000 10,000\\n0 100\\nnew minimum value\\nnew motifMatrix Profile\\nPower Usage Data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='is detected, which indicates a new time series discord. The time \\nseries discord corresponds to the first occurrence of a heat \\npump pattern in the power usage data.  \\n \\nFigure 15. top) The matrix profile for the first 10,473 minutes. bottom) The \\nmaximum value of the matrix profile corresponds to a time series discord. \\nright) The time series discord detected is the first  heat pump pattern \\noccurrence in the dataset. \\nThe maximum time needed to process a single data point \\nwith STAMP I in this dataset is 0.005 second s, which is less \\nthan 0.01% of the data sampling rate. Thus, on this dataset we \\ncould continue monitoring with the STAMP I algorithm for \\nseveral decades before running out of time or memory. \\nH. Profile-Based Shapelet Discovery \\nShapelets are time series subsequences which are in some \\nsense the maximally representative of a class [23][27]. \\nShapelets can be used to classify time series (essentially, the \\nnearest shapelet algorithm), offering the benefits of speed, \\nintuitiveness and at least on some domains, significantly \\nimproved accuracy [23]. However, these advantages come at \\nthe cost of a very expensive training phase, with O( n2m4) time \\ncomplexity, where m is the length of the longest time series \\nobject in the dataset, and n is the number of objects in the \\ntraining set. In order to mitigate this high ti me complexity, \\nresearchers have proposed various distance pruning techniques \\nand candidate ranking approaches for both the admissible [27] \\nand appro ximate [23] shapelet discovery. Nevertheless, \\nscalability remains the bottleneck.  \\nBecause shapelets are essentially supervised motifs, and we \\nhave shown that STAMP can find motifs very quickly, it is \\nnatural to ask if STAMP has implications for shapelet \\ndiscovery. While space limitations prohibit a detailed \\nconsideration of this question, we briefly sketch out and test \\nthis possibility as follows. \\nAs shown in Figure 16, we can use matrix profile to \\nheuristically ‚Äúsuggest‚Äù candidate shapelets. We consider two \\ntime series TA (green/bold) and TB (pink/light) with class 1 and \\nclass 0 being their corresponding class label, and we take  JAB, \\nJAA, JBA and JBB. Our claim is the differences in the heights of \\nPAB, PAA (or PBA, PBB) are strong indicators of good candidate \\nshapelets. The intuition is that if a discriminative pattern is \\npresent in say, class 1, but not in class 0, then we expect to see a \\n‚Äúbump‚Äù in the PAB (the intuition holds if the order is reversed). \\nA significant difference (quantified by a threshold shown in \\ndashed line) between the heights of PAA and PAB curves \\ntherefore indicates the occurrence of good candidate shapelets, \\npatterns that only occur in one of the two classes. \\n \\nFigure 16. top.left) Two time series TA and TB formed by concatenating \\ninstances of class 1 and 0 respectively of ArrowHead [6]. bottom) The height \\ndifference between PAB (or PBA) and PAA (or PBB) are suggestive of  good \\nshapelets. top.right) An example of good shapelet extracted from class 1. \\nThe time taken to compute  all four matrix profiles is 1.0 \\nseconds and the time to further evaluate the two twelve \\ncandidates selected takes 2.7 seconds. On the same machine, \\nthe brute force shapelet classifier takes 4.2 minutes with 2,364 \\ncandidates. Note that, in this toy demonst ration, the speedup is \\n68X, however, for larger datasets, the speedup is greater [24]. \\nI. Profile-Based Semantic Segmentation \\nThe goal of time series semantic segmentation is to partition \\na dataset containing multiple activities/regimes into atomic \\nbehaviors or conditions. For example, for human activity data \\nthe regimes might later be mapped to { eating, working, \\ncommuting,...}[29]. As this example suggests, most work in this \\narea is highly domain -dependent. In contrast, here we show \\nhow the matrix profile index can be emp loyed for domain \\nagnostic time series segmentation.  \\nThe intuition of our approach is as follows. Within a single \\nregime we might expect that most subsequences will have a \\nnearest neighbor close by (in time). For example, consider the \\ntoy problem shown in Figure 17 which shows two obvious \\nregimes. We would expect that the nearest neighbor to the first \\nrun gait cycle is the second or third or fourth run cycle, but it \\nwill almost certainly not be one of the walk cycles. In general, \\nthis tendency for nearest neighbor pointers not to cross the \\nboundaries corresponding to regime changes may be sufficient \\nto discover these boundaries, and of course, this is precisely the \\ninformation that is encoded in the matrix profile index. \\nThus, for each point we count how many ‚Äúarcs‚Äù connecting \\ntwo nearest neighbors cross it if we connect each subsequence \\nto its nearest neighbor as shown in Figure 17. \\n \\nFigure 17. top) A (toy) time series ( red) and nearest neighbor locations for \\neach subsequence. bottom) The number of arcs crossing above each \\nsubsequence. \\nWe applied the procedure described above to a heavily \\nstudied activity segmentation problem [29], which was derived \\nfrom the CMU Motion Capture data base [16]. The recordings \\nare represented as multi -dimensional time series, and most \\nresearch efforts carefully select the best subset for the \\nsegmentation task. For example, [29] states that ‚Äú we only \\nconsider the 14 most informative joints out of 29 .‚Äù In contrast, \\nwe attempt this with a single dimension. The only parameter we \\nneed to set is sliding window size . In Figure 18 we show the \\nsegmenting results obtained using our approach , with  \\nannotations taken from [29] for context. \\nMuch of the evaluation in this community lacks formal \\nmetrics, preferring instead visual sanity tests like the one in \\nFigure 18. Given this, we can say that our approach is very \\ncompetitive on this dataset, in spite of the fact that we \\nhandicapped ourselves to only consider one dimension.  \\n0 100\\n0 10,0005,000\\nMatrix Profile\\nPower Usage Data\\nnew maximum value\\nnew discord\\n0\\n7\\n0 1,400\\nPABPAA\\n0 1,600\\nTA\\nTB\\n0 300\\nAn example of good shapelet\\n0 1,400\\nPBA\\nPBB\\n8\\n0\\n0 1,400\\nDiff(PAB ,PAA) Threshold\\n-1\\n4\\n-2\\n4\\n0 1,400\\nDiff(PBA ,PBB) Threshold\\n0\\n1\\n2\\n1 2 0\\nwalking slow    walking slow     run       run run run\\nNumber of arcs intersecting each point'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content=\"Figure 18. top) A matrix profile ( blue) obtained for the time series ( red) and \\nnumber of arcs crossing each point ( green). Low values of this green curve \\ncoorespond to candidate split points. bottom) Human annotations of the \\nactivities: w ‚Äì walk, s ‚Äì stretch, p ‚Äì punch, c ‚Äì chop, t ‚Äì turn, d ‚Äì drink. \\nV. CONCLUSION \\nWe have introduced a scalable algorithm for creating time \\nseries subsequences joins. Our algorithm is simple, fast, \\nparallelizable and parameter -free, and can be in crementally \\nupdated for moderately fast data arrival rates. We have shown \\nthat our algorithm has implications for many existing tasks, \\nsuch as motif discovery, discord discovery, shapelet discovery \\nand semantic segmentation, and may open up new avenues for  \\nresearch, including computing various definitions of time series \\nset difference. Our code is freely available for the community to \\nconfirm, extend and exploit our ideas. \\nREFERENCES \\n[1] R. J. Bayardo, Y. Ma and R. Srikant, ‚ÄúScaling up all pairs similarity \\nsearch,‚Äù WWW 2007, pp 131-140. \\n[2] N. Begum and E. Keogh, ‚ÄúRare time series motif discovery from \\nunbounded streams,‚Äù PVLDB 8(2): 149-160, 2014. \\n[3] G. Beroza, ‚ÄúPersonal Correspondence,‚Äù Jan 21th, 2016. \\n[4] T. Bouezmarni and J. Rombouts, ‚ÄúNonparametric density estimation for \\npositive time series,‚Äù CSDA, 54, 245-261, 2010.  \\n[5] V. Chandola, D. Cheboli and V. Kumar, ‚ÄúDetecting anomalies in a time \\nseries database,‚Äù UMN TR09-004. \\n[6] T. Chen  et al ., ‚ÄúThe UCR time series classification archive,‚Äù \\nhttp://www.cs.ucr.edu/~eamonn/time_series_data/. \\n[7] ‚ÄúConvolution - Wikipedia, the free encyclopedia,‚Äù  \\nhttps://en.wikipedia.org/wiki/Convolution, Accessed: 2016-01-19. \\n[8] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang and E. J. Keogh, \\n‚ÄúQuerying and mining of time series data: experimental comparison of \\nrepresentations and distance measures,‚Äù PVLDB 1(2): 1542-1552. 2008. \\n[9] J. Hughes et al. , ‚ÄúChimpanzee and human Y chromosomes are \\nremarkably divergent in structure‚Äù Nature 463, (2010). \\n[10] H. Lee, R. Ng and K. Shim, ‚ÄúSimilarity join size estimation using \\nLocality sensitive hashing,‚Äù PVLDB, 4(6):338‚Äì349, 2011. \\n[11] W. Luo, H. Tan, H. Mao  and L. M.  Ni, ‚ÄúEfficient similarity joins on \\nmassive high-dimensional datasets using mapreduce,‚Äù In MDM'12, IEEE \\nComputer Society, pp. 1-10. \\n[12] Y. Ma, X. Meng and S. Wang, ‚ÄúParallel similarity joins on massive high-\\ndimensional data using MapReduce ,‚Äù Concurrency and Computation , \\nVolume 28, Issue 1 Jan 2016. Pages 166‚Äì183. \\n[13] S. V. Makonin, ‚ÄúAMPds: a public dataset for load disaggregation and \\neco-feedback research,‚Äù EPEC 2013, pp 1-6. \\n[14] MASS: http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html \\n[15] G. D. F. Morales and A. Gionis, ‚Äústreaming similarity self-join,‚Äù Proc. \\nVLDB Endow, 2016. \\n[16] Motion Capture Database, http://mocap.cs.cmu.edu/ \\n[17] A. Mueen, H. Hamooni and T. Estrada, ‚ÄúTime series join on subsequence \\ncorrelation,‚Äù IEEE ICDM 2014, pp. 450-459. \\n[18] A. Mueen, E. Keogh, Q. Zhu, S. Cash and B. Westover, ‚ÄúExact discovery \\nof time series motif,‚Äù SDM 2009. \\n[19] D. Murray et al. , ‚ÄúA data management platform for personalised real-\\ntime energy feedback,‚Äù In EEDAL 2015. \\n[20] V. Niennattrakul et al, ‚ÄúData editing techniques to allow the application \\nof distance-based outlier detection to streams,‚Äù ICDM 2010: 947-952. \\n[21] D. Patnaik, et al, ‚ÄúSustainable operation and management of data center \\nchillers using temporal data mining,‚Äù KDD 2009. \\n[22] T. Rakthanmanon et al., ‚ÄúSearching and Mining Trillions of Time Series \\nSubsequences under Dynamic Time Warping,‚Äù In KDD 2012, 262-270. \\n[23] T. Rakthanmanon and E. Keogh, ‚ÄúFast shapelets: a scalable algorithm for \\ndiscovering time series shapelets,‚Äù SDM, 2013. \\n[24] Supporting page. http://www.cs.ucr.edu/~eamonn/MatrixProfile.html \\n[25] C. D. Truong and D. T.  Anh, ‚ÄúAn Efficient Method for Motif and \\nAnomaly Detection in Time Series‚Äù IJBIDM, Vol. 10, No. 4, 2015. \\n[26] A. Tucker and X. Liu, ‚ÄúA Bayesian  Network Approach to Explaining \\nTime Series with Changing Structure,‚Äù Intell Data Anal, 8 (5) (2004). \\n[27] L. Ye and E. Keogh, ‚ÄúTime series shapelets: a new primitive for data \\nmining,‚Äù ACM SIGKDD, 2009, pp 947-56. \\n[28] C. Yoon, O. O‚ÄôReilly, K. Bergen and G. Beroza, ‚ÄúEarthquake detection \\nthrough computationally efficient similarity search,‚Äù Sci. Adv. 2015. \\n[29] F. Zhou, F. Torre and J.  Hodgins ‚Äú Aligned Cluster Analysis for \\nTemporal Segmentation of Human Motion,‚Äù IEEE FG'2008. \\n[30] S. Zilberstein and S. Russell, ‚ÄúApproximate Reasoning Using Anytime \\nAlgorithms,‚Äù In Imprecise and Approximate Computation , Kluwer \\nAcademic Publishers, 1995. \\n \\n0 5,000 10,000\\nMatrix  Profile\\nSplit Points   \\nPrediction\\nOne-dimension of multi-d time series: Subject 86, recording 4, dimension 30\\nW S P N      W C T           D            P         W\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Matrix Profile I: All Pairs Similarity Joins for Time Series: \\nA Unifying View that Includes Motifs, Discords and Shapelets \\nChin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,  \\nHoang Anh Dau, ‚Ä†Diego Furtado Silva, ‚Ä°Abdullah Mueen, and Eamonn Keogh \\nUniversity of California, Riverside, ‚Ä†Universidade de S√£o Paulo, ‚Ä°University of New Mexico \\n{myeh003, yzhu015, lulan001, nbegu001, yding007, hdau001}@ucr.edu, diegofsilva@icmc.usp.br, mueen@unm.edu, eamonn@cs.ucr.edu \\n \\nAbstract‚Äî The all-pairs-similarity-search (or similarity join) \\nproblem has been extensively studied for text and a handful of \\nother datatypes. However, surprisingly little progress has been  \\nmade on similarity joins for time series subsequences. The lack of  \\nprogress probably stems from the daunting nature of the \\nproblem. For even modest sized data sets the obvious nested -loop \\nalgorithm can take months, and the typical speed -up techniques \\nin this domain (i.e., indexing, lower -bounding, triangular -\\ninequality pruning and early abandoning) at best produce one or \\ntwo orders of magnitude speedup. In this work we introduce a \\nnovel scalable algorithm for time series subsequence all -pairs-\\nsimilarity-search. For exceptionally large datasets, the algorithm \\ncan be trivially cast as an anytime algorithm and produce high -\\nquality approximate solutions in reasonable  time. The exact \\nsimilarity join algorithm computes the answer to the time series \\nmotif and time series discord  problem as a side -effect, and our \\nalgorithm incidentally provides the fastest known algorithm for \\nboth these  extensively-studied problems. We de monstrate the \\nutility of our ideas for many time series data mining problems, \\nincluding motif discovery, novelty discovery,  shapelet discovery,  \\nsemantic segmentation, density estimation, and contrast set \\nmining.    \\nKeywords‚ÄîTime Series; Similarity Joins; Motif Discovery \\nI. INTRODUCTION \\nThe all-pairs-similarity-search (also known as similarity \\njoin) problem comes in several variants. The basic task is this: \\nGiven a collection of data objects, retrieve the nearest neighbor \\nfor each object . In the text domain the  algorithm has \\napplications in a host of problems, including community \\ndiscovery, duplicate detection, collaborative filtering, \\nclustering, and query refinement [1]. While virtually all text \\nprocessing algorithms have analogues in time series data \\nmining, there has been surprisingly little progress on Time \\nSeries subsequences All-Pairs-Similarity-Search (TSAPSS). \\nWe believe that this lack of progress stems not from a lack \\nof interest in this useful primitive, but from the daunting nature \\nof the problem. Consider the following example that reflects the \\nneeds of an industrial collaborator. A boiler at a che mical \\nrefinery reports pressure once a minute. After a year, we have a \\ntime series of length 525,600. A plant manager may wish to do \\na similarity self-join on this data with week -long subsequences \\n(10,080) to discover operating regimes (summer vs. winter o r \\nlight distillate vs. heavy distillate etc.) The obvious nested loop \\nalgorithm requires 132,880,692,960 Euclidean distance \\ncomputations. If we assume each one takes 0.0001 seconds, \\nthen the join will take 153.8 days. The core contribution of this \\nwork is to show that we can reduce this time to 6.3 hours, using \\nan off-the-shelf desktop computer. Moreover, we show that this \\njoin can be computed and/or updated incrementally. Thus we \\ncould maintain this join essentially forever on a standard \\ndesktop, even if the data arrival frequency was much faster than \\nonce a minute. \\nOur algorithm uses an ultra -fast similarity search algorithm \\nunder z -normalized Euclidean distance as a subroutine, \\nexploiting the overlap between subsequences using the classic \\nFast Fourier Transform (FFT) algorithm.  \\nOur method has the following advantages/features: \\n\\uf0b7 It is exact, providing no false positives or false dismissals. \\n\\uf0b7 It is simple and parameter -free. In contrast, the more \\ngeneral metric space APSS algorithms require building and \\ntuning spatial access methods and/or hash functions.  \\n\\uf0b7 Our algorithm requires an inconsequential space overhead, \\njust O(n) with a small constant factor. \\n\\uf0b7 While our exact algorithm is extremely scalable, for \\nextremely large datasets we can compute the results in an \\nanytime fashion, allowing ultra-fast approximate solutions.  \\n\\uf0b7 Having computed the similarity join for a dataset, we can \\nincrementally upd ate it very efficiently. In many domains \\nthis means we can effectively maintain exact joins on \\nstreaming data forever.  \\n\\uf0b7 Our method provides full joins, eliminating the need to \\nspecify a similarity threshold, which as we will show, is a \\nnear impossible task in this domain.  \\n\\uf0b7 Our algorithm is embarrassingly parallelizable, both on \\nmulticore processors and in distributed systems. \\nGiven all these features, our algorithm has implications for \\nmany time series data mining tasks [5][18][28].  \\nThe rest of the paper is organized as follows. Section II \\nreviews related work an d introduces the necessary background \\nmaterials and definitions. In Section III we introduce our \\nalgorithm and its anytime and incremental variants. Section IV \\nsees a detailed empirical evaluation of our algorithm and shows \\nits implications for many data mining tasks. Finally, in Section \\nV we offer conclusions and directions for future work. \\nII. RELATED WORK AND BACKGROUND \\nThe basic variant of  similarity join problem we are \\ninterested in is as follows : Given a collection of data objects, \\nretrieve the nearest neighbor for every object.   \\nOther common variants include retrieving the top-K nearest \\nneighbors or the nearest neighbor for each object if that \\nneighbor is within a user-supplied threshold, œÑ. (Such variations \\nare trivial generalizations of our proposed algorithm, so we \\nomit them from further discussion). The latter variant results in \\na much easier problem, provided that the threshold is small. For \\nexample, [1] notes that virtually all research efforts ‚Äú exploit a \\nsimilarity threshold more aggressively in order to limit the set'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='of candidate pairs that are considered..  [or] ... to reduce the \\namount of information indexed in the first place.‚Äù \\nThis critical dependence on œÑ is a major issue for text joins, \\nas it is known that ‚Äú join size can change dramatically \\ndepending on the input similarity threshold ‚Äù [10]. However, \\nthis issue is even more critical for time series for two reasons. \\nFirst, unlike similarity (which is bounded between zero and \\none), the Euclidean distance is effectively unbounded, and \\ngenerally not intuitive. For example, if two heartbeats have a \\nEuclidean distance of 17.1, are they similar? Even for a domain \\nexpert that knows the sampling rate and the noise level of the \\ndata, this is not obvious. Second, a single threshold can produce \\nradically different output sizes, even for datasets that are very \\nsimilar.  Consider Figure 1 which shows the output size vs. \\nthreshold setting for the first and s econd halves of a ten -day \\nperiod monitoring data center chillers [21]. For the first five \\ndays a threshold of 0.6 would return zero items, but for the \\nsecond five days the same setting would return 108 items.  This \\nshows the difficulty in selecting  an appropriate threshold. Our \\nsolution is to have no threshold, and do a full join. \\n \\nFigure 1.  Output size vs. threshold for data center chillers [21]. Values beyond \\n2.0 are truncated for clarity (but archived at [24]).  \\nA handful of efforts have considered joins on time series, \\nachieving speedup by (in addition to the use of MapReduce) \\nconverting the data to lower -dimensional representations such \\nas PAA [11] or SAX [12] and exploiting lower bounds and/o r \\nLocality Sensitive Hashing (LSH) to prune some calculations. \\nHowever, the methods are very complex, with many (10 -plus) \\nparameters to adjust. As [11] acknowledges with admirable \\ncandor, ‚ÄúReasoning about the optimal settings is not trivial.‚Äù In \\ncontrast, our proposed algorithm has zero parameters to set. \\nA very recent research effort [28] has tackled the scalability \\nissue by converting the real -valued time series into discrete \\n‚Äúfingerprints‚Äù before using a LSH approach, much like the text \\nretrieval community [1]. They produced impressive speedup, \\nbut they also experienced false negatives. Moreover, the \\napproach has several parameters that need to be set; for \\nexample, they need to set the threshold to a very precise 0.818.  \\nAs we shall show, our algorithm allows both anytime and \\nincremental (i.e. streaming) versions. While a streaming join \\nalgorithm for text was recently introduced [15], we are not \\naware of any such algorithms for time series data or general \\nmetric spaces. More generally, there is a large amount of \\nliterature on joins for text processing [1]. Such work is \\ninteresting, but of little utility given our constraints, data type \\nand problem setting. We require full joins, not threshold joins, \\nand we are unwilling to allow the possibility of false negatives. \\nA. Definitions and Notation \\nWe begin by defining the data type of interest, time series: \\nDefinition 1:  A time series T is a sequence of real -valued \\nnumbers ti: T = t1, t2, ..., tn where n is the length of T. \\nWe are not interested in the global properties of time series, \\nbut in the similarity between local subsequences:  \\nDefinition 2:  A subsequence Ti,m of a T is a continuous \\nsubset of the values from T of length m starting from \\nposition i.   Ti,m = ti, ti+1,‚Ä¶, ti+m-1, where 1 ‚â§  i ‚â§  n-m+1.  \\nWe can take any subsequence from a time series and \\ncompute its distance to all sequences. We call an ordered vector \\nof such distances a distance profile: \\nDefinition 3:  A distance profile D is a vector of the \\nEuclidean distances between a given query and each \\nsubsequence in an all-subsequences set (see Definition 4).  \\nNote that we are assuming that the distance is measured \\nusing the Euclidean distance between the z -normalized \\nsubsequences [8]. The distance profile can be considered a meta \\ntime series that annotates the time series T that was used to \\ngenerate it. The first three definitions are illustrated in Figure 2. \\n \\nFigure 2. A subsequence Q extracted from a time series T is used as a query to \\nevery subsequence in T. The vector of all distances is a distance profile. \\nNote that if the query and all-subsequences set belong to the \\nsame time series, the distance profile must be zero at the \\nlocation of the query, and close to zero just before and just \\nafter. Such matches are c alled trivial matches in the literature \\n[18], and are avoided by ignoring an exclusion zone (shown as \\na gray region) of m/2 before and after the location of the query. \\nWe are interested in similarity join of all subsequences of a \\ngiven time series. We define an all-subsequences set of a given \\ntime series as a set that contains all possible subsequences from \\nthe time series. The notion of all-subsequences set is purely for \\nnotational purposes. In our implementation, we do not actually \\nextract the subsequences in this form as it would require \\nsignificant time and space overhead. \\nDefinition 4: An all-subsequences set A of a time series T \\nis an ordered set of all possible subsequences of T obtained \\nby sliding a window of length m across T: A ={T1,m,, \\nT2,m,‚Ä¶, Tn-m+1,m}, where m is a user -defined subsequence \\nlength. We use A[i] to denote Ti,m. \\nWe are interested in the nearest neighbor (i.e., 1NN) relation \\nbetween subsequences; therefore, we define a 1NN-join \\nfunction which indicates the nearest neighbor relation between \\nthe two input subsequences. \\nDefinition 5:  1NN-join function :  given two all -\\nsubsequences sets A and B and two subsequences A[i] and \\nB[j], a 1NN-join function  Œ∏1nn (A[i], B[j]) is a Boolean \\nfunction which returns ‚Äútrue‚Äù only if B[j] is the nearest \\nneighbor of A[i] in the set B. \\nWith the defined join function, a similarity join set  can be \\ngenerated by applying the similarity join operator on two input \\nall-subsequences sets. \\nDefinition 6: Similarity join set : given all -subsequences \\nsets A and B, a similarity join set  JAB of A and B is a set \\ncontaining pairs of each subsequence in A with its nearest \\nneighbor in B: JAB={‚å© A[i], B[j] ‚å™ |Œ∏1nn (A[i], B[j])}.  We \\ndenote this formally as JAB = A‚ãà\\uf0711nnB. \\n0\\n400\\n800\\n1200\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\\nFirst 5 Days\\nSecond 5 Days\\nData Center Chillers\\nT, a snippet of an energy \\nconsumption\\n2,0000 m/2m/2\\nQ, query of length m\\nNote that |D| = |T|-|Q|+1D, a distance profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='We measure the Euclidean distance between each pair \\nwithin a similarity join set and store the resultants into an \\nordered vector. We call the result vector the matrix profile. \\nDefinition 7:  A matrix profile (or just profile) PAB is a \\nvector of the Euclidean distances between each pair in JAB. \\nWe call this vector the matrix profile because one \\n(inefficient) way to compute it would be to compute the full \\ndistance matrix of all the subsequences in one time series with \\nall the subsequence in another time series and extract the \\nsmallest value in each row (the smallest non-diagonal value for \\nthe self-join case).  In Figure 3 we show the matrix profile of \\nour running example. \\n \\nFigure 3. A time series T, and its self-join matrix profile P.  \\nLike the distance profile, the matrix profile can be \\nconsidered a meta time series annotating the time series T if the \\nmatrix profile is generated by joining T with itself. The profile \\nhas a host of interesting and exploitable properties. For \\nexample, the highest point on the profile corresponds to the \\ntime series discord [5], th e ( tied) lowest points correspond to \\nthe locations of the best time series motif pair [18], and the \\nvariance can be seen as a measure of the T‚Äôs com plexity. \\nMoreover, the histogram of the values in the matrix profile is \\nthe exact answer to the time series density estimation [4]. \\nWe name this special case of the similarity join set \\n(Definition 6) as self-similarity join set, and the corresponding \\nprofile as self-similarity join profile. \\nDefinition 8:  A self-similarity join  set JAA is a result of \\nsimilarity join of the set  A with itself . We denote this \\nformally as JAA = A ‚ãà\\uf0711nn A. We denote the corresponding \\nmatrix profile or self-similarity join profile as PAA. \\nNote that we exclude trivial matches when self -similarity \\njoin is performed, i.e., if A[i] and A[j] are subsequences from \\nthe same all -subsequences set A, Œ∏1nn (A[i], B[j]) is ‚Äúfalse‚Äù \\nwhen A[i] and A[j] are a trivially matched pair. \\nThe ith element in the matrix profile tells us the Euclidean \\ndistance to the nearest neighbor of the subsequence of T, \\nstarting at i.  However, it does not tell us where that neighbor is \\nlocated. This information is recorded in matrix profile index. \\nDefinition 9: A matrix profile index IAB of a similarity join \\nset JAB is a vector of integers where IAB[i] = j if {A[i], B[j]} \\n‚àà JAB. \\nBy storing the neighboring information this way, we can \\nefficiently retrieve the nearest neighbor of A[i] by accessing the \\nith element in the matrix profile index. \\nNote that the function which computes the similarity join set \\nof two input time series  is not symmetric; therefore, JAB ‚â† JBA,  \\nPAB ‚â† PBA, and IAB ‚â† IBA. \\nFor ease of presentation, we have confined this work to the \\nsingle dimensional case; however, nothing intrinsically \\nprecludes generalizations to multidimensional data. \\nSummary of the Previous Section \\nThe previous section was rather dense, so before moving on \\nwe summarize the main takeaway points. We can create two \\nmeta time series, the matrix profile and the matrix profile index, \\nto annotate a time series TA with the distance and location of all \\nits subsequences nearest neighbors in itself or another time \\nseries TB. These two data objects explicitly or implicitly contain \\nthe answers to many time series data mining tasks. However, \\nthey appear to be too expensive to compute to be practica l. In \\nthe next section we will show an algorithm that can compute \\nthese efficiently. \\nIII. ALGORITHMS \\nWe are finally in a position to explain our algorithm s. We \\nbegin by stating the fundamental intuition, which stems from \\nthe relationship between distance profiles and the matrix \\nprofile. As Figure 2 and Figure 3 visually suggest, all distance \\nprofiles (excluding the trivial match region) are upper bound \\napproximations to the matrix profile. More critically, if we \\ncompute all the distance profiles, an d take the minimum value \\nat each location, the result is the matrix profile! \\nThis tells us that if we have a fast way to compute the \\ndistance profiles, then we also have a fast way to compute the \\nmatrix profile. As we shall show in the next section, we have an \\nultra-fast way to compute the distance profiles. \\nA. Mueen‚Äôs ultra-fast Algorithm for Similarity Search (MASS) \\nWe begin by introducing a novel Euclidean distance \\nsimilarity search algorithm for time series data. The algorithm \\ndoes not just find the nearest neighbor to a query and return its \\ndistance; it returns the distance to every subsequence. In \\nparticular, it computes the distance profile, as shown in Figure \\n2. The algorithm requires just O( nlogn) time by exploiting the \\nFFT to calculate  the dot products between the query and all \\nsubsequences of the time series.  \\nWe need to carefully qualify the claim of ‚Äúultra-fast‚Äù. There \\nare dozens of algorithms for time series similarity search that \\nutilize index structures to efficiently locate neighbors [8]. While \\nsuch algorithms can be faster in the  best case , all these \\nalgorithms degenerate to brute force search in the worst case 1 \\n(actually, much worse than brute force search due to the \\noverhead of the index). Likewise, there are index -free methods \\nthat achieve speed -up using various early abandoning tricks \\n[22], but they too degrade to brute force search in the worst \\ncase. In contrast, the performance of the algorithms outlined in \\nTABLE I and TABLE II is completely independent of the data. \\nTABLE I. CALCULATION OF SLIDING DOT PRODUCTS \\nProcedure SlidingDotProduct(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: The dot product between Q and all subsequences in T \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nn ‚Üê Length(T), m ‚Üê Length(Q) \\nTa ‚Üê Append T with n zeros   \\nQr ‚Üê Reverse(Q)  \\nQra ‚Üê Append Qr with 2n-m zeros \\nQraf ‚Üê FFT(Qra), Taf ‚Üê FFT(Ta) \\nQT ‚Üê InverseFFT(ElementwiseMultiplication(Qraf, Taf)) \\nreturn QT \\n \\n1 There are many such worse case scenarios, including high levels of noise blurring \\nthe distinction between closest and furthest neighbors, and rendering triangular -\\ninequality pruning and early abandoning worthless. \\n2,0000\\nP, a matrix profile\\nT, a snippet of an energy \\nconsumption\\nNote that |P| = |T|-|Q|+1'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Line 1 determines the length of both the time series T and \\nthe query Q. In line 2, we use that information to append T with \\nan equal number of zeros. In line 3, we obtain the mirror image \\nof the original query. This reversing ensures that a convolution \\n(i.e. ‚Äúcrisscrossed‚Äù multiplication) essentially produces in-order \\nalignment. Because we require both vectors to be the s ame \\nlength, in line 4 we append enough zeros to the (now reversed) \\nquery so that, like Ta, it is also of length 2 n. In line 5, the \\nalgorithm calculates Fourier transforms of the appended -\\nreversed query (Qra) and the appended time series Ta. Note that \\nwe use FFT algorithm which is an O(nlogn) algorithm. The Qraf \\nand the Taf produced in line 5 are vectors of complex numbers \\nrepresenting frequency components of the two time series. The \\nalgorithm calculates the element-wise multiplication of the two \\ncomplex vectors and performs inverse FFT on the product. \\nLines 5-6 are the class ic convolution operation on two vectors \\n[7]. Figure 4 shows a toy example of the sliding dot product \\nfunction in work. The algorithm time complexity does not \\ndepend on the length of the query (m). \\n \\nFigure 4. A toy example of convolution operation being used to calculate  \\nsliding dot products for time series data. Note the reverse and append \\noperation on T and q in the input. Fifteen dot products are calculated for every \\nslide. The cells m = 2 to n = 4 from left ( red/bold arrows) contain valid \\nproducts. TABLE II takes this subroutine and uses it to create a distance \\nprofile (see Definition 3). \\nIn line 1 of TABLE II, we invoke the dot products code \\noutlined in TABLE I. The formula to calculate the z-normalized \\nEuclidean distance D[i] between two time series subsequence Q \\nand Ti,m using their dot product, QT[i] is (see [24] for \\nderivation): \\nùê∑[ùëñ] = ‚àö2ùëö(1‚àíùëÑùëá[ùëñ]‚àíùëöùúáùëÑùëÄùëá[ùëñ]\\nùëöùúéùëÑùõ¥ùëá[ùëñ] ) \\nwhere m is the subsequence length, ŒºQ is the mean of Q, \\nMT[i] is the mean of Ti,m, œÉQ is the standard deviation of Q, and  \\nŒ£T[i] is the standard deviation of Ti,m. Normally, it takes O( m) \\ntime to calculate the mean and standard deviation for every \\nsubsequence of a long time series. However, here we exploit a \\ntechnique noted in [22] in a different context. We cache \\ncumulative sums of the values and square of the values in the \\ntime series. At any stage the two cumulative sum vectors are \\nsufficient to calculate the mean an d the standard deviation of \\nany subsequence of any length.  See [14] for an elaborate \\ndescription and variations of MASS. \\nTABLE II. MUEEN‚ÄôS ALGORITHM FOR SIMILARITY SEARCH (MASS) \\nProcedure MASS(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: A distance profile D of the query Q  \\n1 \\n2 \\n3 \\n4 \\nQT ‚Üê SlidingDotProducts(Q, T) \\nŒºQ, œÉQ, ŒúT, Œ£T  ‚Üê ComputeMeanStd(Q, T)    // see [22] \\nD ‚Üê CalculateDistanceProfile(Q, T, QT, ŒºQ, œÉQ, ŒúT, Œ£T) \\nreturn D \\nUnlike the dozens of time series KNN search algorithms in \\nthe literature [8], this algorithm calculates the distance to every \\nsubsequence, i.e. the distance profile  of time series T. \\nAlternatively, in join nomenclature, the algorithm produces one \\nfull row of the all -pair similarity matrix. Thus, as we show in \\nthe next section, our join algorithm is simply a loop that \\ncomputes each full row of the all -pair similarity matrix and \\nupdates the current ‚Äúbest-so-far‚Äù matrix profile when needed. \\nB. The STAMP Algorithm \\nWe call our join algorithm STAMP, Scalable Time series \\nAnytime Matrix Profile. The algorithm is outlined in TABLE \\nIII. In line 1, we extract the length of TB. In line 2, we allocate \\nmemory and initial matrix profile PAB and matrix profile index \\nIAB. From lines 3 to line 6, we calculate the distance profiles D \\nusing each subsequence B[idx] in the time series TB and the \\ntime series TA. Then, we perform pairwise minimum for each \\nelement in D with the paired element in PAB (i.e., min( D[i], \\nPAB[i]) for i = 0 to length(D) - 1.) We also update IAB[i] with idx \\nwhen D[i] ‚â§ PAB[i] as we perform the  pairwise minimum \\noperation. Finally, we return the result PAB and IAB in line 7.  \\nNote that the algorithm presented in TABLE III computes \\nthe matrix profile for the general similarity join. To modify the \\ncurrent algorithm to compute  the self -similarity join matrix \\nprofile of a time series TA, we simply replace TB in line 1 with \\nTA, replace B with A in line 4, and ignore trivial match in D \\nwhen performing ElementWiseMin in line 5. \\nTABLE III. THE STAMP ALGORITHM \\nProcedure STAMP(TA, TB, m) \\nInput: Two user provided time series, TA and TB, interested \\nsubsequence length m \\nOutput: A matrix profile PAB and associated matrix profile index IAB of \\nTA join TB, JAB = A‚ãà\\uf0711nnB \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nnB ‚Üê Length(TB) \\nPAB ‚Üê infs, IAB ‚Üê zeros, idxes ‚Üê 1:nB-m+1 \\nfor each idx in idxes    // In any order, but random for anytime algorithm \\n          D ‚Üê MASS(B[idx], TA) \\n          PAB, IAB ‚Üê ElementWiseMin(PAB, IAB, D, idx) \\nend for \\nreturn PAB, IAB \\nTo parallelize the STAMP algorithm for multicore \\nmachines, we simply distribute the indexes to secondary \\nprocess run in each core, and the secondary processes use the \\nindexes they received to update their own PAB and IAB. Once the \\nmain process returns from  all secondary processes, we use \\nElementWiseMin to merge the received PAB and IAB.  \\nC. An Anytime Algorithm for TSAPSS \\nWhile the exact algorithm introduced in the previous section \\nis extremely scalable, there will always be datasets for which \\ntime needed for an exact solution is untenable. We can mitigate \\nthis by computing the results in an anytime fashion, allowing \\nfast approximate solutions [30]. To a dd the anytime nature to \\nthe STAMP algorithm, we simply ensure a randomized search \\norder in line 2 of TABLE III.  \\nWe can compute a (post-hoc) measurement of the quality of \\nan anytime solution by measuring the Root -Mean-Squared-\\nError (RMSE) between the true matrix profile and the current \\nbest-so-far mat rix profile. As Figure 5 suggests, with an \\nexperiment on random walk data, the algorithm converges very \\nquickly. \\nQ2T1 0 00 00 00 00 0Q1T4Q2T2+Q1T1 Q2T3+Q1T2 Q2T4+Q1T3\\nOutput\\nInputT2T1 T4T3 00 00 Q1Q2 00 00 00'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Figure 5. main) The decre ase in RMSE as the STAMP algorithm updates \\nmatrix profile with the distance profile calculated at each iteration.  inset) The \\napproximate matrix profile at the 10% mark is visually indistinguishable from \\nthe final matrix profile. \\nZilberstein [30] gives a number of desirable properties of \\nanytime algorithms, including Low Overhead , Interruptibility, \\nMonotonicity, Recognizable Quality, Diminishing Returns and \\nPreemptability (these properties are mostly obvious from their \\nnames, but full definitions are at [30]). \\nBecause each subsequence‚Äôs distance profile is bounded \\nbelow by the exact matrix profile, updating an approximate \\nmatrix profile with a distance profile with pairwise minimum \\noperation either drives the approximate solution closer the exact \\nsolution or retains the current approximate solution. Thus , we \\nhave guaranteed Monotonicity. From Figure 5, the approximate \\nmatrix profile converges to the exact matrix profile \\nsuperlinearly; therefore, we have strong Diminishing Returns. \\nWe can easily achieve Interruptibility and Preemptability by \\nsimply inserting a few lines of code between lines 5 and 6 of \\nTABLE III that read: \\n5new \\n6new \\n7new \\nif CheckForUserInterrupt  = TRUE \\n          Report({PAB, IAB}, ‚ÄòHere is an approximate answer.‚Äô) \\nif GetUserChoice = ‚Äòfurther refine‚Äô, CONTINUE, else BREAK \\nThe space and time overhead for the anytime property is \\neffectively zero, thus we have Low Overhead. This leaves only \\nthe property of Recognizable Quality. Here we must resort to a \\nprobabilistic argument.  The convergence curve shown in  \\nFigure 5 is very typical, so we could use past convergence \\ncurves to predict the quality of solution when interrupted on \\nsimilar data.  \\nD. Time and Space Complexity \\nThe overall complexity of the proposed algorithm is \\nO(n2logn) where n is the length of the time series. However, our \\nexperiments (see Section 4.1) empirically suggest that the \\nruntime of  STAMP‚Äôs growth rate is roughly O( n2) instead of \\nO(n2logn). One possible e xplanation for this is that the nlogn \\nfactor comes from the FFT subroutine. Because FFT is so \\nimportant in many applications, it is extraordinarily well \\noptimized. Thus, the empirical runtime is very close to linear. \\nIn contrast to the above, the brute for ce nested loop \\napproach has a time complexity of O(n2m). Recall the industrial \\nexample in the introduction section. We have  m = 10,080, but \\nlog(n) = 5.7, so we would expect our approach to be about \\n1,768 times faster. In fact, we are empirically even faster. The \\ncomplexity analysis downplays the details of important constant \\nfactors. The nested loop algorithm must also z -normalize the \\nsubsequences. This either requires O( nm) time, but with an \\nuntenable O(nm) space overhead, or an O( n2m) time overhead. \\nAnd r ecall that this is before a single Euclidean distance \\ncalculation is performed.  \\nFinally, we mention one quirk of our algorithm which we \\ninherit from using the highly optimized FFT subroutine. Our \\nalgorithm is fastest when n is an integer power of two, slower \\nfor non -power of two but composite numbers, and slowest \\nwhen n is prime. The difference (for otherwise similar values of \\nn) can approach a factor of 1.6x. Thus, where possible, it is \\nworth contriving the best case by tru ncation or zero-padding to \\nthe nearest power of two. \\nE. Incrementally Maintaining TSAPSS \\nUp to this point we have discussed the batch version of \\nTSAPSS. By batch, we mean that the STAMP algorithm needs \\nto see the entire time series TA and TB (or just TA if we are \\ncalculating the self -similarity join matrix profile) before \\ncreating the matrix profile. However, it would be advantageous \\nif we could build the matrix profile incrementally. Given that \\nwe have performed a batch construction of matrix profile, i f \\nnew data arrives, it would clearly be preferable to incrementally \\nadjust the current profile, rather than start from scratch.  \\nBecause the matrix profile solves both the times series motif \\nand the time series discord problems, an incremental version of \\nSTAMP would automatically provide the first incremental \\nversions of both algorithms. We call such an algorithm the \\nSTAMPI (STAMP Incremental) algorithm. \\nWe will demonstrate our ability to incrementally maintain \\nthe matrix profile in this section. For simpli city and brevity \\nTABLE IV only shows the algorithm to maintain the self -\\nsimilarity join. The generalizations are obvious. \\nTABLE IV. THE STAMPI ALGORITHM \\nProcedure STAMPI(TA, t, PAA, IAA) \\nInput: The original time series TA, a new data point t following TA, the \\nmatrix profile PAA and its associated matrix profile index IAA of TA.  \\nOutput: The incrementally updated matrix profile PAA,new and its matrix \\nprofile index IAA,new of the current time series TA,new= TA, t. \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nTA,new = [TA, t] \\nS ‚Üê last subsequence in TA,new, idx ‚Üê index of S in TA,new \\nD ‚Üê MASS (S, TA) \\nPAA, IAA ‚Üê ElementWiseMin(PAA, IAA, D, idx) \\npAA,last, iAA,last ‚Üê FindMin(D) \\nPAA,new ‚Üê [PAA, pAA,last], IAA,new ‚Üê [IAA, iAA,last] \\nreturn PAA,new, IAA,new \\nFor clarity, we denote the updated time series as TA,new, the \\nupdated matrix profile as PAA,new and the associated matrix \\nprofile index as IAA,new. As each additional data point t arrives, \\nthe size of the time series TA increases by one, and a new \\nsubsequence S is generated at the end of TA,new. In line 3 we \\nobtain the distance profile of S with regard to TA. Then, as in the \\noriginal STAMP algorithm,  in line 4 we perform  a pairwise \\ncomparison between every element in D with the corresponding \\nelement in PAA to see if the corresponding element in PAA needs \\nto be updated. In line 5, we find the nearest neighbor of S and \\nthe associated index by evaluating the minimum value of D. \\nFinally, in line 6, we obtain the new matrix profile and \\nassociated matrix profile index by concatenating the results in \\nline 4 and line 5.  \\nThe time complexity of the STAMP I algorithm is O(nlogn) \\nwhere n is the length of size of th e current time series TA. Note \\nthat as we maintain the profile, each incremental call of \\nSTAMPI requires invoking the FFT subroutine with a slightly \\nlonger time series ( n becomes n+1 ). Thus it gets very slightly \\nslower at each time step. Therefore, the best way to measure the \\nperformance is to ask for the Maximum Time Horizon (MTH), \\nin essence the answer to this question: ‚Äú Given this arrival rate, \\nhow long can we maintain the profile before we can no longer \\nupdate fast enough?‚Äù \\nNote that the subsequence length m is not considered in the \\nMTH evaluation. Recall that overall time complexity of the \\n10,0000\\nRMSE\\niteration1,000\\napproximate matrix \\nprofile at 1,000 iterations.\\nexact matrix profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='algorithm is determined by the efficiency of the FFT \\nsubroutine, which is independent of m. We have computed the \\nMTH for two common scenarios of interest to the community. \\n\\uf0b7 House Electrical Demand  [19]: This dataset is updated \\nevery eight seconds. By iteratively calling the STAMP I \\nalgorithm, we can maintain the profile for 5.8 years.  \\n\\uf0b7 Oil Refinery :  Most telemetry in oil refineries is sampled \\nat once a minute [26]. The relatively low sampling rate \\nreflects the ‚Äúinertia‚Äù of massive boilers/condensers. Even if \\nwe maintain the profile for 40 years, the update time is only \\naround 6.78 seconds. Moreover, th e raw data, matrix \\nprofile and index would only require 0.5 gigabytes of main \\nmemory. Thus the MTH here is forty plus years. Given \\nprojected improvements in hardware, this effectively \\nmeans we can maintain the matrix profile forever. \\nAs impressive as these  numbers are, they are actually quite \\npessimistic. For simplicity we assume that every value in the \\nmatrix profile index will be updated at each time step.  \\nHowever, empirically, much less than 0.1% of them need to be \\nupdated. If it is possible to prove an upper bound on the number \\nof changes to the matrix profile index per update, then we could \\ngreatly extend the MTH  or handle much faster sampling rates. \\nWe leave such considerations for future work. \\nIV. EXPERIMENTAL EVALUATION \\nWe begin by stating our experimen tal philosophy. We have \\ndesigned all experiments such that they are easily reproducible. \\nTo this end, we have built a webpage [24] which contains al l \\ndatasets and code used in this work, together with spreadsheets \\nwhich contain the raw numbers and some supporting videos.  \\nGiven page limits, and the utility of our algorithm for a host \\nof existing (and several new) data mining tasks, we have chosen \\nto c onduct exceptionally broad  but shallow experiments. We \\nhave conducted such deep detailed experiments and placed \\nthem at [24]. Unless otherwise state d we measure wall clock \\ntime on an Intel i7@4GHz with 4 cores. \\nA. Scalability of Profile-Based Self-Join \\nBecause the time performance of STAMP is independent of \\nthe data quality or any user inputs (there are none except the \\nchoice of m, which does not affect the speed), our scalability \\nexperiments are unusually brief.  In TABLE V we show the \\ntime required for a self -join with m fixed to 256, for \\nincreasingly long time series.  \\nTABLE V.  TIME REQUIRED FOR A SELF-JOIN WITH M = 256, VARYING N \\nValue of n 217 218 219 220 221 \\nTime Required 15.1 min 70.4 min 5.4 hours 24.4 hours 4.2 days \\nIn TABLE VI, we show the time required for a self -join \\nwith n fixed to 2 17, for increasing long m. Again recall that \\nunlike virtually all other time series data mining algorithms in \\nthe literature whose performance degrades for longer \\nsubsequences [8][18], the running time of STAMP does not \\ndepend on m. \\nTABLE VI. TIME REQUIRED FOR A SELF-JOIN WITH N= 217, VARYING M \\nValue of m 64 128 256 512 1,024 \\nTime Required 15.1 min 15.1 min 15.1 min 15.0 min 14.5 min \\nFinally, we further exploit the simple parallelizability of the \\nalgorithm by using four 16 -core virtual machines on Microsoft \\nAzure to redo the two -million join ( n = 2 21 and m = 256) \\nexperiment. By scaling up the computational power, we have \\nreduced the running time from 4.2 days to just 14.1 hours. This \\nuse of cloud computing required writing just few dozen lines of \\nsimple additional code [24]. \\nIt is difficult to find good baselines to compare to. We \\nbelieve STAMP is the only algorithm that does full, exact joins \\non time series subsequences. Most algorithms do only threshold \\njoins and/or do approximate joins, and are not specialized for \\ntime series subsequences. However, we searched for the best \\nbaselines and made the following concessions to  the rival \\nalgorithms to allow comparisons. \\n\\uf0b7 TSFRDAA [11]: While STAMP returns all nearest \\nneighbors, we adjust the threshold (the selectively) of \\nTSFRDAA such that only needs to return the top 1% nearest \\nneighbors, a much easier task. \\n\\uf0b7 HDSJI-SAX [12]: This method  allows false negatives ; we \\nignore this. It needs time to build indexes ; we do not count \\nthis time, and as above, we allow it to return  the only the \\ntop 1% nearest neighbors ( as bef ore, not the 100% the \\nSTAMP returns, and therefore a much easier task.). \\n\\uf0b7 Optimized Nested Loop (ONL): Here we use a nested -loop \\njoin optimized in the following manner. We use a state -of-\\nthe-art DFT indexing technique to do the search in the \\ninner loop, and we do not count the time needed to build \\nthe indexes [8]. We carefully adjust  parameters for best \\nperformance.  \\nWe consider the first 2 18 data points of the ECG dataset \\nused in [22], with a query length of 256, about one heartbeat ; \\nTABLE VII shows the results.  \\nTABLE VII. TIME FOR A SELF-JOIN WITH M = 256, VARYING ALGORITHMS \\nAlgorithm TSFRDAA HDSJI-SAX ONL STAMP \\nTime Required 51.7 hours 19.6 min 28.1 hours 1.17 hours \\nAs these results show, even if we ignore the limitations of \\nthe baseline methods, STAMP is still significantly faster. \\nB. Profile-Based Self-Join \\nA recent paper notes that many fundamental problems in \\nseismology can be solved by joining seismometer telemetry \\n[28], including the disc overy of foreshocks, aftershocks, \\ntriggered earthquakes, volcanic activity and induced seismicity. \\nHowever, the paper notes a join with a query length of 200 on a \\ndata stream of length 604,781 requires 9.5 days. Their solution, \\na clever transformation of t he data to allow LSH based \\ntechniques, does achieve significant speedup, but at the cost of \\nfalse negatives and the need for significant parameter tuning.  \\nThe authors kindly shared their data, and, as we hint at in \\nFigure 6, confirmed that STAMP does not have false negatives. \\n \\nFigure 6. top) An excerpt of a seismic time series aligned with its matrix \\nprofile (bottom). The ground truth provided by the authors of [28] requires that \\nthe events occurring at time 4,050 and 7,800 match. \\nWe repeated the n = 604,781, m = 200 experime nt and \\nfound it took just 8.9 hours to finish. As impressive as this is, in \\nthe next section we show that we can do even better.   \\n4,000 5,000 6,000 7,000 8,000 9,000\\n0\\n5\\n10\\n15\\nSeismic time series (excerpt) \\nMatrix Profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='C. The Utility of Anytime STAMP \\nThe seismology dataset offers an excellent opportunity to \\ndemonstrate the utility of the anytime version of our algorithm. \\nThe authors of [28] revealed in their long -term ambition of \\nmining even larger datasets [3]. In Figure 7 we repeated the \\nexperiment with the snippet shown in Figure 6, this time \\nreporting the best-so-far matrix profile reported by the \\nalgorithm at various milestones. Even with just 0.25% of the \\ndistances computed (that is to say, 400 times faster) the correct \\nanswer has emerged. Thus, we can provide the correct answers \\nto the seismologists in just minutes, rather than the 9.5 days.  \\n \\nFigure 7. top) An excerpt of the seismic data that is also shown in Figure 6. \\ntop-to-bottom) The approximations of the matrix profile for increasing \\ninterrupt times. By the time we have computed just 0.25% of the calculations \\nrequired, the minimum of the matrix profile points to the ground truth. \\nTo show the generality of this anytime feature of STAMP, \\nwe consider a very different dataset. As shown in Figure \\n8.inset), it is possible to convert DNA to a time ser ies [22]. We \\nconverted the Y-chromosome of the Chimpanzee this way. The \\nresulting time series is  little over one -million in length. We \\nperformed a self-join with m = 60,000.  Figure 8.bottom shows \\nthe best motif is so well conserved (ignoring the first 20%), that \\nit must correspond to a recent (in evolutionary tim e) gene \\nduplication event. In fact, in a subsequent analysis we \\ndiscovered that ‚Äúmuch of the Y (Chimp chromosome) consists of \\nlengthy, highly similar repeat units, or ‚Äòamplicons‚Äô‚Äù [9]. \\n \\nFigure 8. top) The Y -chromosome of the Chimp in time series space with its \\nmatrix profile. bottom) A zoom -in of the top motif discovered using anytime \\nSTAMP, we believe it to be an amplicon [9]. \\nThis demanding join would take just over a day of CPU \\ntime (see TABLE V). However, using anytime STAMP we \\nhave the result shown above after doing just 0.021% of the \\ncomputations, in about 18 seconds. At [24] we have videos that \\nshow the rapid convergence of the anytime variant of STAMP. \\nD. Profile-Based Similarity Join Set \\nIn this section we show two uses of similarity join set. The \\nfirst use is more familiar to APSS users, quantifying what is \\nsimilar between two time series. The second example, \\nquantifying what is different between two time series, is novel  \\nand can only be supported by threshold -free algorithms that \\nreport the nearest neighbor for all objects. \\nTime Series Set Similarity \\nGiven two (or more) time series collected under different \\nconditions or treatments, a data analyst may wish to know what \\npatterns (if any) are conserved between the two time series. To \\ndemonstrate the utility of automating this, we consider a simple \\nbut in tuitive example. Figure 9 shows the raw audio of two \\npopular songs converted to Mel Frequency Cepstral \\nCoefficients (MFCCs). Specifically, the songs used in this \\nexample are ‚ÄúUnder Pressure‚Äù by Queen an d David Bowie and \\n‚ÄúIce Ice Baby ‚Äù by the American rapper Vanilla Ice. Normally, \\nthere are 13 MFCCs; here, we consider just one for simplicity. \\n \\n \\nFigure 9. Two songs represented by just the 2 nd MFCC at 100Hz. We \\nrecognize that it is difficult to see any structure in these times series; however, \\nthis difficulty is the motivation for this experiment. \\nEven for these two relatively short time series, visual \\ninspection does not offer immediate answers. The problem here \\nis compounded by the size reproduction, but is not significantly \\neasier with large-scale [24] or interactive graphic tools. We ran \\nJAB (Queen-Bowie, Vanilla Ice) on these datasets with m = 500 \\n(five seconds), the best match, corresponding the minimum \\nvalue of the matrix profile PAB, is shown in Figure 10. \\n \\nFigure 10. The result of JAB (Queen-Bowie (red/bold), Vanilla-Ice (green/fine)) \\nproduces a strongly conserved five second region. \\nReaders may know the cause for this highly conserved \\nsubsequence. It corresponds to the famous baseline of ‚ÄúUnder \\nPressure,‚Äù which was sampled (plagiarized) by Vanilla Ice in \\nhis song. The join took 21.9 seconds. The ability to find \\nconserved structure in apparently disparate time series could \\nopen many avenues of research in medicine and industry. \\nTime Series Difference \\nWe introduce the Time Series Diff (TSD) operator, which \\ninformally asks ‚ÄúWhat happens in time series T A, that does not \\nhappen in time series TB?‚Äù Here TA/TB could be an ECG before \\na drug is administered/after a drug is administered, telemetry \\nbefore a successful launch/before a catastrophic launch etc. The \\nTSD is simply the subsequence referred to by the maximum \\nvalue of the JAB join‚Äôs profile PAB. \\nWe begin with a simple intuitive example. The UK and US \\nversions of the Harry Potter audiobook series are performed by \\ndifferent narrators, and have a handful of differences in the text. \\nFor example, the UK version contains: \\nHarry was passionate about Quidditch. He had played as Seeker on the Gryffindor \\nhouse Quidditch team ever since his first year at Hogwarts and owned a Firebolt, one \\nof the best racing brooms in the world... \\nBut the corresponding USA version has: \\nHarry had been on the Gryffindor House Quidditch team ever since his first year at \\nHogwarts and owned one of the best racing brooms in the world, a Firebolt. \\nAs shown in Figure 11, we can convert the audio \\ncorresponding to these snippets into MFCCs and invoke a JAB \\njoin set to produce a matrix profile PAB that represents the \\ndifferences between them. As Figure 11.left shows, the low \\nvalues of this profile correspond to identical spoken phrases \\n(despite having two different narrators). However here we are \\n4,000 5,000 6,000 7,000 8,000 9,000\\ndata\\n0.25%\\n1%\\n100%\\n0 1,000,0000\\n100\\n200\\nPan troglodytes Y-chromosome \\n0 60,000\\n12,749,475 to 14,249,474 bp\\n622,725 to 2,122,724 bp\\nT1 = 0;\\nfor i = 1 to length(chromosome) \\nif chromosomei = A, then Ti+1 = Ti + 2 \\nif chromosomei = G, then Ti+1 = Ti + 1\\nif chromosomei = C, then Ti+1 = Ti - 1 \\nif chromosomei = T, then Ti+1 = Ti - 2 \\nend\\n0\\nQueen-Bowie\\n1,000 2,000\\n-10\\n0\\n10\\nVanilla Ice\\n0 250 500-3\\n-2\\n-1\\n0\\n1\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='interested in the differences, the maximum value of the profile. \\nAs we can see in Figure 11.right, here the profile corresponds \\nto a phrase unique to the USA edition.  \\n \\nFigure 11. The 2 nd MFCC of snippets from the USA ( pink/bold) and UK \\n(green/fine) Harry Potter audiobooks.  The JAB join of the two longer sections \\nin the main text produce s mostly small values in the profile correspond to the \\nsame phrase (left), the largest value in  the profile corresponds to a phrase \\nunique to the USA edition (right). \\nThe time required to do this is just 0.067 seconds, much \\nfaster than real time. While this demonstration is trivial, in [24] \\nwe show an example applied to ECG telemetry.  \\nE. Profile-Based Motif Discovery \\nSince their introduction in 2003, time series motifs have \\nbecome one of the most frequently used primitives in time  \\nseries data mining, with applications in dozens of domains [2]. \\nThere are several proposed definitions for time series motifs, \\nbut in [18] it is argued that if you can solve the most basic \\nvariant, the closest (non -trivial) pair of subsequences, then all \\nother variants only require some minor additional calculations. \\nNote that the locations of the two (tying) minimum values of \\nthe matrix profile are exactly the locations of the 1st motif pair. \\nThe fastest known exact algorithm for computing time \\nseries motifs is the MK algorithm [18]. Note, however, that this \\nalgorithm‚Äôs time performance depends on the time series itself. \\nIn contrast, the Profile -Based Motif Discovery (PBMD) takes \\ntime independent of the data. To see this, we compared the two \\napproaches on an electrocardiogram of length 65,536. In Figure \\n12.left we ask what happens as we search for lon ger and longer \\nmotifs. In Figure 12.right we ask what happens if the motif \\nlength is fixed to m = 512, but the data becomes increasing \\nnoisy.  \\n \\nFigure 12. The time required to find the top -motif pairs in a time series of \\nlength 216 for increasingly long motif lengths ( left), and for a length fixed to \\n512, but in the face of increasing noise levels (right). \\nThese results show that  even in the best case for MK, \\nPBMD is competitive, but as we have longer queries and/or \\nnoisier data, its advantage becomes unassailable. Moreover, \\nPBMD inherits STAMP‚Äôs anytime and incremental \\ncomputability, and is easily parallelizable.  \\nF. Profile-Based Discord Discovery \\nA time series discord  is the subsequence that has the \\nmaximum distance to its nearest neighbor. While this is a \\nsimple definition, time series discords are known to be very \\ncompetitive as novelty/anomaly detectors  [5]. Note that as \\nshown in Figure 13, the time series discord is encoded as the \\nmaximum value in a matrix profile. \\n \\nFigure 13. top) An excerpt from an ECG incorporating a premature ventricular \\ncontraction (red/bold). bottom) The time series profile peaks exactly at the \\nbeginning of the PVC. \\nThe time taken to compute the discord  is obviously just the \\ntime needed to compute the matrix profile ( here, 0.9 seconds). \\nThere are a few dozen discord discovery algorithms in the \\nliterature. Some of them may be competitive in the best case, \\nbut just like motif -discovery algorithms they all degenerate to \\nbrute force search in the worst case, and none allow the anytime \\nproperties that we inherit from using STAMP.  \\nG. Incrementally Maintaining Motifs and Discords \\nWe have demonstrated the ability to detect time series \\nmotifs and discords using the matrix profile in the previous two \\nsections. However, we assumed that the entire time series was \\navailable beforehand. Here we remove this assumption and \\nshow how STAMP I allows us to incrementa lly maintain time \\nseries motifs/ discords in an online fashion. There are other \\nattempts at one [20][2] or both [25] of these tasks, but they are \\nall approximate and allow false dismissals.   \\nIn Section III.E, we introduced the STAMPI algorithm. The \\nability to incrementally maintain the matrix profile implies the \\nability to exactly maintain the time series motif [18] and/or time \\nseries discord [5] in streaming data. We simply need to keep \\ntrack of the extreme values of the incrementally-growing matrix \\nprofile, report a new pair of motifs when a new minimum value \\nis detected, and report a new discord when we see a new \\nmaximum value. \\nWe demonstrate the utility of these ideas on the AMPds \\ndataset [13]. Here the kitchen fridge and the heat pump are both \\nplugged into a single metered power supply. For the first week, \\nonly the refrigerator is running. At the end of the week, the \\nweather gets cold and the heat pump is turned on. The sampling \\nrate is one sample/m inute, and th e subsequence length is 100. \\nWe apply the STAMP algorithm to the first three days of data, \\nthen invoke STAMP I to handle newly arriving data , report an \\nevent when we detect a new extreme value. \\nOur first event occurs at the 9,864 th minute (6 da y 20 hour \\n24 minute). As shown in Figure 14, a new minimum value is \\ndetected, which indicates a new time series motif. The just -\\narrived 100 -minute-long pattern looks v ery similar to another \\npattern that occurred five hours earlier. While there is a lot of \\nregularity in the fridge data in general, the exceptional \\nsimilarity observed here suggested some underlying physical \\nmechanism that caused such a perfectly -conserved pattern, \\nperhaps a mechanical ice-making ‚Äúsubroutine.‚Äù \\n \\nFigure 14. top) The matrix profile of the first 9,864 minutes of data. bottom) \\nThe minimum value of the matrix profile corresponds to a pair of time series \\nmotifs in the power usage data. right) The time series motif detected. \\nOur second event occurs at the 10,473 th minute (7 day 6 \\nhour 33 minute). As shown in Figure 15, a new maximum value \\n‚Ä¶indor house Quidditch team ever since his first ye‚Ä¶\\nHarry had been on the Gryffindor House Quidditch te..\\nsince his first year at Hogwarts and owned a Fire..\\nsince his first year at Hogwarts and owned on..\\nED = 2.9 \\nClosest Match \\n0 100(1.6 seconds) 0 100\\nED = 10.4\\n(1.6 seconds)\\nFurthest Match (Time Series Difference) \\n256 512 1,024 2,048 4,096\\n0\\n40\\n80\\n120\\n160\\n200\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nSubsequence Length  (m)\\nTime Taken (min)\\n80 40 0\\n0\\n70\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nNoise Added (dB)  \\n0 1000 2000 3000\\nECG qtdb\\nSel102\\n(excerpt)\\nPremature Ventricular \\nContraction\\nTime Series Profile\\n0 5,000 10,000\\n0 100\\nnew minimum value\\nnew motifMatrix Profile\\nPower Usage Data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='is detected, which indicates a new time series discord. The time \\nseries discord corresponds to the first occurrence of a heat \\npump pattern in the power usage data.  \\n \\nFigure 15. top) The matrix profile for the first 10,473 minutes. bottom) The \\nmaximum value of the matrix profile corresponds to a time series discord. \\nright) The time series discord detected is the first  heat pump pattern \\noccurrence in the dataset. \\nThe maximum time needed to process a single data point \\nwith STAMP I in this dataset is 0.005 second s, which is less \\nthan 0.01% of the data sampling rate. Thus, on this dataset we \\ncould continue monitoring with the STAMP I algorithm for \\nseveral decades before running out of time or memory. \\nH. Profile-Based Shapelet Discovery \\nShapelets are time series subsequences which are in some \\nsense the maximally representative of a class [23][27]. \\nShapelets can be used to classify time series (essentially, the \\nnearest shapelet algorithm), offering the benefits of speed, \\nintuitiveness and at least on some domains, significantly \\nimproved accuracy [23]. However, these advantages come at \\nthe cost of a very expensive training phase, with O( n2m4) time \\ncomplexity, where m is the length of the longest time series \\nobject in the dataset, and n is the number of objects in the \\ntraining set. In order to mitigate this high ti me complexity, \\nresearchers have proposed various distance pruning techniques \\nand candidate ranking approaches for both the admissible [27] \\nand appro ximate [23] shapelet discovery. Nevertheless, \\nscalability remains the bottleneck.  \\nBecause shapelets are essentially supervised motifs, and we \\nhave shown that STAMP can find motifs very quickly, it is \\nnatural to ask if STAMP has implications for shapelet \\ndiscovery. While space limitations prohibit a detailed \\nconsideration of this question, we briefly sketch out and test \\nthis possibility as follows. \\nAs shown in Figure 16, we can use matrix profile to \\nheuristically ‚Äúsuggest‚Äù candidate shapelets. We consider two \\ntime series TA (green/bold) and TB (pink/light) with class 1 and \\nclass 0 being their corresponding class label, and we take  JAB, \\nJAA, JBA and JBB. Our claim is the differences in the heights of \\nPAB, PAA (or PBA, PBB) are strong indicators of good candidate \\nshapelets. The intuition is that if a discriminative pattern is \\npresent in say, class 1, but not in class 0, then we expect to see a \\n‚Äúbump‚Äù in the PAB (the intuition holds if the order is reversed). \\nA significant difference (quantified by a threshold shown in \\ndashed line) between the heights of PAA and PAB curves \\ntherefore indicates the occurrence of good candidate shapelets, \\npatterns that only occur in one of the two classes. \\n \\nFigure 16. top.left) Two time series TA and TB formed by concatenating \\ninstances of class 1 and 0 respectively of ArrowHead [6]. bottom) The height \\ndifference between PAB (or PBA) and PAA (or PBB) are suggestive of  good \\nshapelets. top.right) An example of good shapelet extracted from class 1. \\nThe time taken to compute  all four matrix profiles is 1.0 \\nseconds and the time to further evaluate the two twelve \\ncandidates selected takes 2.7 seconds. On the same machine, \\nthe brute force shapelet classifier takes 4.2 minutes with 2,364 \\ncandidates. Note that, in this toy demonst ration, the speedup is \\n68X, however, for larger datasets, the speedup is greater [24]. \\nI. Profile-Based Semantic Segmentation \\nThe goal of time series semantic segmentation is to partition \\na dataset containing multiple activities/regimes into atomic \\nbehaviors or conditions. For example, for human activity data \\nthe regimes might later be mapped to { eating, working, \\ncommuting,...}[29]. As this example suggests, most work in this \\narea is highly domain -dependent. In contrast, here we show \\nhow the matrix profile index can be emp loyed for domain \\nagnostic time series segmentation.  \\nThe intuition of our approach is as follows. Within a single \\nregime we might expect that most subsequences will have a \\nnearest neighbor close by (in time). For example, consider the \\ntoy problem shown in Figure 17 which shows two obvious \\nregimes. We would expect that the nearest neighbor to the first \\nrun gait cycle is the second or third or fourth run cycle, but it \\nwill almost certainly not be one of the walk cycles. In general, \\nthis tendency for nearest neighbor pointers not to cross the \\nboundaries corresponding to regime changes may be sufficient \\nto discover these boundaries, and of course, this is precisely the \\ninformation that is encoded in the matrix profile index. \\nThus, for each point we count how many ‚Äúarcs‚Äù connecting \\ntwo nearest neighbors cross it if we connect each subsequence \\nto its nearest neighbor as shown in Figure 17. \\n \\nFigure 17. top) A (toy) time series ( red) and nearest neighbor locations for \\neach subsequence. bottom) The number of arcs crossing above each \\nsubsequence. \\nWe applied the procedure described above to a heavily \\nstudied activity segmentation problem [29], which was derived \\nfrom the CMU Motion Capture data base [16]. The recordings \\nare represented as multi -dimensional time series, and most \\nresearch efforts carefully select the best subset for the \\nsegmentation task. For example, [29] states that ‚Äú we only \\nconsider the 14 most informative joints out of 29 .‚Äù In contrast, \\nwe attempt this with a single dimension. The only parameter we \\nneed to set is sliding window size . In Figure 18 we show the \\nsegmenting results obtained using our approach , with  \\nannotations taken from [29] for context. \\nMuch of the evaluation in this community lacks formal \\nmetrics, preferring instead visual sanity tests like the one in \\nFigure 18. Given this, we can say that our approach is very \\ncompetitive on this dataset, in spite of the fact that we \\nhandicapped ourselves to only consider one dimension.  \\n0 100\\n0 10,0005,000\\nMatrix Profile\\nPower Usage Data\\nnew maximum value\\nnew discord\\n0\\n7\\n0 1,400\\nPABPAA\\n0 1,600\\nTA\\nTB\\n0 300\\nAn example of good shapelet\\n0 1,400\\nPBA\\nPBB\\n8\\n0\\n0 1,400\\nDiff(PAB ,PAA) Threshold\\n-1\\n4\\n-2\\n4\\n0 1,400\\nDiff(PBA ,PBB) Threshold\\n0\\n1\\n2\\n1 2 0\\nwalking slow    walking slow     run       run run run\\nNumber of arcs intersecting each point'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content=\"Figure 18. top) A matrix profile ( blue) obtained for the time series ( red) and \\nnumber of arcs crossing each point ( green). Low values of this green curve \\ncoorespond to candidate split points. bottom) Human annotations of the \\nactivities: w ‚Äì walk, s ‚Äì stretch, p ‚Äì punch, c ‚Äì chop, t ‚Äì turn, d ‚Äì drink. \\nV. CONCLUSION \\nWe have introduced a scalable algorithm for creating time \\nseries subsequences joins. Our algorithm is simple, fast, \\nparallelizable and parameter -free, and can be in crementally \\nupdated for moderately fast data arrival rates. We have shown \\nthat our algorithm has implications for many existing tasks, \\nsuch as motif discovery, discord discovery, shapelet discovery \\nand semantic segmentation, and may open up new avenues for  \\nresearch, including computing various definitions of time series \\nset difference. Our code is freely available for the community to \\nconfirm, extend and exploit our ideas. \\nREFERENCES \\n[1] R. J. Bayardo, Y. Ma and R. Srikant, ‚ÄúScaling up all pairs similarity \\nsearch,‚Äù WWW 2007, pp 131-140. \\n[2] N. Begum and E. Keogh, ‚ÄúRare time series motif discovery from \\nunbounded streams,‚Äù PVLDB 8(2): 149-160, 2014. \\n[3] G. Beroza, ‚ÄúPersonal Correspondence,‚Äù Jan 21th, 2016. \\n[4] T. Bouezmarni and J. Rombouts, ‚ÄúNonparametric density estimation for \\npositive time series,‚Äù CSDA, 54, 245-261, 2010.  \\n[5] V. Chandola, D. Cheboli and V. Kumar, ‚ÄúDetecting anomalies in a time \\nseries database,‚Äù UMN TR09-004. \\n[6] T. Chen  et al ., ‚ÄúThe UCR time series classification archive,‚Äù \\nhttp://www.cs.ucr.edu/~eamonn/time_series_data/. \\n[7] ‚ÄúConvolution - Wikipedia, the free encyclopedia,‚Äù  \\nhttps://en.wikipedia.org/wiki/Convolution, Accessed: 2016-01-19. \\n[8] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang and E. J. Keogh, \\n‚ÄúQuerying and mining of time series data: experimental comparison of \\nrepresentations and distance measures,‚Äù PVLDB 1(2): 1542-1552. 2008. \\n[9] J. Hughes et al. , ‚ÄúChimpanzee and human Y chromosomes are \\nremarkably divergent in structure‚Äù Nature 463, (2010). \\n[10] H. Lee, R. Ng and K. Shim, ‚ÄúSimilarity join size estimation using \\nLocality sensitive hashing,‚Äù PVLDB, 4(6):338‚Äì349, 2011. \\n[11] W. Luo, H. Tan, H. Mao  and L. M.  Ni, ‚ÄúEfficient similarity joins on \\nmassive high-dimensional datasets using mapreduce,‚Äù In MDM'12, IEEE \\nComputer Society, pp. 1-10. \\n[12] Y. Ma, X. Meng and S. Wang, ‚ÄúParallel similarity joins on massive high-\\ndimensional data using MapReduce ,‚Äù Concurrency and Computation , \\nVolume 28, Issue 1 Jan 2016. Pages 166‚Äì183. \\n[13] S. V. Makonin, ‚ÄúAMPds: a public dataset for load disaggregation and \\neco-feedback research,‚Äù EPEC 2013, pp 1-6. \\n[14] MASS: http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html \\n[15] G. D. F. Morales and A. Gionis, ‚Äústreaming similarity self-join,‚Äù Proc. \\nVLDB Endow, 2016. \\n[16] Motion Capture Database, http://mocap.cs.cmu.edu/ \\n[17] A. Mueen, H. Hamooni and T. Estrada, ‚ÄúTime series join on subsequence \\ncorrelation,‚Äù IEEE ICDM 2014, pp. 450-459. \\n[18] A. Mueen, E. Keogh, Q. Zhu, S. Cash and B. Westover, ‚ÄúExact discovery \\nof time series motif,‚Äù SDM 2009. \\n[19] D. Murray et al. , ‚ÄúA data management platform for personalised real-\\ntime energy feedback,‚Äù In EEDAL 2015. \\n[20] V. Niennattrakul et al, ‚ÄúData editing techniques to allow the application \\nof distance-based outlier detection to streams,‚Äù ICDM 2010: 947-952. \\n[21] D. Patnaik, et al, ‚ÄúSustainable operation and management of data center \\nchillers using temporal data mining,‚Äù KDD 2009. \\n[22] T. Rakthanmanon et al., ‚ÄúSearching and Mining Trillions of Time Series \\nSubsequences under Dynamic Time Warping,‚Äù In KDD 2012, 262-270. \\n[23] T. Rakthanmanon and E. Keogh, ‚ÄúFast shapelets: a scalable algorithm for \\ndiscovering time series shapelets,‚Äù SDM, 2013. \\n[24] Supporting page. http://www.cs.ucr.edu/~eamonn/MatrixProfile.html \\n[25] C. D. Truong and D. T.  Anh, ‚ÄúAn Efficient Method for Motif and \\nAnomaly Detection in Time Series‚Äù IJBIDM, Vol. 10, No. 4, 2015. \\n[26] A. Tucker and X. Liu, ‚ÄúA Bayesian  Network Approach to Explaining \\nTime Series with Changing Structure,‚Äù Intell Data Anal, 8 (5) (2004). \\n[27] L. Ye and E. Keogh, ‚ÄúTime series shapelets: a new primitive for data \\nmining,‚Äù ACM SIGKDD, 2009, pp 947-56. \\n[28] C. Yoon, O. O‚ÄôReilly, K. Bergen and G. Beroza, ‚ÄúEarthquake detection \\nthrough computationally efficient similarity search,‚Äù Sci. Adv. 2015. \\n[29] F. Zhou, F. Torre and J.  Hodgins ‚Äú Aligned Cluster Analysis for \\nTemporal Segmentation of Human Motion,‚Äù IEEE FG'2008. \\n[30] S. Zilberstein and S. Russell, ‚ÄúApproximate Reasoning Using Anytime \\nAlgorithms,‚Äù In Imprecise and Approximate Computation , Kluwer \\nAcademic Publishers, 1995. \\n \\n0 5,000 10,000\\nMatrix  Profile\\nSplit Points   \\nPrediction\\nOne-dimension of multi-d time series: Subject 86, recording 4, dimension 30\\nW S P N      W C T           D            P         W\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Matrix Profile I: All Pairs Similarity Joins for Time Series: \\nA Unifying View that Includes Motifs, Discords and Shapelets \\nChin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,  \\nHoang Anh Dau, ‚Ä†Diego Furtado Silva, ‚Ä°Abdullah Mueen, and Eamonn Keogh \\nUniversity of California, Riverside, ‚Ä†Universidade de S√£o Paulo, ‚Ä°University of New Mexico \\n{myeh003, yzhu015, lulan001, nbegu001, yding007, hdau001}@ucr.edu, diegofsilva@icmc.usp.br, mueen@unm.edu, eamonn@cs.ucr.edu \\n \\nAbstract‚Äî The all-pairs-similarity-search (or similarity join) \\nproblem has been extensively studied for text and a handful of \\nother datatypes. However, surprisingly little progress has been  \\nmade on similarity joins for time series subsequences. The lack of  \\nprogress probably stems from the daunting nature of the \\nproblem. For even modest sized data sets the obvious nested -loop \\nalgorithm can take months, and the typical speed -up techniques \\nin this domain (i.e., indexing, lower -bounding, triangular -\\ninequality pruning and early abandoning) at best produce one or \\ntwo orders of magnitude speedup. In this work we introduce a \\nnovel scalable algorithm for time series subsequence all -pairs-\\nsimilarity-search. For exceptionally large datasets, the algorithm \\ncan be trivially cast as an anytime algorithm and produce high -\\nquality approximate solutions in reasonable  time. The exact \\nsimilarity join algorithm computes the answer to the time series \\nmotif and time series discord  problem as a side -effect, and our \\nalgorithm incidentally provides the fastest known algorithm for \\nboth these  extensively-studied problems. We de monstrate the \\nutility of our ideas for many time series data mining problems, \\nincluding motif discovery, novelty discovery,  shapelet discovery,  \\nsemantic segmentation, density estimation, and contrast set \\nmining.    \\nKeywords‚ÄîTime Series; Similarity Joins; Motif Discovery \\nI. INTRODUCTION \\nThe all-pairs-similarity-search (also known as similarity \\njoin) problem comes in several variants. The basic task is this: \\nGiven a collection of data objects, retrieve the nearest neighbor \\nfor each object . In the text domain the  algorithm has \\napplications in a host of problems, including community \\ndiscovery, duplicate detection, collaborative filtering, \\nclustering, and query refinement [1]. While virtually all text \\nprocessing algorithms have analogues in time series data \\nmining, there has been surprisingly little progress on Time \\nSeries subsequences All-Pairs-Similarity-Search (TSAPSS). \\nWe believe that this lack of progress stems not from a lack \\nof interest in this useful primitive, but from the daunting nature \\nof the problem. Consider the following example that reflects the \\nneeds of an industrial collaborator. A boiler at a che mical \\nrefinery reports pressure once a minute. After a year, we have a \\ntime series of length 525,600. A plant manager may wish to do \\na similarity self-join on this data with week -long subsequences \\n(10,080) to discover operating regimes (summer vs. winter o r \\nlight distillate vs. heavy distillate etc.) The obvious nested loop \\nalgorithm requires 132,880,692,960 Euclidean distance \\ncomputations. If we assume each one takes 0.0001 seconds, \\nthen the join will take 153.8 days. The core contribution of this \\nwork is to show that we can reduce this time to 6.3 hours, using \\nan off-the-shelf desktop computer. Moreover, we show that this \\njoin can be computed and/or updated incrementally. Thus we \\ncould maintain this join essentially forever on a standard \\ndesktop, even if the data arrival frequency was much faster than \\nonce a minute. \\nOur algorithm uses an ultra -fast similarity search algorithm \\nunder z -normalized Euclidean distance as a subroutine, \\nexploiting the overlap between subsequences using the classic \\nFast Fourier Transform (FFT) algorithm.  \\nOur method has the following advantages/features: \\n\\uf0b7 It is exact, providing no false positives or false dismissals. \\n\\uf0b7 It is simple and parameter -free. In contrast, the more \\ngeneral metric space APSS algorithms require building and \\ntuning spatial access methods and/or hash functions.  \\n\\uf0b7 Our algorithm requires an inconsequential space overhead, \\njust O(n) with a small constant factor. \\n\\uf0b7 While our exact algorithm is extremely scalable, for \\nextremely large datasets we can compute the results in an \\nanytime fashion, allowing ultra-fast approximate solutions.  \\n\\uf0b7 Having computed the similarity join for a dataset, we can \\nincrementally upd ate it very efficiently. In many domains \\nthis means we can effectively maintain exact joins on \\nstreaming data forever.  \\n\\uf0b7 Our method provides full joins, eliminating the need to \\nspecify a similarity threshold, which as we will show, is a \\nnear impossible task in this domain.  \\n\\uf0b7 Our algorithm is embarrassingly parallelizable, both on \\nmulticore processors and in distributed systems. \\nGiven all these features, our algorithm has implications for \\nmany time series data mining tasks [5][18][28].  \\nThe rest of the paper is organized as follows. Section II \\nreviews related work an d introduces the necessary background \\nmaterials and definitions. In Section III we introduce our \\nalgorithm and its anytime and incremental variants. Section IV \\nsees a detailed empirical evaluation of our algorithm and shows \\nits implications for many data mining tasks. Finally, in Section \\nV we offer conclusions and directions for future work. \\nII. RELATED WORK AND BACKGROUND \\nThe basic variant of  similarity join problem we are \\ninterested in is as follows : Given a collection of data objects, \\nretrieve the nearest neighbor for every object.   \\nOther common variants include retrieving the top-K nearest \\nneighbors or the nearest neighbor for each object if that \\nneighbor is within a user-supplied threshold, œÑ. (Such variations \\nare trivial generalizations of our proposed algorithm, so we \\nomit them from further discussion). The latter variant results in \\na much easier problem, provided that the threshold is small. For \\nexample, [1] notes that virtually all research efforts ‚Äú exploit a \\nsimilarity threshold more aggressively in order to limit the set'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='of candidate pairs that are considered..  [or] ... to reduce the \\namount of information indexed in the first place.‚Äù \\nThis critical dependence on œÑ is a major issue for text joins, \\nas it is known that ‚Äú join size can change dramatically \\ndepending on the input similarity threshold ‚Äù [10]. However, \\nthis issue is even more critical for time series for two reasons. \\nFirst, unlike similarity (which is bounded between zero and \\none), the Euclidean distance is effectively unbounded, and \\ngenerally not intuitive. For example, if two heartbeats have a \\nEuclidean distance of 17.1, are they similar? Even for a domain \\nexpert that knows the sampling rate and the noise level of the \\ndata, this is not obvious. Second, a single threshold can produce \\nradically different output sizes, even for datasets that are very \\nsimilar.  Consider Figure 1 which shows the output size vs. \\nthreshold setting for the first and s econd halves of a ten -day \\nperiod monitoring data center chillers [21]. For the first five \\ndays a threshold of 0.6 would return zero items, but for the \\nsecond five days the same setting would return 108 items.  This \\nshows the difficulty in selecting  an appropriate threshold. Our \\nsolution is to have no threshold, and do a full join. \\n \\nFigure 1.  Output size vs. threshold for data center chillers [21]. Values beyond \\n2.0 are truncated for clarity (but archived at [24]).  \\nA handful of efforts have considered joins on time series, \\nachieving speedup by (in addition to the use of MapReduce) \\nconverting the data to lower -dimensional representations such \\nas PAA [11] or SAX [12] and exploiting lower bounds and/o r \\nLocality Sensitive Hashing (LSH) to prune some calculations. \\nHowever, the methods are very complex, with many (10 -plus) \\nparameters to adjust. As [11] acknowledges with admirable \\ncandor, ‚ÄúReasoning about the optimal settings is not trivial.‚Äù In \\ncontrast, our proposed algorithm has zero parameters to set. \\nA very recent research effort [28] has tackled the scalability \\nissue by converting the real -valued time series into discrete \\n‚Äúfingerprints‚Äù before using a LSH approach, much like the text \\nretrieval community [1]. They produced impressive speedup, \\nbut they also experienced false negatives. Moreover, the \\napproach has several parameters that need to be set; for \\nexample, they need to set the threshold to a very precise 0.818.  \\nAs we shall show, our algorithm allows both anytime and \\nincremental (i.e. streaming) versions. While a streaming join \\nalgorithm for text was recently introduced [15], we are not \\naware of any such algorithms for time series data or general \\nmetric spaces. More generally, there is a large amount of \\nliterature on joins for text processing [1]. Such work is \\ninteresting, but of little utility given our constraints, data type \\nand problem setting. We require full joins, not threshold joins, \\nand we are unwilling to allow the possibility of false negatives. \\nA. Definitions and Notation \\nWe begin by defining the data type of interest, time series: \\nDefinition 1:  A time series T is a sequence of real -valued \\nnumbers ti: T = t1, t2, ..., tn where n is the length of T. \\nWe are not interested in the global properties of time series, \\nbut in the similarity between local subsequences:  \\nDefinition 2:  A subsequence Ti,m of a T is a continuous \\nsubset of the values from T of length m starting from \\nposition i.   Ti,m = ti, ti+1,‚Ä¶, ti+m-1, where 1 ‚â§  i ‚â§  n-m+1.  \\nWe can take any subsequence from a time series and \\ncompute its distance to all sequences. We call an ordered vector \\nof such distances a distance profile: \\nDefinition 3:  A distance profile D is a vector of the \\nEuclidean distances between a given query and each \\nsubsequence in an all-subsequences set (see Definition 4).  \\nNote that we are assuming that the distance is measured \\nusing the Euclidean distance between the z -normalized \\nsubsequences [8]. The distance profile can be considered a meta \\ntime series that annotates the time series T that was used to \\ngenerate it. The first three definitions are illustrated in Figure 2. \\n \\nFigure 2. A subsequence Q extracted from a time series T is used as a query to \\nevery subsequence in T. The vector of all distances is a distance profile. \\nNote that if the query and all-subsequences set belong to the \\nsame time series, the distance profile must be zero at the \\nlocation of the query, and close to zero just before and just \\nafter. Such matches are c alled trivial matches in the literature \\n[18], and are avoided by ignoring an exclusion zone (shown as \\na gray region) of m/2 before and after the location of the query. \\nWe are interested in similarity join of all subsequences of a \\ngiven time series. We define an all-subsequences set of a given \\ntime series as a set that contains all possible subsequences from \\nthe time series. The notion of all-subsequences set is purely for \\nnotational purposes. In our implementation, we do not actually \\nextract the subsequences in this form as it would require \\nsignificant time and space overhead. \\nDefinition 4: An all-subsequences set A of a time series T \\nis an ordered set of all possible subsequences of T obtained \\nby sliding a window of length m across T: A ={T1,m,, \\nT2,m,‚Ä¶, Tn-m+1,m}, where m is a user -defined subsequence \\nlength. We use A[i] to denote Ti,m. \\nWe are interested in the nearest neighbor (i.e., 1NN) relation \\nbetween subsequences; therefore, we define a 1NN-join \\nfunction which indicates the nearest neighbor relation between \\nthe two input subsequences. \\nDefinition 5:  1NN-join function :  given two all -\\nsubsequences sets A and B and two subsequences A[i] and \\nB[j], a 1NN-join function  Œ∏1nn (A[i], B[j]) is a Boolean \\nfunction which returns ‚Äútrue‚Äù only if B[j] is the nearest \\nneighbor of A[i] in the set B. \\nWith the defined join function, a similarity join set  can be \\ngenerated by applying the similarity join operator on two input \\nall-subsequences sets. \\nDefinition 6: Similarity join set : given all -subsequences \\nsets A and B, a similarity join set  JAB of A and B is a set \\ncontaining pairs of each subsequence in A with its nearest \\nneighbor in B: JAB={‚å© A[i], B[j] ‚å™ |Œ∏1nn (A[i], B[j])}.  We \\ndenote this formally as JAB = A‚ãà\\uf0711nnB. \\n0\\n400\\n800\\n1200\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\\nFirst 5 Days\\nSecond 5 Days\\nData Center Chillers\\nT, a snippet of an energy \\nconsumption\\n2,0000 m/2m/2\\nQ, query of length m\\nNote that |D| = |T|-|Q|+1D, a distance profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='We measure the Euclidean distance between each pair \\nwithin a similarity join set and store the resultants into an \\nordered vector. We call the result vector the matrix profile. \\nDefinition 7:  A matrix profile (or just profile) PAB is a \\nvector of the Euclidean distances between each pair in JAB. \\nWe call this vector the matrix profile because one \\n(inefficient) way to compute it would be to compute the full \\ndistance matrix of all the subsequences in one time series with \\nall the subsequence in another time series and extract the \\nsmallest value in each row (the smallest non-diagonal value for \\nthe self-join case).  In Figure 3 we show the matrix profile of \\nour running example. \\n \\nFigure 3. A time series T, and its self-join matrix profile P.  \\nLike the distance profile, the matrix profile can be \\nconsidered a meta time series annotating the time series T if the \\nmatrix profile is generated by joining T with itself. The profile \\nhas a host of interesting and exploitable properties. For \\nexample, the highest point on the profile corresponds to the \\ntime series discord [5], th e ( tied) lowest points correspond to \\nthe locations of the best time series motif pair [18], and the \\nvariance can be seen as a measure of the T‚Äôs com plexity. \\nMoreover, the histogram of the values in the matrix profile is \\nthe exact answer to the time series density estimation [4]. \\nWe name this special case of the similarity join set \\n(Definition 6) as self-similarity join set, and the corresponding \\nprofile as self-similarity join profile. \\nDefinition 8:  A self-similarity join  set JAA is a result of \\nsimilarity join of the set  A with itself . We denote this \\nformally as JAA = A ‚ãà\\uf0711nn A. We denote the corresponding \\nmatrix profile or self-similarity join profile as PAA. \\nNote that we exclude trivial matches when self -similarity \\njoin is performed, i.e., if A[i] and A[j] are subsequences from \\nthe same all -subsequences set A, Œ∏1nn (A[i], B[j]) is ‚Äúfalse‚Äù \\nwhen A[i] and A[j] are a trivially matched pair. \\nThe ith element in the matrix profile tells us the Euclidean \\ndistance to the nearest neighbor of the subsequence of T, \\nstarting at i.  However, it does not tell us where that neighbor is \\nlocated. This information is recorded in matrix profile index. \\nDefinition 9: A matrix profile index IAB of a similarity join \\nset JAB is a vector of integers where IAB[i] = j if {A[i], B[j]} \\n‚àà JAB. \\nBy storing the neighboring information this way, we can \\nefficiently retrieve the nearest neighbor of A[i] by accessing the \\nith element in the matrix profile index. \\nNote that the function which computes the similarity join set \\nof two input time series  is not symmetric; therefore, JAB ‚â† JBA,  \\nPAB ‚â† PBA, and IAB ‚â† IBA. \\nFor ease of presentation, we have confined this work to the \\nsingle dimensional case; however, nothing intrinsically \\nprecludes generalizations to multidimensional data. \\nSummary of the Previous Section \\nThe previous section was rather dense, so before moving on \\nwe summarize the main takeaway points. We can create two \\nmeta time series, the matrix profile and the matrix profile index, \\nto annotate a time series TA with the distance and location of all \\nits subsequences nearest neighbors in itself or another time \\nseries TB. These two data objects explicitly or implicitly contain \\nthe answers to many time series data mining tasks. However, \\nthey appear to be too expensive to compute to be practica l. In \\nthe next section we will show an algorithm that can compute \\nthese efficiently. \\nIII. ALGORITHMS \\nWe are finally in a position to explain our algorithm s. We \\nbegin by stating the fundamental intuition, which stems from \\nthe relationship between distance profiles and the matrix \\nprofile. As Figure 2 and Figure 3 visually suggest, all distance \\nprofiles (excluding the trivial match region) are upper bound \\napproximations to the matrix profile. More critically, if we \\ncompute all the distance profiles, an d take the minimum value \\nat each location, the result is the matrix profile! \\nThis tells us that if we have a fast way to compute the \\ndistance profiles, then we also have a fast way to compute the \\nmatrix profile. As we shall show in the next section, we have an \\nultra-fast way to compute the distance profiles. \\nA. Mueen‚Äôs ultra-fast Algorithm for Similarity Search (MASS) \\nWe begin by introducing a novel Euclidean distance \\nsimilarity search algorithm for time series data. The algorithm \\ndoes not just find the nearest neighbor to a query and return its \\ndistance; it returns the distance to every subsequence. In \\nparticular, it computes the distance profile, as shown in Figure \\n2. The algorithm requires just O( nlogn) time by exploiting the \\nFFT to calculate  the dot products between the query and all \\nsubsequences of the time series.  \\nWe need to carefully qualify the claim of ‚Äúultra-fast‚Äù. There \\nare dozens of algorithms for time series similarity search that \\nutilize index structures to efficiently locate neighbors [8]. While \\nsuch algorithms can be faster in the  best case , all these \\nalgorithms degenerate to brute force search in the worst case 1 \\n(actually, much worse than brute force search due to the \\noverhead of the index). Likewise, there are index -free methods \\nthat achieve speed -up using various early abandoning tricks \\n[22], but they too degrade to brute force search in the worst \\ncase. In contrast, the performance of the algorithms outlined in \\nTABLE I and TABLE II is completely independent of the data. \\nTABLE I. CALCULATION OF SLIDING DOT PRODUCTS \\nProcedure SlidingDotProduct(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: The dot product between Q and all subsequences in T \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nn ‚Üê Length(T), m ‚Üê Length(Q) \\nTa ‚Üê Append T with n zeros   \\nQr ‚Üê Reverse(Q)  \\nQra ‚Üê Append Qr with 2n-m zeros \\nQraf ‚Üê FFT(Qra), Taf ‚Üê FFT(Ta) \\nQT ‚Üê InverseFFT(ElementwiseMultiplication(Qraf, Taf)) \\nreturn QT \\n \\n1 There are many such worse case scenarios, including high levels of noise blurring \\nthe distinction between closest and furthest neighbors, and rendering triangular -\\ninequality pruning and early abandoning worthless. \\n2,0000\\nP, a matrix profile\\nT, a snippet of an energy \\nconsumption\\nNote that |P| = |T|-|Q|+1'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Line 1 determines the length of both the time series T and \\nthe query Q. In line 2, we use that information to append T with \\nan equal number of zeros. In line 3, we obtain the mirror image \\nof the original query. This reversing ensures that a convolution \\n(i.e. ‚Äúcrisscrossed‚Äù multiplication) essentially produces in-order \\nalignment. Because we require both vectors to be the s ame \\nlength, in line 4 we append enough zeros to the (now reversed) \\nquery so that, like Ta, it is also of length 2 n. In line 5, the \\nalgorithm calculates Fourier transforms of the appended -\\nreversed query (Qra) and the appended time series Ta. Note that \\nwe use FFT algorithm which is an O(nlogn) algorithm. The Qraf \\nand the Taf produced in line 5 are vectors of complex numbers \\nrepresenting frequency components of the two time series. The \\nalgorithm calculates the element-wise multiplication of the two \\ncomplex vectors and performs inverse FFT on the product. \\nLines 5-6 are the class ic convolution operation on two vectors \\n[7]. Figure 4 shows a toy example of the sliding dot product \\nfunction in work. The algorithm time complexity does not \\ndepend on the length of the query (m). \\n \\nFigure 4. A toy example of convolution operation being used to calculate  \\nsliding dot products for time series data. Note the reverse and append \\noperation on T and q in the input. Fifteen dot products are calculated for every \\nslide. The cells m = 2 to n = 4 from left ( red/bold arrows) contain valid \\nproducts. TABLE II takes this subroutine and uses it to create a distance \\nprofile (see Definition 3). \\nIn line 1 of TABLE II, we invoke the dot products code \\noutlined in TABLE I. The formula to calculate the z-normalized \\nEuclidean distance D[i] between two time series subsequence Q \\nand Ti,m using their dot product, QT[i] is (see [24] for \\nderivation): \\nùê∑[ùëñ] = ‚àö2ùëö(1‚àíùëÑùëá[ùëñ]‚àíùëöùúáùëÑùëÄùëá[ùëñ]\\nùëöùúéùëÑùõ¥ùëá[ùëñ] ) \\nwhere m is the subsequence length, ŒºQ is the mean of Q, \\nMT[i] is the mean of Ti,m, œÉQ is the standard deviation of Q, and  \\nŒ£T[i] is the standard deviation of Ti,m. Normally, it takes O( m) \\ntime to calculate the mean and standard deviation for every \\nsubsequence of a long time series. However, here we exploit a \\ntechnique noted in [22] in a different context. We cache \\ncumulative sums of the values and square of the values in the \\ntime series. At any stage the two cumulative sum vectors are \\nsufficient to calculate the mean an d the standard deviation of \\nany subsequence of any length.  See [14] for an elaborate \\ndescription and variations of MASS. \\nTABLE II. MUEEN‚ÄôS ALGORITHM FOR SIMILARITY SEARCH (MASS) \\nProcedure MASS(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: A distance profile D of the query Q  \\n1 \\n2 \\n3 \\n4 \\nQT ‚Üê SlidingDotProducts(Q, T) \\nŒºQ, œÉQ, ŒúT, Œ£T  ‚Üê ComputeMeanStd(Q, T)    // see [22] \\nD ‚Üê CalculateDistanceProfile(Q, T, QT, ŒºQ, œÉQ, ŒúT, Œ£T) \\nreturn D \\nUnlike the dozens of time series KNN search algorithms in \\nthe literature [8], this algorithm calculates the distance to every \\nsubsequence, i.e. the distance profile  of time series T. \\nAlternatively, in join nomenclature, the algorithm produces one \\nfull row of the all -pair similarity matrix. Thus, as we show in \\nthe next section, our join algorithm is simply a loop that \\ncomputes each full row of the all -pair similarity matrix and \\nupdates the current ‚Äúbest-so-far‚Äù matrix profile when needed. \\nB. The STAMP Algorithm \\nWe call our join algorithm STAMP, Scalable Time series \\nAnytime Matrix Profile. The algorithm is outlined in TABLE \\nIII. In line 1, we extract the length of TB. In line 2, we allocate \\nmemory and initial matrix profile PAB and matrix profile index \\nIAB. From lines 3 to line 6, we calculate the distance profiles D \\nusing each subsequence B[idx] in the time series TB and the \\ntime series TA. Then, we perform pairwise minimum for each \\nelement in D with the paired element in PAB (i.e., min( D[i], \\nPAB[i]) for i = 0 to length(D) - 1.) We also update IAB[i] with idx \\nwhen D[i] ‚â§ PAB[i] as we perform the  pairwise minimum \\noperation. Finally, we return the result PAB and IAB in line 7.  \\nNote that the algorithm presented in TABLE III computes \\nthe matrix profile for the general similarity join. To modify the \\ncurrent algorithm to compute  the self -similarity join matrix \\nprofile of a time series TA, we simply replace TB in line 1 with \\nTA, replace B with A in line 4, and ignore trivial match in D \\nwhen performing ElementWiseMin in line 5. \\nTABLE III. THE STAMP ALGORITHM \\nProcedure STAMP(TA, TB, m) \\nInput: Two user provided time series, TA and TB, interested \\nsubsequence length m \\nOutput: A matrix profile PAB and associated matrix profile index IAB of \\nTA join TB, JAB = A‚ãà\\uf0711nnB \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nnB ‚Üê Length(TB) \\nPAB ‚Üê infs, IAB ‚Üê zeros, idxes ‚Üê 1:nB-m+1 \\nfor each idx in idxes    // In any order, but random for anytime algorithm \\n          D ‚Üê MASS(B[idx], TA) \\n          PAB, IAB ‚Üê ElementWiseMin(PAB, IAB, D, idx) \\nend for \\nreturn PAB, IAB \\nTo parallelize the STAMP algorithm for multicore \\nmachines, we simply distribute the indexes to secondary \\nprocess run in each core, and the secondary processes use the \\nindexes they received to update their own PAB and IAB. Once the \\nmain process returns from  all secondary processes, we use \\nElementWiseMin to merge the received PAB and IAB.  \\nC. An Anytime Algorithm for TSAPSS \\nWhile the exact algorithm introduced in the previous section \\nis extremely scalable, there will always be datasets for which \\ntime needed for an exact solution is untenable. We can mitigate \\nthis by computing the results in an anytime fashion, allowing \\nfast approximate solutions [30]. To a dd the anytime nature to \\nthe STAMP algorithm, we simply ensure a randomized search \\norder in line 2 of TABLE III.  \\nWe can compute a (post-hoc) measurement of the quality of \\nan anytime solution by measuring the Root -Mean-Squared-\\nError (RMSE) between the true matrix profile and the current \\nbest-so-far mat rix profile. As Figure 5 suggests, with an \\nexperiment on random walk data, the algorithm converges very \\nquickly. \\nQ2T1 0 00 00 00 00 0Q1T4Q2T2+Q1T1 Q2T3+Q1T2 Q2T4+Q1T3\\nOutput\\nInputT2T1 T4T3 00 00 Q1Q2 00 00 00'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Figure 5. main) The decre ase in RMSE as the STAMP algorithm updates \\nmatrix profile with the distance profile calculated at each iteration.  inset) The \\napproximate matrix profile at the 10% mark is visually indistinguishable from \\nthe final matrix profile. \\nZilberstein [30] gives a number of desirable properties of \\nanytime algorithms, including Low Overhead , Interruptibility, \\nMonotonicity, Recognizable Quality, Diminishing Returns and \\nPreemptability (these properties are mostly obvious from their \\nnames, but full definitions are at [30]). \\nBecause each subsequence‚Äôs distance profile is bounded \\nbelow by the exact matrix profile, updating an approximate \\nmatrix profile with a distance profile with pairwise minimum \\noperation either drives the approximate solution closer the exact \\nsolution or retains the current approximate solution. Thus , we \\nhave guaranteed Monotonicity. From Figure 5, the approximate \\nmatrix profile converges to the exact matrix profile \\nsuperlinearly; therefore, we have strong Diminishing Returns. \\nWe can easily achieve Interruptibility and Preemptability by \\nsimply inserting a few lines of code between lines 5 and 6 of \\nTABLE III that read: \\n5new \\n6new \\n7new \\nif CheckForUserInterrupt  = TRUE \\n          Report({PAB, IAB}, ‚ÄòHere is an approximate answer.‚Äô) \\nif GetUserChoice = ‚Äòfurther refine‚Äô, CONTINUE, else BREAK \\nThe space and time overhead for the anytime property is \\neffectively zero, thus we have Low Overhead. This leaves only \\nthe property of Recognizable Quality. Here we must resort to a \\nprobabilistic argument.  The convergence curve shown in  \\nFigure 5 is very typical, so we could use past convergence \\ncurves to predict the quality of solution when interrupted on \\nsimilar data.  \\nD. Time and Space Complexity \\nThe overall complexity of the proposed algorithm is \\nO(n2logn) where n is the length of the time series. However, our \\nexperiments (see Section 4.1) empirically suggest that the \\nruntime of  STAMP‚Äôs growth rate is roughly O( n2) instead of \\nO(n2logn). One possible e xplanation for this is that the nlogn \\nfactor comes from the FFT subroutine. Because FFT is so \\nimportant in many applications, it is extraordinarily well \\noptimized. Thus, the empirical runtime is very close to linear. \\nIn contrast to the above, the brute for ce nested loop \\napproach has a time complexity of O(n2m). Recall the industrial \\nexample in the introduction section. We have  m = 10,080, but \\nlog(n) = 5.7, so we would expect our approach to be about \\n1,768 times faster. In fact, we are empirically even faster. The \\ncomplexity analysis downplays the details of important constant \\nfactors. The nested loop algorithm must also z -normalize the \\nsubsequences. This either requires O( nm) time, but with an \\nuntenable O(nm) space overhead, or an O( n2m) time overhead. \\nAnd r ecall that this is before a single Euclidean distance \\ncalculation is performed.  \\nFinally, we mention one quirk of our algorithm which we \\ninherit from using the highly optimized FFT subroutine. Our \\nalgorithm is fastest when n is an integer power of two, slower \\nfor non -power of two but composite numbers, and slowest \\nwhen n is prime. The difference (for otherwise similar values of \\nn) can approach a factor of 1.6x. Thus, where possible, it is \\nworth contriving the best case by tru ncation or zero-padding to \\nthe nearest power of two. \\nE. Incrementally Maintaining TSAPSS \\nUp to this point we have discussed the batch version of \\nTSAPSS. By batch, we mean that the STAMP algorithm needs \\nto see the entire time series TA and TB (or just TA if we are \\ncalculating the self -similarity join matrix profile) before \\ncreating the matrix profile. However, it would be advantageous \\nif we could build the matrix profile incrementally. Given that \\nwe have performed a batch construction of matrix profile, i f \\nnew data arrives, it would clearly be preferable to incrementally \\nadjust the current profile, rather than start from scratch.  \\nBecause the matrix profile solves both the times series motif \\nand the time series discord problems, an incremental version of \\nSTAMP would automatically provide the first incremental \\nversions of both algorithms. We call such an algorithm the \\nSTAMPI (STAMP Incremental) algorithm. \\nWe will demonstrate our ability to incrementally maintain \\nthe matrix profile in this section. For simpli city and brevity \\nTABLE IV only shows the algorithm to maintain the self -\\nsimilarity join. The generalizations are obvious. \\nTABLE IV. THE STAMPI ALGORITHM \\nProcedure STAMPI(TA, t, PAA, IAA) \\nInput: The original time series TA, a new data point t following TA, the \\nmatrix profile PAA and its associated matrix profile index IAA of TA.  \\nOutput: The incrementally updated matrix profile PAA,new and its matrix \\nprofile index IAA,new of the current time series TA,new= TA, t. \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nTA,new = [TA, t] \\nS ‚Üê last subsequence in TA,new, idx ‚Üê index of S in TA,new \\nD ‚Üê MASS (S, TA) \\nPAA, IAA ‚Üê ElementWiseMin(PAA, IAA, D, idx) \\npAA,last, iAA,last ‚Üê FindMin(D) \\nPAA,new ‚Üê [PAA, pAA,last], IAA,new ‚Üê [IAA, iAA,last] \\nreturn PAA,new, IAA,new \\nFor clarity, we denote the updated time series as TA,new, the \\nupdated matrix profile as PAA,new and the associated matrix \\nprofile index as IAA,new. As each additional data point t arrives, \\nthe size of the time series TA increases by one, and a new \\nsubsequence S is generated at the end of TA,new. In line 3 we \\nobtain the distance profile of S with regard to TA. Then, as in the \\noriginal STAMP algorithm,  in line 4 we perform  a pairwise \\ncomparison between every element in D with the corresponding \\nelement in PAA to see if the corresponding element in PAA needs \\nto be updated. In line 5, we find the nearest neighbor of S and \\nthe associated index by evaluating the minimum value of D. \\nFinally, in line 6, we obtain the new matrix profile and \\nassociated matrix profile index by concatenating the results in \\nline 4 and line 5.  \\nThe time complexity of the STAMP I algorithm is O(nlogn) \\nwhere n is the length of size of th e current time series TA. Note \\nthat as we maintain the profile, each incremental call of \\nSTAMPI requires invoking the FFT subroutine with a slightly \\nlonger time series ( n becomes n+1 ). Thus it gets very slightly \\nslower at each time step. Therefore, the best way to measure the \\nperformance is to ask for the Maximum Time Horizon (MTH), \\nin essence the answer to this question: ‚Äú Given this arrival rate, \\nhow long can we maintain the profile before we can no longer \\nupdate fast enough?‚Äù \\nNote that the subsequence length m is not considered in the \\nMTH evaluation. Recall that overall time complexity of the \\n10,0000\\nRMSE\\niteration1,000\\napproximate matrix \\nprofile at 1,000 iterations.\\nexact matrix profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='algorithm is determined by the efficiency of the FFT \\nsubroutine, which is independent of m. We have computed the \\nMTH for two common scenarios of interest to the community. \\n\\uf0b7 House Electrical Demand  [19]: This dataset is updated \\nevery eight seconds. By iteratively calling the STAMP I \\nalgorithm, we can maintain the profile for 5.8 years.  \\n\\uf0b7 Oil Refinery :  Most telemetry in oil refineries is sampled \\nat once a minute [26]. The relatively low sampling rate \\nreflects the ‚Äúinertia‚Äù of massive boilers/condensers. Even if \\nwe maintain the profile for 40 years, the update time is only \\naround 6.78 seconds. Moreover, th e raw data, matrix \\nprofile and index would only require 0.5 gigabytes of main \\nmemory. Thus the MTH here is forty plus years. Given \\nprojected improvements in hardware, this effectively \\nmeans we can maintain the matrix profile forever. \\nAs impressive as these  numbers are, they are actually quite \\npessimistic. For simplicity we assume that every value in the \\nmatrix profile index will be updated at each time step.  \\nHowever, empirically, much less than 0.1% of them need to be \\nupdated. If it is possible to prove an upper bound on the number \\nof changes to the matrix profile index per update, then we could \\ngreatly extend the MTH  or handle much faster sampling rates. \\nWe leave such considerations for future work. \\nIV. EXPERIMENTAL EVALUATION \\nWe begin by stating our experimen tal philosophy. We have \\ndesigned all experiments such that they are easily reproducible. \\nTo this end, we have built a webpage [24] which contains al l \\ndatasets and code used in this work, together with spreadsheets \\nwhich contain the raw numbers and some supporting videos.  \\nGiven page limits, and the utility of our algorithm for a host \\nof existing (and several new) data mining tasks, we have chosen \\nto c onduct exceptionally broad  but shallow experiments. We \\nhave conducted such deep detailed experiments and placed \\nthem at [24]. Unless otherwise state d we measure wall clock \\ntime on an Intel i7@4GHz with 4 cores. \\nA. Scalability of Profile-Based Self-Join \\nBecause the time performance of STAMP is independent of \\nthe data quality or any user inputs (there are none except the \\nchoice of m, which does not affect the speed), our scalability \\nexperiments are unusually brief.  In TABLE V we show the \\ntime required for a self -join with m fixed to 256, for \\nincreasingly long time series.  \\nTABLE V.  TIME REQUIRED FOR A SELF-JOIN WITH M = 256, VARYING N \\nValue of n 217 218 219 220 221 \\nTime Required 15.1 min 70.4 min 5.4 hours 24.4 hours 4.2 days \\nIn TABLE VI, we show the time required for a self -join \\nwith n fixed to 2 17, for increasing long m. Again recall that \\nunlike virtually all other time series data mining algorithms in \\nthe literature whose performance degrades for longer \\nsubsequences [8][18], the running time of STAMP does not \\ndepend on m. \\nTABLE VI. TIME REQUIRED FOR A SELF-JOIN WITH N= 217, VARYING M \\nValue of m 64 128 256 512 1,024 \\nTime Required 15.1 min 15.1 min 15.1 min 15.0 min 14.5 min \\nFinally, we further exploit the simple parallelizability of the \\nalgorithm by using four 16 -core virtual machines on Microsoft \\nAzure to redo the two -million join ( n = 2 21 and m = 256) \\nexperiment. By scaling up the computational power, we have \\nreduced the running time from 4.2 days to just 14.1 hours. This \\nuse of cloud computing required writing just few dozen lines of \\nsimple additional code [24]. \\nIt is difficult to find good baselines to compare to. We \\nbelieve STAMP is the only algorithm that does full, exact joins \\non time series subsequences. Most algorithms do only threshold \\njoins and/or do approximate joins, and are not specialized for \\ntime series subsequences. However, we searched for the best \\nbaselines and made the following concessions to  the rival \\nalgorithms to allow comparisons. \\n\\uf0b7 TSFRDAA [11]: While STAMP returns all nearest \\nneighbors, we adjust the threshold (the selectively) of \\nTSFRDAA such that only needs to return the top 1% nearest \\nneighbors, a much easier task. \\n\\uf0b7 HDSJI-SAX [12]: This method  allows false negatives ; we \\nignore this. It needs time to build indexes ; we do not count \\nthis time, and as above, we allow it to return  the only the \\ntop 1% nearest neighbors ( as bef ore, not the 100% the \\nSTAMP returns, and therefore a much easier task.). \\n\\uf0b7 Optimized Nested Loop (ONL): Here we use a nested -loop \\njoin optimized in the following manner. We use a state -of-\\nthe-art DFT indexing technique to do the search in the \\ninner loop, and we do not count the time needed to build \\nthe indexes [8]. We carefully adjust  parameters for best \\nperformance.  \\nWe consider the first 2 18 data points of the ECG dataset \\nused in [22], with a query length of 256, about one heartbeat ; \\nTABLE VII shows the results.  \\nTABLE VII. TIME FOR A SELF-JOIN WITH M = 256, VARYING ALGORITHMS \\nAlgorithm TSFRDAA HDSJI-SAX ONL STAMP \\nTime Required 51.7 hours 19.6 min 28.1 hours 1.17 hours \\nAs these results show, even if we ignore the limitations of \\nthe baseline methods, STAMP is still significantly faster. \\nB. Profile-Based Self-Join \\nA recent paper notes that many fundamental problems in \\nseismology can be solved by joining seismometer telemetry \\n[28], including the disc overy of foreshocks, aftershocks, \\ntriggered earthquakes, volcanic activity and induced seismicity. \\nHowever, the paper notes a join with a query length of 200 on a \\ndata stream of length 604,781 requires 9.5 days. Their solution, \\na clever transformation of t he data to allow LSH based \\ntechniques, does achieve significant speedup, but at the cost of \\nfalse negatives and the need for significant parameter tuning.  \\nThe authors kindly shared their data, and, as we hint at in \\nFigure 6, confirmed that STAMP does not have false negatives. \\n \\nFigure 6. top) An excerpt of a seismic time series aligned with its matrix \\nprofile (bottom). The ground truth provided by the authors of [28] requires that \\nthe events occurring at time 4,050 and 7,800 match. \\nWe repeated the n = 604,781, m = 200 experime nt and \\nfound it took just 8.9 hours to finish. As impressive as this is, in \\nthe next section we show that we can do even better.   \\n4,000 5,000 6,000 7,000 8,000 9,000\\n0\\n5\\n10\\n15\\nSeismic time series (excerpt) \\nMatrix Profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='C. The Utility of Anytime STAMP \\nThe seismology dataset offers an excellent opportunity to \\ndemonstrate the utility of the anytime version of our algorithm. \\nThe authors of [28] revealed in their long -term ambition of \\nmining even larger datasets [3]. In Figure 7 we repeated the \\nexperiment with the snippet shown in Figure 6, this time \\nreporting the best-so-far matrix profile reported by the \\nalgorithm at various milestones. Even with just 0.25% of the \\ndistances computed (that is to say, 400 times faster) the correct \\nanswer has emerged. Thus, we can provide the correct answers \\nto the seismologists in just minutes, rather than the 9.5 days.  \\n \\nFigure 7. top) An excerpt of the seismic data that is also shown in Figure 6. \\ntop-to-bottom) The approximations of the matrix profile for increasing \\ninterrupt times. By the time we have computed just 0.25% of the calculations \\nrequired, the minimum of the matrix profile points to the ground truth. \\nTo show the generality of this anytime feature of STAMP, \\nwe consider a very different dataset. As shown in Figure \\n8.inset), it is possible to convert DNA to a time ser ies [22]. We \\nconverted the Y-chromosome of the Chimpanzee this way. The \\nresulting time series is  little over one -million in length. We \\nperformed a self-join with m = 60,000.  Figure 8.bottom shows \\nthe best motif is so well conserved (ignoring the first 20%), that \\nit must correspond to a recent (in evolutionary tim e) gene \\nduplication event. In fact, in a subsequent analysis we \\ndiscovered that ‚Äúmuch of the Y (Chimp chromosome) consists of \\nlengthy, highly similar repeat units, or ‚Äòamplicons‚Äô‚Äù [9]. \\n \\nFigure 8. top) The Y -chromosome of the Chimp in time series space with its \\nmatrix profile. bottom) A zoom -in of the top motif discovered using anytime \\nSTAMP, we believe it to be an amplicon [9]. \\nThis demanding join would take just over a day of CPU \\ntime (see TABLE V). However, using anytime STAMP we \\nhave the result shown above after doing just 0.021% of the \\ncomputations, in about 18 seconds. At [24] we have videos that \\nshow the rapid convergence of the anytime variant of STAMP. \\nD. Profile-Based Similarity Join Set \\nIn this section we show two uses of similarity join set. The \\nfirst use is more familiar to APSS users, quantifying what is \\nsimilar between two time series. The second example, \\nquantifying what is different between two time series, is novel  \\nand can only be supported by threshold -free algorithms that \\nreport the nearest neighbor for all objects. \\nTime Series Set Similarity \\nGiven two (or more) time series collected under different \\nconditions or treatments, a data analyst may wish to know what \\npatterns (if any) are conserved between the two time series. To \\ndemonstrate the utility of automating this, we consider a simple \\nbut in tuitive example. Figure 9 shows the raw audio of two \\npopular songs converted to Mel Frequency Cepstral \\nCoefficients (MFCCs). Specifically, the songs used in this \\nexample are ‚ÄúUnder Pressure‚Äù by Queen an d David Bowie and \\n‚ÄúIce Ice Baby ‚Äù by the American rapper Vanilla Ice. Normally, \\nthere are 13 MFCCs; here, we consider just one for simplicity. \\n \\n \\nFigure 9. Two songs represented by just the 2 nd MFCC at 100Hz. We \\nrecognize that it is difficult to see any structure in these times series; however, \\nthis difficulty is the motivation for this experiment. \\nEven for these two relatively short time series, visual \\ninspection does not offer immediate answers. The problem here \\nis compounded by the size reproduction, but is not significantly \\neasier with large-scale [24] or interactive graphic tools. We ran \\nJAB (Queen-Bowie, Vanilla Ice) on these datasets with m = 500 \\n(five seconds), the best match, corresponding the minimum \\nvalue of the matrix profile PAB, is shown in Figure 10. \\n \\nFigure 10. The result of JAB (Queen-Bowie (red/bold), Vanilla-Ice (green/fine)) \\nproduces a strongly conserved five second region. \\nReaders may know the cause for this highly conserved \\nsubsequence. It corresponds to the famous baseline of ‚ÄúUnder \\nPressure,‚Äù which was sampled (plagiarized) by Vanilla Ice in \\nhis song. The join took 21.9 seconds. The ability to find \\nconserved structure in apparently disparate time series could \\nopen many avenues of research in medicine and industry. \\nTime Series Difference \\nWe introduce the Time Series Diff (TSD) operator, which \\ninformally asks ‚ÄúWhat happens in time series T A, that does not \\nhappen in time series TB?‚Äù Here TA/TB could be an ECG before \\na drug is administered/after a drug is administered, telemetry \\nbefore a successful launch/before a catastrophic launch etc. The \\nTSD is simply the subsequence referred to by the maximum \\nvalue of the JAB join‚Äôs profile PAB. \\nWe begin with a simple intuitive example. The UK and US \\nversions of the Harry Potter audiobook series are performed by \\ndifferent narrators, and have a handful of differences in the text. \\nFor example, the UK version contains: \\nHarry was passionate about Quidditch. He had played as Seeker on the Gryffindor \\nhouse Quidditch team ever since his first year at Hogwarts and owned a Firebolt, one \\nof the best racing brooms in the world... \\nBut the corresponding USA version has: \\nHarry had been on the Gryffindor House Quidditch team ever since his first year at \\nHogwarts and owned one of the best racing brooms in the world, a Firebolt. \\nAs shown in Figure 11, we can convert the audio \\ncorresponding to these snippets into MFCCs and invoke a JAB \\njoin set to produce a matrix profile PAB that represents the \\ndifferences between them. As Figure 11.left shows, the low \\nvalues of this profile correspond to identical spoken phrases \\n(despite having two different narrators). However here we are \\n4,000 5,000 6,000 7,000 8,000 9,000\\ndata\\n0.25%\\n1%\\n100%\\n0 1,000,0000\\n100\\n200\\nPan troglodytes Y-chromosome \\n0 60,000\\n12,749,475 to 14,249,474 bp\\n622,725 to 2,122,724 bp\\nT1 = 0;\\nfor i = 1 to length(chromosome) \\nif chromosomei = A, then Ti+1 = Ti + 2 \\nif chromosomei = G, then Ti+1 = Ti + 1\\nif chromosomei = C, then Ti+1 = Ti - 1 \\nif chromosomei = T, then Ti+1 = Ti - 2 \\nend\\n0\\nQueen-Bowie\\n1,000 2,000\\n-10\\n0\\n10\\nVanilla Ice\\n0 250 500-3\\n-2\\n-1\\n0\\n1\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='interested in the differences, the maximum value of the profile. \\nAs we can see in Figure 11.right, here the profile corresponds \\nto a phrase unique to the USA edition.  \\n \\nFigure 11. The 2 nd MFCC of snippets from the USA ( pink/bold) and UK \\n(green/fine) Harry Potter audiobooks.  The JAB join of the two longer sections \\nin the main text produce s mostly small values in the profile correspond to the \\nsame phrase (left), the largest value in  the profile corresponds to a phrase \\nunique to the USA edition (right). \\nThe time required to do this is just 0.067 seconds, much \\nfaster than real time. While this demonstration is trivial, in [24] \\nwe show an example applied to ECG telemetry.  \\nE. Profile-Based Motif Discovery \\nSince their introduction in 2003, time series motifs have \\nbecome one of the most frequently used primitives in time  \\nseries data mining, with applications in dozens of domains [2]. \\nThere are several proposed definitions for time series motifs, \\nbut in [18] it is argued that if you can solve the most basic \\nvariant, the closest (non -trivial) pair of subsequences, then all \\nother variants only require some minor additional calculations. \\nNote that the locations of the two (tying) minimum values of \\nthe matrix profile are exactly the locations of the 1st motif pair. \\nThe fastest known exact algorithm for computing time \\nseries motifs is the MK algorithm [18]. Note, however, that this \\nalgorithm‚Äôs time performance depends on the time series itself. \\nIn contrast, the Profile -Based Motif Discovery (PBMD) takes \\ntime independent of the data. To see this, we compared the two \\napproaches on an electrocardiogram of length 65,536. In Figure \\n12.left we ask what happens as we search for lon ger and longer \\nmotifs. In Figure 12.right we ask what happens if the motif \\nlength is fixed to m = 512, but the data becomes increasing \\nnoisy.  \\n \\nFigure 12. The time required to find the top -motif pairs in a time series of \\nlength 216 for increasingly long motif lengths ( left), and for a length fixed to \\n512, but in the face of increasing noise levels (right). \\nThese results show that  even in the best case for MK, \\nPBMD is competitive, but as we have longer queries and/or \\nnoisier data, its advantage becomes unassailable. Moreover, \\nPBMD inherits STAMP‚Äôs anytime and incremental \\ncomputability, and is easily parallelizable.  \\nF. Profile-Based Discord Discovery \\nA time series discord  is the subsequence that has the \\nmaximum distance to its nearest neighbor. While this is a \\nsimple definition, time series discords are known to be very \\ncompetitive as novelty/anomaly detectors  [5]. Note that as \\nshown in Figure 13, the time series discord is encoded as the \\nmaximum value in a matrix profile. \\n \\nFigure 13. top) An excerpt from an ECG incorporating a premature ventricular \\ncontraction (red/bold). bottom) The time series profile peaks exactly at the \\nbeginning of the PVC. \\nThe time taken to compute the discord  is obviously just the \\ntime needed to compute the matrix profile ( here, 0.9 seconds). \\nThere are a few dozen discord discovery algorithms in the \\nliterature. Some of them may be competitive in the best case, \\nbut just like motif -discovery algorithms they all degenerate to \\nbrute force search in the worst case, and none allow the anytime \\nproperties that we inherit from using STAMP.  \\nG. Incrementally Maintaining Motifs and Discords \\nWe have demonstrated the ability to detect time series \\nmotifs and discords using the matrix profile in the previous two \\nsections. However, we assumed that the entire time series was \\navailable beforehand. Here we remove this assumption and \\nshow how STAMP I allows us to incrementa lly maintain time \\nseries motifs/ discords in an online fashion. There are other \\nattempts at one [20][2] or both [25] of these tasks, but they are \\nall approximate and allow false dismissals.   \\nIn Section III.E, we introduced the STAMPI algorithm. The \\nability to incrementally maintain the matrix profile implies the \\nability to exactly maintain the time series motif [18] and/or time \\nseries discord [5] in streaming data. We simply need to keep \\ntrack of the extreme values of the incrementally-growing matrix \\nprofile, report a new pair of motifs when a new minimum value \\nis detected, and report a new discord when we see a new \\nmaximum value. \\nWe demonstrate the utility of these ideas on the AMPds \\ndataset [13]. Here the kitchen fridge and the heat pump are both \\nplugged into a single metered power supply. For the first week, \\nonly the refrigerator is running. At the end of the week, the \\nweather gets cold and the heat pump is turned on. The sampling \\nrate is one sample/m inute, and th e subsequence length is 100. \\nWe apply the STAMP algorithm to the first three days of data, \\nthen invoke STAMP I to handle newly arriving data , report an \\nevent when we detect a new extreme value. \\nOur first event occurs at the 9,864 th minute (6 da y 20 hour \\n24 minute). As shown in Figure 14, a new minimum value is \\ndetected, which indicates a new time series motif. The just -\\narrived 100 -minute-long pattern looks v ery similar to another \\npattern that occurred five hours earlier. While there is a lot of \\nregularity in the fridge data in general, the exceptional \\nsimilarity observed here suggested some underlying physical \\nmechanism that caused such a perfectly -conserved pattern, \\nperhaps a mechanical ice-making ‚Äúsubroutine.‚Äù \\n \\nFigure 14. top) The matrix profile of the first 9,864 minutes of data. bottom) \\nThe minimum value of the matrix profile corresponds to a pair of time series \\nmotifs in the power usage data. right) The time series motif detected. \\nOur second event occurs at the 10,473 th minute (7 day 6 \\nhour 33 minute). As shown in Figure 15, a new maximum value \\n‚Ä¶indor house Quidditch team ever since his first ye‚Ä¶\\nHarry had been on the Gryffindor House Quidditch te..\\nsince his first year at Hogwarts and owned a Fire..\\nsince his first year at Hogwarts and owned on..\\nED = 2.9 \\nClosest Match \\n0 100(1.6 seconds) 0 100\\nED = 10.4\\n(1.6 seconds)\\nFurthest Match (Time Series Difference) \\n256 512 1,024 2,048 4,096\\n0\\n40\\n80\\n120\\n160\\n200\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nSubsequence Length  (m)\\nTime Taken (min)\\n80 40 0\\n0\\n70\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nNoise Added (dB)  \\n0 1000 2000 3000\\nECG qtdb\\nSel102\\n(excerpt)\\nPremature Ventricular \\nContraction\\nTime Series Profile\\n0 5,000 10,000\\n0 100\\nnew minimum value\\nnew motifMatrix Profile\\nPower Usage Data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='is detected, which indicates a new time series discord. The time \\nseries discord corresponds to the first occurrence of a heat \\npump pattern in the power usage data.  \\n \\nFigure 15. top) The matrix profile for the first 10,473 minutes. bottom) The \\nmaximum value of the matrix profile corresponds to a time series discord. \\nright) The time series discord detected is the first  heat pump pattern \\noccurrence in the dataset. \\nThe maximum time needed to process a single data point \\nwith STAMP I in this dataset is 0.005 second s, which is less \\nthan 0.01% of the data sampling rate. Thus, on this dataset we \\ncould continue monitoring with the STAMP I algorithm for \\nseveral decades before running out of time or memory. \\nH. Profile-Based Shapelet Discovery \\nShapelets are time series subsequences which are in some \\nsense the maximally representative of a class [23][27]. \\nShapelets can be used to classify time series (essentially, the \\nnearest shapelet algorithm), offering the benefits of speed, \\nintuitiveness and at least on some domains, significantly \\nimproved accuracy [23]. However, these advantages come at \\nthe cost of a very expensive training phase, with O( n2m4) time \\ncomplexity, where m is the length of the longest time series \\nobject in the dataset, and n is the number of objects in the \\ntraining set. In order to mitigate this high ti me complexity, \\nresearchers have proposed various distance pruning techniques \\nand candidate ranking approaches for both the admissible [27] \\nand appro ximate [23] shapelet discovery. Nevertheless, \\nscalability remains the bottleneck.  \\nBecause shapelets are essentially supervised motifs, and we \\nhave shown that STAMP can find motifs very quickly, it is \\nnatural to ask if STAMP has implications for shapelet \\ndiscovery. While space limitations prohibit a detailed \\nconsideration of this question, we briefly sketch out and test \\nthis possibility as follows. \\nAs shown in Figure 16, we can use matrix profile to \\nheuristically ‚Äúsuggest‚Äù candidate shapelets. We consider two \\ntime series TA (green/bold) and TB (pink/light) with class 1 and \\nclass 0 being their corresponding class label, and we take  JAB, \\nJAA, JBA and JBB. Our claim is the differences in the heights of \\nPAB, PAA (or PBA, PBB) are strong indicators of good candidate \\nshapelets. The intuition is that if a discriminative pattern is \\npresent in say, class 1, but not in class 0, then we expect to see a \\n‚Äúbump‚Äù in the PAB (the intuition holds if the order is reversed). \\nA significant difference (quantified by a threshold shown in \\ndashed line) between the heights of PAA and PAB curves \\ntherefore indicates the occurrence of good candidate shapelets, \\npatterns that only occur in one of the two classes. \\n \\nFigure 16. top.left) Two time series TA and TB formed by concatenating \\ninstances of class 1 and 0 respectively of ArrowHead [6]. bottom) The height \\ndifference between PAB (or PBA) and PAA (or PBB) are suggestive of  good \\nshapelets. top.right) An example of good shapelet extracted from class 1. \\nThe time taken to compute  all four matrix profiles is 1.0 \\nseconds and the time to further evaluate the two twelve \\ncandidates selected takes 2.7 seconds. On the same machine, \\nthe brute force shapelet classifier takes 4.2 minutes with 2,364 \\ncandidates. Note that, in this toy demonst ration, the speedup is \\n68X, however, for larger datasets, the speedup is greater [24]. \\nI. Profile-Based Semantic Segmentation \\nThe goal of time series semantic segmentation is to partition \\na dataset containing multiple activities/regimes into atomic \\nbehaviors or conditions. For example, for human activity data \\nthe regimes might later be mapped to { eating, working, \\ncommuting,...}[29]. As this example suggests, most work in this \\narea is highly domain -dependent. In contrast, here we show \\nhow the matrix profile index can be emp loyed for domain \\nagnostic time series segmentation.  \\nThe intuition of our approach is as follows. Within a single \\nregime we might expect that most subsequences will have a \\nnearest neighbor close by (in time). For example, consider the \\ntoy problem shown in Figure 17 which shows two obvious \\nregimes. We would expect that the nearest neighbor to the first \\nrun gait cycle is the second or third or fourth run cycle, but it \\nwill almost certainly not be one of the walk cycles. In general, \\nthis tendency for nearest neighbor pointers not to cross the \\nboundaries corresponding to regime changes may be sufficient \\nto discover these boundaries, and of course, this is precisely the \\ninformation that is encoded in the matrix profile index. \\nThus, for each point we count how many ‚Äúarcs‚Äù connecting \\ntwo nearest neighbors cross it if we connect each subsequence \\nto its nearest neighbor as shown in Figure 17. \\n \\nFigure 17. top) A (toy) time series ( red) and nearest neighbor locations for \\neach subsequence. bottom) The number of arcs crossing above each \\nsubsequence. \\nWe applied the procedure described above to a heavily \\nstudied activity segmentation problem [29], which was derived \\nfrom the CMU Motion Capture data base [16]. The recordings \\nare represented as multi -dimensional time series, and most \\nresearch efforts carefully select the best subset for the \\nsegmentation task. For example, [29] states that ‚Äú we only \\nconsider the 14 most informative joints out of 29 .‚Äù In contrast, \\nwe attempt this with a single dimension. The only parameter we \\nneed to set is sliding window size . In Figure 18 we show the \\nsegmenting results obtained using our approach , with  \\nannotations taken from [29] for context. \\nMuch of the evaluation in this community lacks formal \\nmetrics, preferring instead visual sanity tests like the one in \\nFigure 18. Given this, we can say that our approach is very \\ncompetitive on this dataset, in spite of the fact that we \\nhandicapped ourselves to only consider one dimension.  \\n0 100\\n0 10,0005,000\\nMatrix Profile\\nPower Usage Data\\nnew maximum value\\nnew discord\\n0\\n7\\n0 1,400\\nPABPAA\\n0 1,600\\nTA\\nTB\\n0 300\\nAn example of good shapelet\\n0 1,400\\nPBA\\nPBB\\n8\\n0\\n0 1,400\\nDiff(PAB ,PAA) Threshold\\n-1\\n4\\n-2\\n4\\n0 1,400\\nDiff(PBA ,PBB) Threshold\\n0\\n1\\n2\\n1 2 0\\nwalking slow    walking slow     run       run run run\\nNumber of arcs intersecting each point'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content=\"Figure 18. top) A matrix profile ( blue) obtained for the time series ( red) and \\nnumber of arcs crossing each point ( green). Low values of this green curve \\ncoorespond to candidate split points. bottom) Human annotations of the \\nactivities: w ‚Äì walk, s ‚Äì stretch, p ‚Äì punch, c ‚Äì chop, t ‚Äì turn, d ‚Äì drink. \\nV. CONCLUSION \\nWe have introduced a scalable algorithm for creating time \\nseries subsequences joins. Our algorithm is simple, fast, \\nparallelizable and parameter -free, and can be in crementally \\nupdated for moderately fast data arrival rates. We have shown \\nthat our algorithm has implications for many existing tasks, \\nsuch as motif discovery, discord discovery, shapelet discovery \\nand semantic segmentation, and may open up new avenues for  \\nresearch, including computing various definitions of time series \\nset difference. Our code is freely available for the community to \\nconfirm, extend and exploit our ideas. \\nREFERENCES \\n[1] R. J. Bayardo, Y. Ma and R. Srikant, ‚ÄúScaling up all pairs similarity \\nsearch,‚Äù WWW 2007, pp 131-140. \\n[2] N. Begum and E. Keogh, ‚ÄúRare time series motif discovery from \\nunbounded streams,‚Äù PVLDB 8(2): 149-160, 2014. \\n[3] G. Beroza, ‚ÄúPersonal Correspondence,‚Äù Jan 21th, 2016. \\n[4] T. Bouezmarni and J. Rombouts, ‚ÄúNonparametric density estimation for \\npositive time series,‚Äù CSDA, 54, 245-261, 2010.  \\n[5] V. Chandola, D. Cheboli and V. Kumar, ‚ÄúDetecting anomalies in a time \\nseries database,‚Äù UMN TR09-004. \\n[6] T. Chen  et al ., ‚ÄúThe UCR time series classification archive,‚Äù \\nhttp://www.cs.ucr.edu/~eamonn/time_series_data/. \\n[7] ‚ÄúConvolution - Wikipedia, the free encyclopedia,‚Äù  \\nhttps://en.wikipedia.org/wiki/Convolution, Accessed: 2016-01-19. \\n[8] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang and E. J. Keogh, \\n‚ÄúQuerying and mining of time series data: experimental comparison of \\nrepresentations and distance measures,‚Äù PVLDB 1(2): 1542-1552. 2008. \\n[9] J. Hughes et al. , ‚ÄúChimpanzee and human Y chromosomes are \\nremarkably divergent in structure‚Äù Nature 463, (2010). \\n[10] H. Lee, R. Ng and K. Shim, ‚ÄúSimilarity join size estimation using \\nLocality sensitive hashing,‚Äù PVLDB, 4(6):338‚Äì349, 2011. \\n[11] W. Luo, H. Tan, H. Mao  and L. M.  Ni, ‚ÄúEfficient similarity joins on \\nmassive high-dimensional datasets using mapreduce,‚Äù In MDM'12, IEEE \\nComputer Society, pp. 1-10. \\n[12] Y. Ma, X. Meng and S. Wang, ‚ÄúParallel similarity joins on massive high-\\ndimensional data using MapReduce ,‚Äù Concurrency and Computation , \\nVolume 28, Issue 1 Jan 2016. Pages 166‚Äì183. \\n[13] S. V. Makonin, ‚ÄúAMPds: a public dataset for load disaggregation and \\neco-feedback research,‚Äù EPEC 2013, pp 1-6. \\n[14] MASS: http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html \\n[15] G. D. F. Morales and A. Gionis, ‚Äústreaming similarity self-join,‚Äù Proc. \\nVLDB Endow, 2016. \\n[16] Motion Capture Database, http://mocap.cs.cmu.edu/ \\n[17] A. Mueen, H. Hamooni and T. Estrada, ‚ÄúTime series join on subsequence \\ncorrelation,‚Äù IEEE ICDM 2014, pp. 450-459. \\n[18] A. Mueen, E. Keogh, Q. Zhu, S. Cash and B. Westover, ‚ÄúExact discovery \\nof time series motif,‚Äù SDM 2009. \\n[19] D. Murray et al. , ‚ÄúA data management platform for personalised real-\\ntime energy feedback,‚Äù In EEDAL 2015. \\n[20] V. Niennattrakul et al, ‚ÄúData editing techniques to allow the application \\nof distance-based outlier detection to streams,‚Äù ICDM 2010: 947-952. \\n[21] D. Patnaik, et al, ‚ÄúSustainable operation and management of data center \\nchillers using temporal data mining,‚Äù KDD 2009. \\n[22] T. Rakthanmanon et al., ‚ÄúSearching and Mining Trillions of Time Series \\nSubsequences under Dynamic Time Warping,‚Äù In KDD 2012, 262-270. \\n[23] T. Rakthanmanon and E. Keogh, ‚ÄúFast shapelets: a scalable algorithm for \\ndiscovering time series shapelets,‚Äù SDM, 2013. \\n[24] Supporting page. http://www.cs.ucr.edu/~eamonn/MatrixProfile.html \\n[25] C. D. Truong and D. T.  Anh, ‚ÄúAn Efficient Method for Motif and \\nAnomaly Detection in Time Series‚Äù IJBIDM, Vol. 10, No. 4, 2015. \\n[26] A. Tucker and X. Liu, ‚ÄúA Bayesian  Network Approach to Explaining \\nTime Series with Changing Structure,‚Äù Intell Data Anal, 8 (5) (2004). \\n[27] L. Ye and E. Keogh, ‚ÄúTime series shapelets: a new primitive for data \\nmining,‚Äù ACM SIGKDD, 2009, pp 947-56. \\n[28] C. Yoon, O. O‚ÄôReilly, K. Bergen and G. Beroza, ‚ÄúEarthquake detection \\nthrough computationally efficient similarity search,‚Äù Sci. Adv. 2015. \\n[29] F. Zhou, F. Torre and J.  Hodgins ‚Äú Aligned Cluster Analysis for \\nTemporal Segmentation of Human Motion,‚Äù IEEE FG'2008. \\n[30] S. Zilberstein and S. Russell, ‚ÄúApproximate Reasoning Using Anytime \\nAlgorithms,‚Äù In Imprecise and Approximate Computation , Kluwer \\nAcademic Publishers, 1995. \\n \\n0 5,000 10,000\\nMatrix  Profile\\nSplit Points   \\nPrediction\\nOne-dimension of multi-d time series: Subject 86, recording 4, dimension 30\\nW S P N      W C T           D            P         W\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Matrix Profile I: All Pairs Similarity Joins for Time Series: \\nA Unifying View that Includes Motifs, Discords and Shapelets \\nChin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,  \\nHoang Anh Dau, ‚Ä†Diego Furtado Silva, ‚Ä°Abdullah Mueen, and Eamonn Keogh \\nUniversity of California, Riverside, ‚Ä†Universidade de S√£o Paulo, ‚Ä°University of New Mexico \\n{myeh003, yzhu015, lulan001, nbegu001, yding007, hdau001}@ucr.edu, diegofsilva@icmc.usp.br, mueen@unm.edu, eamonn@cs.ucr.edu \\n \\nAbstract‚Äî The all-pairs-similarity-search (or similarity join) \\nproblem has been extensively studied for text and a handful of \\nother datatypes. However, surprisingly little progress has been  \\nmade on similarity joins for time series subsequences. The lack of  \\nprogress probably stems from the daunting nature of the \\nproblem. For even modest sized data sets the obvious nested -loop \\nalgorithm can take months, and the typical speed -up techniques \\nin this domain (i.e., indexing, lower -bounding, triangular -\\ninequality pruning and early abandoning) at best produce one or \\ntwo orders of magnitude speedup. In this work we introduce a \\nnovel scalable algorithm for time series subsequence all -pairs-\\nsimilarity-search. For exceptionally large datasets, the algorithm \\ncan be trivially cast as an anytime algorithm and produce high -\\nquality approximate solutions in reasonable  time. The exact \\nsimilarity join algorithm computes the answer to the time series \\nmotif and time series discord  problem as a side -effect, and our \\nalgorithm incidentally provides the fastest known algorithm for \\nboth these  extensively-studied problems. We de monstrate the \\nutility of our ideas for many time series data mining problems, \\nincluding motif discovery, novelty discovery,  shapelet discovery,  \\nsemantic segmentation, density estimation, and contrast set \\nmining.    \\nKeywords‚ÄîTime Series; Similarity Joins; Motif Discovery \\nI. INTRODUCTION \\nThe all-pairs-similarity-search (also known as similarity \\njoin) problem comes in several variants. The basic task is this: \\nGiven a collection of data objects, retrieve the nearest neighbor \\nfor each object . In the text domain the  algorithm has \\napplications in a host of problems, including community \\ndiscovery, duplicate detection, collaborative filtering, \\nclustering, and query refinement [1]. While virtually all text \\nprocessing algorithms have analogues in time series data \\nmining, there has been surprisingly little progress on Time \\nSeries subsequences All-Pairs-Similarity-Search (TSAPSS). \\nWe believe that this lack of progress stems not from a lack \\nof interest in this useful primitive, but from the daunting nature \\nof the problem. Consider the following example that reflects the \\nneeds of an industrial collaborator. A boiler at a che mical \\nrefinery reports pressure once a minute. After a year, we have a \\ntime series of length 525,600. A plant manager may wish to do \\na similarity self-join on this data with week -long subsequences \\n(10,080) to discover operating regimes (summer vs. winter o r \\nlight distillate vs. heavy distillate etc.) The obvious nested loop \\nalgorithm requires 132,880,692,960 Euclidean distance \\ncomputations. If we assume each one takes 0.0001 seconds, \\nthen the join will take 153.8 days. The core contribution of this \\nwork is to show that we can reduce this time to 6.3 hours, using \\nan off-the-shelf desktop computer. Moreover, we show that this \\njoin can be computed and/or updated incrementally. Thus we \\ncould maintain this join essentially forever on a standard \\ndesktop, even if the data arrival frequency was much faster than \\nonce a minute. \\nOur algorithm uses an ultra -fast similarity search algorithm \\nunder z -normalized Euclidean distance as a subroutine, \\nexploiting the overlap between subsequences using the classic \\nFast Fourier Transform (FFT) algorithm.  \\nOur method has the following advantages/features: \\n\\uf0b7 It is exact, providing no false positives or false dismissals. \\n\\uf0b7 It is simple and parameter -free. In contrast, the more \\ngeneral metric space APSS algorithms require building and \\ntuning spatial access methods and/or hash functions.  \\n\\uf0b7 Our algorithm requires an inconsequential space overhead, \\njust O(n) with a small constant factor. \\n\\uf0b7 While our exact algorithm is extremely scalable, for \\nextremely large datasets we can compute the results in an \\nanytime fashion, allowing ultra-fast approximate solutions.  \\n\\uf0b7 Having computed the similarity join for a dataset, we can \\nincrementally upd ate it very efficiently. In many domains \\nthis means we can effectively maintain exact joins on \\nstreaming data forever.  \\n\\uf0b7 Our method provides full joins, eliminating the need to \\nspecify a similarity threshold, which as we will show, is a \\nnear impossible task in this domain.  \\n\\uf0b7 Our algorithm is embarrassingly parallelizable, both on \\nmulticore processors and in distributed systems. \\nGiven all these features, our algorithm has implications for \\nmany time series data mining tasks [5][18][28].  \\nThe rest of the paper is organized as follows. Section II \\nreviews related work an d introduces the necessary background \\nmaterials and definitions. In Section III we introduce our \\nalgorithm and its anytime and incremental variants. Section IV \\nsees a detailed empirical evaluation of our algorithm and shows \\nits implications for many data mining tasks. Finally, in Section \\nV we offer conclusions and directions for future work. \\nII. RELATED WORK AND BACKGROUND \\nThe basic variant of  similarity join problem we are \\ninterested in is as follows : Given a collection of data objects, \\nretrieve the nearest neighbor for every object.   \\nOther common variants include retrieving the top-K nearest \\nneighbors or the nearest neighbor for each object if that \\nneighbor is within a user-supplied threshold, œÑ. (Such variations \\nare trivial generalizations of our proposed algorithm, so we \\nomit them from further discussion). The latter variant results in \\na much easier problem, provided that the threshold is small. For \\nexample, [1] notes that virtually all research efforts ‚Äú exploit a \\nsimilarity threshold more aggressively in order to limit the set'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='of candidate pairs that are considered..  [or] ... to reduce the \\namount of information indexed in the first place.‚Äù \\nThis critical dependence on œÑ is a major issue for text joins, \\nas it is known that ‚Äú join size can change dramatically \\ndepending on the input similarity threshold ‚Äù [10]. However, \\nthis issue is even more critical for time series for two reasons. \\nFirst, unlike similarity (which is bounded between zero and \\none), the Euclidean distance is effectively unbounded, and \\ngenerally not intuitive. For example, if two heartbeats have a \\nEuclidean distance of 17.1, are they similar? Even for a domain \\nexpert that knows the sampling rate and the noise level of the \\ndata, this is not obvious. Second, a single threshold can produce \\nradically different output sizes, even for datasets that are very \\nsimilar.  Consider Figure 1 which shows the output size vs. \\nthreshold setting for the first and s econd halves of a ten -day \\nperiod monitoring data center chillers [21]. For the first five \\ndays a threshold of 0.6 would return zero items, but for the \\nsecond five days the same setting would return 108 items.  This \\nshows the difficulty in selecting  an appropriate threshold. Our \\nsolution is to have no threshold, and do a full join. \\n \\nFigure 1.  Output size vs. threshold for data center chillers [21]. Values beyond \\n2.0 are truncated for clarity (but archived at [24]).  \\nA handful of efforts have considered joins on time series, \\nachieving speedup by (in addition to the use of MapReduce) \\nconverting the data to lower -dimensional representations such \\nas PAA [11] or SAX [12] and exploiting lower bounds and/o r \\nLocality Sensitive Hashing (LSH) to prune some calculations. \\nHowever, the methods are very complex, with many (10 -plus) \\nparameters to adjust. As [11] acknowledges with admirable \\ncandor, ‚ÄúReasoning about the optimal settings is not trivial.‚Äù In \\ncontrast, our proposed algorithm has zero parameters to set. \\nA very recent research effort [28] has tackled the scalability \\nissue by converting the real -valued time series into discrete \\n‚Äúfingerprints‚Äù before using a LSH approach, much like the text \\nretrieval community [1]. They produced impressive speedup, \\nbut they also experienced false negatives. Moreover, the \\napproach has several parameters that need to be set; for \\nexample, they need to set the threshold to a very precise 0.818.  \\nAs we shall show, our algorithm allows both anytime and \\nincremental (i.e. streaming) versions. While a streaming join \\nalgorithm for text was recently introduced [15], we are not \\naware of any such algorithms for time series data or general \\nmetric spaces. More generally, there is a large amount of \\nliterature on joins for text processing [1]. Such work is \\ninteresting, but of little utility given our constraints, data type \\nand problem setting. We require full joins, not threshold joins, \\nand we are unwilling to allow the possibility of false negatives. \\nA. Definitions and Notation \\nWe begin by defining the data type of interest, time series: \\nDefinition 1:  A time series T is a sequence of real -valued \\nnumbers ti: T = t1, t2, ..., tn where n is the length of T. \\nWe are not interested in the global properties of time series, \\nbut in the similarity between local subsequences:  \\nDefinition 2:  A subsequence Ti,m of a T is a continuous \\nsubset of the values from T of length m starting from \\nposition i.   Ti,m = ti, ti+1,‚Ä¶, ti+m-1, where 1 ‚â§  i ‚â§  n-m+1.  \\nWe can take any subsequence from a time series and \\ncompute its distance to all sequences. We call an ordered vector \\nof such distances a distance profile: \\nDefinition 3:  A distance profile D is a vector of the \\nEuclidean distances between a given query and each \\nsubsequence in an all-subsequences set (see Definition 4).  \\nNote that we are assuming that the distance is measured \\nusing the Euclidean distance between the z -normalized \\nsubsequences [8]. The distance profile can be considered a meta \\ntime series that annotates the time series T that was used to \\ngenerate it. The first three definitions are illustrated in Figure 2. \\n \\nFigure 2. A subsequence Q extracted from a time series T is used as a query to \\nevery subsequence in T. The vector of all distances is a distance profile. \\nNote that if the query and all-subsequences set belong to the \\nsame time series, the distance profile must be zero at the \\nlocation of the query, and close to zero just before and just \\nafter. Such matches are c alled trivial matches in the literature \\n[18], and are avoided by ignoring an exclusion zone (shown as \\na gray region) of m/2 before and after the location of the query. \\nWe are interested in similarity join of all subsequences of a \\ngiven time series. We define an all-subsequences set of a given \\ntime series as a set that contains all possible subsequences from \\nthe time series. The notion of all-subsequences set is purely for \\nnotational purposes. In our implementation, we do not actually \\nextract the subsequences in this form as it would require \\nsignificant time and space overhead. \\nDefinition 4: An all-subsequences set A of a time series T \\nis an ordered set of all possible subsequences of T obtained \\nby sliding a window of length m across T: A ={T1,m,, \\nT2,m,‚Ä¶, Tn-m+1,m}, where m is a user -defined subsequence \\nlength. We use A[i] to denote Ti,m. \\nWe are interested in the nearest neighbor (i.e., 1NN) relation \\nbetween subsequences; therefore, we define a 1NN-join \\nfunction which indicates the nearest neighbor relation between \\nthe two input subsequences. \\nDefinition 5:  1NN-join function :  given two all -\\nsubsequences sets A and B and two subsequences A[i] and \\nB[j], a 1NN-join function  Œ∏1nn (A[i], B[j]) is a Boolean \\nfunction which returns ‚Äútrue‚Äù only if B[j] is the nearest \\nneighbor of A[i] in the set B. \\nWith the defined join function, a similarity join set  can be \\ngenerated by applying the similarity join operator on two input \\nall-subsequences sets. \\nDefinition 6: Similarity join set : given all -subsequences \\nsets A and B, a similarity join set  JAB of A and B is a set \\ncontaining pairs of each subsequence in A with its nearest \\nneighbor in B: JAB={‚å© A[i], B[j] ‚å™ |Œ∏1nn (A[i], B[j])}.  We \\ndenote this formally as JAB = A‚ãà\\uf0711nnB. \\n0\\n400\\n800\\n1200\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\\nFirst 5 Days\\nSecond 5 Days\\nData Center Chillers\\nT, a snippet of an energy \\nconsumption\\n2,0000 m/2m/2\\nQ, query of length m\\nNote that |D| = |T|-|Q|+1D, a distance profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='We measure the Euclidean distance between each pair \\nwithin a similarity join set and store the resultants into an \\nordered vector. We call the result vector the matrix profile. \\nDefinition 7:  A matrix profile (or just profile) PAB is a \\nvector of the Euclidean distances between each pair in JAB. \\nWe call this vector the matrix profile because one \\n(inefficient) way to compute it would be to compute the full \\ndistance matrix of all the subsequences in one time series with \\nall the subsequence in another time series and extract the \\nsmallest value in each row (the smallest non-diagonal value for \\nthe self-join case).  In Figure 3 we show the matrix profile of \\nour running example. \\n \\nFigure 3. A time series T, and its self-join matrix profile P.  \\nLike the distance profile, the matrix profile can be \\nconsidered a meta time series annotating the time series T if the \\nmatrix profile is generated by joining T with itself. The profile \\nhas a host of interesting and exploitable properties. For \\nexample, the highest point on the profile corresponds to the \\ntime series discord [5], th e ( tied) lowest points correspond to \\nthe locations of the best time series motif pair [18], and the \\nvariance can be seen as a measure of the T‚Äôs com plexity. \\nMoreover, the histogram of the values in the matrix profile is \\nthe exact answer to the time series density estimation [4]. \\nWe name this special case of the similarity join set \\n(Definition 6) as self-similarity join set, and the corresponding \\nprofile as self-similarity join profile. \\nDefinition 8:  A self-similarity join  set JAA is a result of \\nsimilarity join of the set  A with itself . We denote this \\nformally as JAA = A ‚ãà\\uf0711nn A. We denote the corresponding \\nmatrix profile or self-similarity join profile as PAA. \\nNote that we exclude trivial matches when self -similarity \\njoin is performed, i.e., if A[i] and A[j] are subsequences from \\nthe same all -subsequences set A, Œ∏1nn (A[i], B[j]) is ‚Äúfalse‚Äù \\nwhen A[i] and A[j] are a trivially matched pair. \\nThe ith element in the matrix profile tells us the Euclidean \\ndistance to the nearest neighbor of the subsequence of T, \\nstarting at i.  However, it does not tell us where that neighbor is \\nlocated. This information is recorded in matrix profile index. \\nDefinition 9: A matrix profile index IAB of a similarity join \\nset JAB is a vector of integers where IAB[i] = j if {A[i], B[j]} \\n‚àà JAB. \\nBy storing the neighboring information this way, we can \\nefficiently retrieve the nearest neighbor of A[i] by accessing the \\nith element in the matrix profile index. \\nNote that the function which computes the similarity join set \\nof two input time series  is not symmetric; therefore, JAB ‚â† JBA,  \\nPAB ‚â† PBA, and IAB ‚â† IBA. \\nFor ease of presentation, we have confined this work to the \\nsingle dimensional case; however, nothing intrinsically \\nprecludes generalizations to multidimensional data. \\nSummary of the Previous Section \\nThe previous section was rather dense, so before moving on \\nwe summarize the main takeaway points. We can create two \\nmeta time series, the matrix profile and the matrix profile index, \\nto annotate a time series TA with the distance and location of all \\nits subsequences nearest neighbors in itself or another time \\nseries TB. These two data objects explicitly or implicitly contain \\nthe answers to many time series data mining tasks. However, \\nthey appear to be too expensive to compute to be practica l. In \\nthe next section we will show an algorithm that can compute \\nthese efficiently. \\nIII. ALGORITHMS \\nWe are finally in a position to explain our algorithm s. We \\nbegin by stating the fundamental intuition, which stems from \\nthe relationship between distance profiles and the matrix \\nprofile. As Figure 2 and Figure 3 visually suggest, all distance \\nprofiles (excluding the trivial match region) are upper bound \\napproximations to the matrix profile. More critically, if we \\ncompute all the distance profiles, an d take the minimum value \\nat each location, the result is the matrix profile! \\nThis tells us that if we have a fast way to compute the \\ndistance profiles, then we also have a fast way to compute the \\nmatrix profile. As we shall show in the next section, we have an \\nultra-fast way to compute the distance profiles. \\nA. Mueen‚Äôs ultra-fast Algorithm for Similarity Search (MASS) \\nWe begin by introducing a novel Euclidean distance \\nsimilarity search algorithm for time series data. The algorithm \\ndoes not just find the nearest neighbor to a query and return its \\ndistance; it returns the distance to every subsequence. In \\nparticular, it computes the distance profile, as shown in Figure \\n2. The algorithm requires just O( nlogn) time by exploiting the \\nFFT to calculate  the dot products between the query and all \\nsubsequences of the time series.  \\nWe need to carefully qualify the claim of ‚Äúultra-fast‚Äù. There \\nare dozens of algorithms for time series similarity search that \\nutilize index structures to efficiently locate neighbors [8]. While \\nsuch algorithms can be faster in the  best case , all these \\nalgorithms degenerate to brute force search in the worst case 1 \\n(actually, much worse than brute force search due to the \\noverhead of the index). Likewise, there are index -free methods \\nthat achieve speed -up using various early abandoning tricks \\n[22], but they too degrade to brute force search in the worst \\ncase. In contrast, the performance of the algorithms outlined in \\nTABLE I and TABLE II is completely independent of the data. \\nTABLE I. CALCULATION OF SLIDING DOT PRODUCTS \\nProcedure SlidingDotProduct(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: The dot product between Q and all subsequences in T \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nn ‚Üê Length(T), m ‚Üê Length(Q) \\nTa ‚Üê Append T with n zeros   \\nQr ‚Üê Reverse(Q)  \\nQra ‚Üê Append Qr with 2n-m zeros \\nQraf ‚Üê FFT(Qra), Taf ‚Üê FFT(Ta) \\nQT ‚Üê InverseFFT(ElementwiseMultiplication(Qraf, Taf)) \\nreturn QT \\n \\n1 There are many such worse case scenarios, including high levels of noise blurring \\nthe distinction between closest and furthest neighbors, and rendering triangular -\\ninequality pruning and early abandoning worthless. \\n2,0000\\nP, a matrix profile\\nT, a snippet of an energy \\nconsumption\\nNote that |P| = |T|-|Q|+1'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Line 1 determines the length of both the time series T and \\nthe query Q. In line 2, we use that information to append T with \\nan equal number of zeros. In line 3, we obtain the mirror image \\nof the original query. This reversing ensures that a convolution \\n(i.e. ‚Äúcrisscrossed‚Äù multiplication) essentially produces in-order \\nalignment. Because we require both vectors to be the s ame \\nlength, in line 4 we append enough zeros to the (now reversed) \\nquery so that, like Ta, it is also of length 2 n. In line 5, the \\nalgorithm calculates Fourier transforms of the appended -\\nreversed query (Qra) and the appended time series Ta. Note that \\nwe use FFT algorithm which is an O(nlogn) algorithm. The Qraf \\nand the Taf produced in line 5 are vectors of complex numbers \\nrepresenting frequency components of the two time series. The \\nalgorithm calculates the element-wise multiplication of the two \\ncomplex vectors and performs inverse FFT on the product. \\nLines 5-6 are the class ic convolution operation on two vectors \\n[7]. Figure 4 shows a toy example of the sliding dot product \\nfunction in work. The algorithm time complexity does not \\ndepend on the length of the query (m). \\n \\nFigure 4. A toy example of convolution operation being used to calculate  \\nsliding dot products for time series data. Note the reverse and append \\noperation on T and q in the input. Fifteen dot products are calculated for every \\nslide. The cells m = 2 to n = 4 from left ( red/bold arrows) contain valid \\nproducts. TABLE II takes this subroutine and uses it to create a distance \\nprofile (see Definition 3). \\nIn line 1 of TABLE II, we invoke the dot products code \\noutlined in TABLE I. The formula to calculate the z-normalized \\nEuclidean distance D[i] between two time series subsequence Q \\nand Ti,m using their dot product, QT[i] is (see [24] for \\nderivation): \\nùê∑[ùëñ] = ‚àö2ùëö(1‚àíùëÑùëá[ùëñ]‚àíùëöùúáùëÑùëÄùëá[ùëñ]\\nùëöùúéùëÑùõ¥ùëá[ùëñ] ) \\nwhere m is the subsequence length, ŒºQ is the mean of Q, \\nMT[i] is the mean of Ti,m, œÉQ is the standard deviation of Q, and  \\nŒ£T[i] is the standard deviation of Ti,m. Normally, it takes O( m) \\ntime to calculate the mean and standard deviation for every \\nsubsequence of a long time series. However, here we exploit a \\ntechnique noted in [22] in a different context. We cache \\ncumulative sums of the values and square of the values in the \\ntime series. At any stage the two cumulative sum vectors are \\nsufficient to calculate the mean an d the standard deviation of \\nany subsequence of any length.  See [14] for an elaborate \\ndescription and variations of MASS. \\nTABLE II. MUEEN‚ÄôS ALGORITHM FOR SIMILARITY SEARCH (MASS) \\nProcedure MASS(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: A distance profile D of the query Q  \\n1 \\n2 \\n3 \\n4 \\nQT ‚Üê SlidingDotProducts(Q, T) \\nŒºQ, œÉQ, ŒúT, Œ£T  ‚Üê ComputeMeanStd(Q, T)    // see [22] \\nD ‚Üê CalculateDistanceProfile(Q, T, QT, ŒºQ, œÉQ, ŒúT, Œ£T) \\nreturn D \\nUnlike the dozens of time series KNN search algorithms in \\nthe literature [8], this algorithm calculates the distance to every \\nsubsequence, i.e. the distance profile  of time series T. \\nAlternatively, in join nomenclature, the algorithm produces one \\nfull row of the all -pair similarity matrix. Thus, as we show in \\nthe next section, our join algorithm is simply a loop that \\ncomputes each full row of the all -pair similarity matrix and \\nupdates the current ‚Äúbest-so-far‚Äù matrix profile when needed. \\nB. The STAMP Algorithm \\nWe call our join algorithm STAMP, Scalable Time series \\nAnytime Matrix Profile. The algorithm is outlined in TABLE \\nIII. In line 1, we extract the length of TB. In line 2, we allocate \\nmemory and initial matrix profile PAB and matrix profile index \\nIAB. From lines 3 to line 6, we calculate the distance profiles D \\nusing each subsequence B[idx] in the time series TB and the \\ntime series TA. Then, we perform pairwise minimum for each \\nelement in D with the paired element in PAB (i.e., min( D[i], \\nPAB[i]) for i = 0 to length(D) - 1.) We also update IAB[i] with idx \\nwhen D[i] ‚â§ PAB[i] as we perform the  pairwise minimum \\noperation. Finally, we return the result PAB and IAB in line 7.  \\nNote that the algorithm presented in TABLE III computes \\nthe matrix profile for the general similarity join. To modify the \\ncurrent algorithm to compute  the self -similarity join matrix \\nprofile of a time series TA, we simply replace TB in line 1 with \\nTA, replace B with A in line 4, and ignore trivial match in D \\nwhen performing ElementWiseMin in line 5. \\nTABLE III. THE STAMP ALGORITHM \\nProcedure STAMP(TA, TB, m) \\nInput: Two user provided time series, TA and TB, interested \\nsubsequence length m \\nOutput: A matrix profile PAB and associated matrix profile index IAB of \\nTA join TB, JAB = A‚ãà\\uf0711nnB \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nnB ‚Üê Length(TB) \\nPAB ‚Üê infs, IAB ‚Üê zeros, idxes ‚Üê 1:nB-m+1 \\nfor each idx in idxes    // In any order, but random for anytime algorithm \\n          D ‚Üê MASS(B[idx], TA) \\n          PAB, IAB ‚Üê ElementWiseMin(PAB, IAB, D, idx) \\nend for \\nreturn PAB, IAB \\nTo parallelize the STAMP algorithm for multicore \\nmachines, we simply distribute the indexes to secondary \\nprocess run in each core, and the secondary processes use the \\nindexes they received to update their own PAB and IAB. Once the \\nmain process returns from  all secondary processes, we use \\nElementWiseMin to merge the received PAB and IAB.  \\nC. An Anytime Algorithm for TSAPSS \\nWhile the exact algorithm introduced in the previous section \\nis extremely scalable, there will always be datasets for which \\ntime needed for an exact solution is untenable. We can mitigate \\nthis by computing the results in an anytime fashion, allowing \\nfast approximate solutions [30]. To a dd the anytime nature to \\nthe STAMP algorithm, we simply ensure a randomized search \\norder in line 2 of TABLE III.  \\nWe can compute a (post-hoc) measurement of the quality of \\nan anytime solution by measuring the Root -Mean-Squared-\\nError (RMSE) between the true matrix profile and the current \\nbest-so-far mat rix profile. As Figure 5 suggests, with an \\nexperiment on random walk data, the algorithm converges very \\nquickly. \\nQ2T1 0 00 00 00 00 0Q1T4Q2T2+Q1T1 Q2T3+Q1T2 Q2T4+Q1T3\\nOutput\\nInputT2T1 T4T3 00 00 Q1Q2 00 00 00'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Figure 5. main) The decre ase in RMSE as the STAMP algorithm updates \\nmatrix profile with the distance profile calculated at each iteration.  inset) The \\napproximate matrix profile at the 10% mark is visually indistinguishable from \\nthe final matrix profile. \\nZilberstein [30] gives a number of desirable properties of \\nanytime algorithms, including Low Overhead , Interruptibility, \\nMonotonicity, Recognizable Quality, Diminishing Returns and \\nPreemptability (these properties are mostly obvious from their \\nnames, but full definitions are at [30]). \\nBecause each subsequence‚Äôs distance profile is bounded \\nbelow by the exact matrix profile, updating an approximate \\nmatrix profile with a distance profile with pairwise minimum \\noperation either drives the approximate solution closer the exact \\nsolution or retains the current approximate solution. Thus , we \\nhave guaranteed Monotonicity. From Figure 5, the approximate \\nmatrix profile converges to the exact matrix profile \\nsuperlinearly; therefore, we have strong Diminishing Returns. \\nWe can easily achieve Interruptibility and Preemptability by \\nsimply inserting a few lines of code between lines 5 and 6 of \\nTABLE III that read: \\n5new \\n6new \\n7new \\nif CheckForUserInterrupt  = TRUE \\n          Report({PAB, IAB}, ‚ÄòHere is an approximate answer.‚Äô) \\nif GetUserChoice = ‚Äòfurther refine‚Äô, CONTINUE, else BREAK \\nThe space and time overhead for the anytime property is \\neffectively zero, thus we have Low Overhead. This leaves only \\nthe property of Recognizable Quality. Here we must resort to a \\nprobabilistic argument.  The convergence curve shown in  \\nFigure 5 is very typical, so we could use past convergence \\ncurves to predict the quality of solution when interrupted on \\nsimilar data.  \\nD. Time and Space Complexity \\nThe overall complexity of the proposed algorithm is \\nO(n2logn) where n is the length of the time series. However, our \\nexperiments (see Section 4.1) empirically suggest that the \\nruntime of  STAMP‚Äôs growth rate is roughly O( n2) instead of \\nO(n2logn). One possible e xplanation for this is that the nlogn \\nfactor comes from the FFT subroutine. Because FFT is so \\nimportant in many applications, it is extraordinarily well \\noptimized. Thus, the empirical runtime is very close to linear. \\nIn contrast to the above, the brute for ce nested loop \\napproach has a time complexity of O(n2m). Recall the industrial \\nexample in the introduction section. We have  m = 10,080, but \\nlog(n) = 5.7, so we would expect our approach to be about \\n1,768 times faster. In fact, we are empirically even faster. The \\ncomplexity analysis downplays the details of important constant \\nfactors. The nested loop algorithm must also z -normalize the \\nsubsequences. This either requires O( nm) time, but with an \\nuntenable O(nm) space overhead, or an O( n2m) time overhead. \\nAnd r ecall that this is before a single Euclidean distance \\ncalculation is performed.  \\nFinally, we mention one quirk of our algorithm which we \\ninherit from using the highly optimized FFT subroutine. Our \\nalgorithm is fastest when n is an integer power of two, slower \\nfor non -power of two but composite numbers, and slowest \\nwhen n is prime. The difference (for otherwise similar values of \\nn) can approach a factor of 1.6x. Thus, where possible, it is \\nworth contriving the best case by tru ncation or zero-padding to \\nthe nearest power of two. \\nE. Incrementally Maintaining TSAPSS \\nUp to this point we have discussed the batch version of \\nTSAPSS. By batch, we mean that the STAMP algorithm needs \\nto see the entire time series TA and TB (or just TA if we are \\ncalculating the self -similarity join matrix profile) before \\ncreating the matrix profile. However, it would be advantageous \\nif we could build the matrix profile incrementally. Given that \\nwe have performed a batch construction of matrix profile, i f \\nnew data arrives, it would clearly be preferable to incrementally \\nadjust the current profile, rather than start from scratch.  \\nBecause the matrix profile solves both the times series motif \\nand the time series discord problems, an incremental version of \\nSTAMP would automatically provide the first incremental \\nversions of both algorithms. We call such an algorithm the \\nSTAMPI (STAMP Incremental) algorithm. \\nWe will demonstrate our ability to incrementally maintain \\nthe matrix profile in this section. For simpli city and brevity \\nTABLE IV only shows the algorithm to maintain the self -\\nsimilarity join. The generalizations are obvious. \\nTABLE IV. THE STAMPI ALGORITHM \\nProcedure STAMPI(TA, t, PAA, IAA) \\nInput: The original time series TA, a new data point t following TA, the \\nmatrix profile PAA and its associated matrix profile index IAA of TA.  \\nOutput: The incrementally updated matrix profile PAA,new and its matrix \\nprofile index IAA,new of the current time series TA,new= TA, t. \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nTA,new = [TA, t] \\nS ‚Üê last subsequence in TA,new, idx ‚Üê index of S in TA,new \\nD ‚Üê MASS (S, TA) \\nPAA, IAA ‚Üê ElementWiseMin(PAA, IAA, D, idx) \\npAA,last, iAA,last ‚Üê FindMin(D) \\nPAA,new ‚Üê [PAA, pAA,last], IAA,new ‚Üê [IAA, iAA,last] \\nreturn PAA,new, IAA,new \\nFor clarity, we denote the updated time series as TA,new, the \\nupdated matrix profile as PAA,new and the associated matrix \\nprofile index as IAA,new. As each additional data point t arrives, \\nthe size of the time series TA increases by one, and a new \\nsubsequence S is generated at the end of TA,new. In line 3 we \\nobtain the distance profile of S with regard to TA. Then, as in the \\noriginal STAMP algorithm,  in line 4 we perform  a pairwise \\ncomparison between every element in D with the corresponding \\nelement in PAA to see if the corresponding element in PAA needs \\nto be updated. In line 5, we find the nearest neighbor of S and \\nthe associated index by evaluating the minimum value of D. \\nFinally, in line 6, we obtain the new matrix profile and \\nassociated matrix profile index by concatenating the results in \\nline 4 and line 5.  \\nThe time complexity of the STAMP I algorithm is O(nlogn) \\nwhere n is the length of size of th e current time series TA. Note \\nthat as we maintain the profile, each incremental call of \\nSTAMPI requires invoking the FFT subroutine with a slightly \\nlonger time series ( n becomes n+1 ). Thus it gets very slightly \\nslower at each time step. Therefore, the best way to measure the \\nperformance is to ask for the Maximum Time Horizon (MTH), \\nin essence the answer to this question: ‚Äú Given this arrival rate, \\nhow long can we maintain the profile before we can no longer \\nupdate fast enough?‚Äù \\nNote that the subsequence length m is not considered in the \\nMTH evaluation. Recall that overall time complexity of the \\n10,0000\\nRMSE\\niteration1,000\\napproximate matrix \\nprofile at 1,000 iterations.\\nexact matrix profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='algorithm is determined by the efficiency of the FFT \\nsubroutine, which is independent of m. We have computed the \\nMTH for two common scenarios of interest to the community. \\n\\uf0b7 House Electrical Demand  [19]: This dataset is updated \\nevery eight seconds. By iteratively calling the STAMP I \\nalgorithm, we can maintain the profile for 5.8 years.  \\n\\uf0b7 Oil Refinery :  Most telemetry in oil refineries is sampled \\nat once a minute [26]. The relatively low sampling rate \\nreflects the ‚Äúinertia‚Äù of massive boilers/condensers. Even if \\nwe maintain the profile for 40 years, the update time is only \\naround 6.78 seconds. Moreover, th e raw data, matrix \\nprofile and index would only require 0.5 gigabytes of main \\nmemory. Thus the MTH here is forty plus years. Given \\nprojected improvements in hardware, this effectively \\nmeans we can maintain the matrix profile forever. \\nAs impressive as these  numbers are, they are actually quite \\npessimistic. For simplicity we assume that every value in the \\nmatrix profile index will be updated at each time step.  \\nHowever, empirically, much less than 0.1% of them need to be \\nupdated. If it is possible to prove an upper bound on the number \\nof changes to the matrix profile index per update, then we could \\ngreatly extend the MTH  or handle much faster sampling rates. \\nWe leave such considerations for future work. \\nIV. EXPERIMENTAL EVALUATION \\nWe begin by stating our experimen tal philosophy. We have \\ndesigned all experiments such that they are easily reproducible. \\nTo this end, we have built a webpage [24] which contains al l \\ndatasets and code used in this work, together with spreadsheets \\nwhich contain the raw numbers and some supporting videos.  \\nGiven page limits, and the utility of our algorithm for a host \\nof existing (and several new) data mining tasks, we have chosen \\nto c onduct exceptionally broad  but shallow experiments. We \\nhave conducted such deep detailed experiments and placed \\nthem at [24]. Unless otherwise state d we measure wall clock \\ntime on an Intel i7@4GHz with 4 cores. \\nA. Scalability of Profile-Based Self-Join \\nBecause the time performance of STAMP is independent of \\nthe data quality or any user inputs (there are none except the \\nchoice of m, which does not affect the speed), our scalability \\nexperiments are unusually brief.  In TABLE V we show the \\ntime required for a self -join with m fixed to 256, for \\nincreasingly long time series.  \\nTABLE V.  TIME REQUIRED FOR A SELF-JOIN WITH M = 256, VARYING N \\nValue of n 217 218 219 220 221 \\nTime Required 15.1 min 70.4 min 5.4 hours 24.4 hours 4.2 days \\nIn TABLE VI, we show the time required for a self -join \\nwith n fixed to 2 17, for increasing long m. Again recall that \\nunlike virtually all other time series data mining algorithms in \\nthe literature whose performance degrades for longer \\nsubsequences [8][18], the running time of STAMP does not \\ndepend on m. \\nTABLE VI. TIME REQUIRED FOR A SELF-JOIN WITH N= 217, VARYING M \\nValue of m 64 128 256 512 1,024 \\nTime Required 15.1 min 15.1 min 15.1 min 15.0 min 14.5 min \\nFinally, we further exploit the simple parallelizability of the \\nalgorithm by using four 16 -core virtual machines on Microsoft \\nAzure to redo the two -million join ( n = 2 21 and m = 256) \\nexperiment. By scaling up the computational power, we have \\nreduced the running time from 4.2 days to just 14.1 hours. This \\nuse of cloud computing required writing just few dozen lines of \\nsimple additional code [24]. \\nIt is difficult to find good baselines to compare to. We \\nbelieve STAMP is the only algorithm that does full, exact joins \\non time series subsequences. Most algorithms do only threshold \\njoins and/or do approximate joins, and are not specialized for \\ntime series subsequences. However, we searched for the best \\nbaselines and made the following concessions to  the rival \\nalgorithms to allow comparisons. \\n\\uf0b7 TSFRDAA [11]: While STAMP returns all nearest \\nneighbors, we adjust the threshold (the selectively) of \\nTSFRDAA such that only needs to return the top 1% nearest \\nneighbors, a much easier task. \\n\\uf0b7 HDSJI-SAX [12]: This method  allows false negatives ; we \\nignore this. It needs time to build indexes ; we do not count \\nthis time, and as above, we allow it to return  the only the \\ntop 1% nearest neighbors ( as bef ore, not the 100% the \\nSTAMP returns, and therefore a much easier task.). \\n\\uf0b7 Optimized Nested Loop (ONL): Here we use a nested -loop \\njoin optimized in the following manner. We use a state -of-\\nthe-art DFT indexing technique to do the search in the \\ninner loop, and we do not count the time needed to build \\nthe indexes [8]. We carefully adjust  parameters for best \\nperformance.  \\nWe consider the first 2 18 data points of the ECG dataset \\nused in [22], with a query length of 256, about one heartbeat ; \\nTABLE VII shows the results.  \\nTABLE VII. TIME FOR A SELF-JOIN WITH M = 256, VARYING ALGORITHMS \\nAlgorithm TSFRDAA HDSJI-SAX ONL STAMP \\nTime Required 51.7 hours 19.6 min 28.1 hours 1.17 hours \\nAs these results show, even if we ignore the limitations of \\nthe baseline methods, STAMP is still significantly faster. \\nB. Profile-Based Self-Join \\nA recent paper notes that many fundamental problems in \\nseismology can be solved by joining seismometer telemetry \\n[28], including the disc overy of foreshocks, aftershocks, \\ntriggered earthquakes, volcanic activity and induced seismicity. \\nHowever, the paper notes a join with a query length of 200 on a \\ndata stream of length 604,781 requires 9.5 days. Their solution, \\na clever transformation of t he data to allow LSH based \\ntechniques, does achieve significant speedup, but at the cost of \\nfalse negatives and the need for significant parameter tuning.  \\nThe authors kindly shared their data, and, as we hint at in \\nFigure 6, confirmed that STAMP does not have false negatives. \\n \\nFigure 6. top) An excerpt of a seismic time series aligned with its matrix \\nprofile (bottom). The ground truth provided by the authors of [28] requires that \\nthe events occurring at time 4,050 and 7,800 match. \\nWe repeated the n = 604,781, m = 200 experime nt and \\nfound it took just 8.9 hours to finish. As impressive as this is, in \\nthe next section we show that we can do even better.   \\n4,000 5,000 6,000 7,000 8,000 9,000\\n0\\n5\\n10\\n15\\nSeismic time series (excerpt) \\nMatrix Profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='C. The Utility of Anytime STAMP \\nThe seismology dataset offers an excellent opportunity to \\ndemonstrate the utility of the anytime version of our algorithm. \\nThe authors of [28] revealed in their long -term ambition of \\nmining even larger datasets [3]. In Figure 7 we repeated the \\nexperiment with the snippet shown in Figure 6, this time \\nreporting the best-so-far matrix profile reported by the \\nalgorithm at various milestones. Even with just 0.25% of the \\ndistances computed (that is to say, 400 times faster) the correct \\nanswer has emerged. Thus, we can provide the correct answers \\nto the seismologists in just minutes, rather than the 9.5 days.  \\n \\nFigure 7. top) An excerpt of the seismic data that is also shown in Figure 6. \\ntop-to-bottom) The approximations of the matrix profile for increasing \\ninterrupt times. By the time we have computed just 0.25% of the calculations \\nrequired, the minimum of the matrix profile points to the ground truth. \\nTo show the generality of this anytime feature of STAMP, \\nwe consider a very different dataset. As shown in Figure \\n8.inset), it is possible to convert DNA to a time ser ies [22]. We \\nconverted the Y-chromosome of the Chimpanzee this way. The \\nresulting time series is  little over one -million in length. We \\nperformed a self-join with m = 60,000.  Figure 8.bottom shows \\nthe best motif is so well conserved (ignoring the first 20%), that \\nit must correspond to a recent (in evolutionary tim e) gene \\nduplication event. In fact, in a subsequent analysis we \\ndiscovered that ‚Äúmuch of the Y (Chimp chromosome) consists of \\nlengthy, highly similar repeat units, or ‚Äòamplicons‚Äô‚Äù [9]. \\n \\nFigure 8. top) The Y -chromosome of the Chimp in time series space with its \\nmatrix profile. bottom) A zoom -in of the top motif discovered using anytime \\nSTAMP, we believe it to be an amplicon [9]. \\nThis demanding join would take just over a day of CPU \\ntime (see TABLE V). However, using anytime STAMP we \\nhave the result shown above after doing just 0.021% of the \\ncomputations, in about 18 seconds. At [24] we have videos that \\nshow the rapid convergence of the anytime variant of STAMP. \\nD. Profile-Based Similarity Join Set \\nIn this section we show two uses of similarity join set. The \\nfirst use is more familiar to APSS users, quantifying what is \\nsimilar between two time series. The second example, \\nquantifying what is different between two time series, is novel  \\nand can only be supported by threshold -free algorithms that \\nreport the nearest neighbor for all objects. \\nTime Series Set Similarity \\nGiven two (or more) time series collected under different \\nconditions or treatments, a data analyst may wish to know what \\npatterns (if any) are conserved between the two time series. To \\ndemonstrate the utility of automating this, we consider a simple \\nbut in tuitive example. Figure 9 shows the raw audio of two \\npopular songs converted to Mel Frequency Cepstral \\nCoefficients (MFCCs). Specifically, the songs used in this \\nexample are ‚ÄúUnder Pressure‚Äù by Queen an d David Bowie and \\n‚ÄúIce Ice Baby ‚Äù by the American rapper Vanilla Ice. Normally, \\nthere are 13 MFCCs; here, we consider just one for simplicity. \\n \\n \\nFigure 9. Two songs represented by just the 2 nd MFCC at 100Hz. We \\nrecognize that it is difficult to see any structure in these times series; however, \\nthis difficulty is the motivation for this experiment. \\nEven for these two relatively short time series, visual \\ninspection does not offer immediate answers. The problem here \\nis compounded by the size reproduction, but is not significantly \\neasier with large-scale [24] or interactive graphic tools. We ran \\nJAB (Queen-Bowie, Vanilla Ice) on these datasets with m = 500 \\n(five seconds), the best match, corresponding the minimum \\nvalue of the matrix profile PAB, is shown in Figure 10. \\n \\nFigure 10. The result of JAB (Queen-Bowie (red/bold), Vanilla-Ice (green/fine)) \\nproduces a strongly conserved five second region. \\nReaders may know the cause for this highly conserved \\nsubsequence. It corresponds to the famous baseline of ‚ÄúUnder \\nPressure,‚Äù which was sampled (plagiarized) by Vanilla Ice in \\nhis song. The join took 21.9 seconds. The ability to find \\nconserved structure in apparently disparate time series could \\nopen many avenues of research in medicine and industry. \\nTime Series Difference \\nWe introduce the Time Series Diff (TSD) operator, which \\ninformally asks ‚ÄúWhat happens in time series T A, that does not \\nhappen in time series TB?‚Äù Here TA/TB could be an ECG before \\na drug is administered/after a drug is administered, telemetry \\nbefore a successful launch/before a catastrophic launch etc. The \\nTSD is simply the subsequence referred to by the maximum \\nvalue of the JAB join‚Äôs profile PAB. \\nWe begin with a simple intuitive example. The UK and US \\nversions of the Harry Potter audiobook series are performed by \\ndifferent narrators, and have a handful of differences in the text. \\nFor example, the UK version contains: \\nHarry was passionate about Quidditch. He had played as Seeker on the Gryffindor \\nhouse Quidditch team ever since his first year at Hogwarts and owned a Firebolt, one \\nof the best racing brooms in the world... \\nBut the corresponding USA version has: \\nHarry had been on the Gryffindor House Quidditch team ever since his first year at \\nHogwarts and owned one of the best racing brooms in the world, a Firebolt. \\nAs shown in Figure 11, we can convert the audio \\ncorresponding to these snippets into MFCCs and invoke a JAB \\njoin set to produce a matrix profile PAB that represents the \\ndifferences between them. As Figure 11.left shows, the low \\nvalues of this profile correspond to identical spoken phrases \\n(despite having two different narrators). However here we are \\n4,000 5,000 6,000 7,000 8,000 9,000\\ndata\\n0.25%\\n1%\\n100%\\n0 1,000,0000\\n100\\n200\\nPan troglodytes Y-chromosome \\n0 60,000\\n12,749,475 to 14,249,474 bp\\n622,725 to 2,122,724 bp\\nT1 = 0;\\nfor i = 1 to length(chromosome) \\nif chromosomei = A, then Ti+1 = Ti + 2 \\nif chromosomei = G, then Ti+1 = Ti + 1\\nif chromosomei = C, then Ti+1 = Ti - 1 \\nif chromosomei = T, then Ti+1 = Ti - 2 \\nend\\n0\\nQueen-Bowie\\n1,000 2,000\\n-10\\n0\\n10\\nVanilla Ice\\n0 250 500-3\\n-2\\n-1\\n0\\n1\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='interested in the differences, the maximum value of the profile. \\nAs we can see in Figure 11.right, here the profile corresponds \\nto a phrase unique to the USA edition.  \\n \\nFigure 11. The 2 nd MFCC of snippets from the USA ( pink/bold) and UK \\n(green/fine) Harry Potter audiobooks.  The JAB join of the two longer sections \\nin the main text produce s mostly small values in the profile correspond to the \\nsame phrase (left), the largest value in  the profile corresponds to a phrase \\nunique to the USA edition (right). \\nThe time required to do this is just 0.067 seconds, much \\nfaster than real time. While this demonstration is trivial, in [24] \\nwe show an example applied to ECG telemetry.  \\nE. Profile-Based Motif Discovery \\nSince their introduction in 2003, time series motifs have \\nbecome one of the most frequently used primitives in time  \\nseries data mining, with applications in dozens of domains [2]. \\nThere are several proposed definitions for time series motifs, \\nbut in [18] it is argued that if you can solve the most basic \\nvariant, the closest (non -trivial) pair of subsequences, then all \\nother variants only require some minor additional calculations. \\nNote that the locations of the two (tying) minimum values of \\nthe matrix profile are exactly the locations of the 1st motif pair. \\nThe fastest known exact algorithm for computing time \\nseries motifs is the MK algorithm [18]. Note, however, that this \\nalgorithm‚Äôs time performance depends on the time series itself. \\nIn contrast, the Profile -Based Motif Discovery (PBMD) takes \\ntime independent of the data. To see this, we compared the two \\napproaches on an electrocardiogram of length 65,536. In Figure \\n12.left we ask what happens as we search for lon ger and longer \\nmotifs. In Figure 12.right we ask what happens if the motif \\nlength is fixed to m = 512, but the data becomes increasing \\nnoisy.  \\n \\nFigure 12. The time required to find the top -motif pairs in a time series of \\nlength 216 for increasingly long motif lengths ( left), and for a length fixed to \\n512, but in the face of increasing noise levels (right). \\nThese results show that  even in the best case for MK, \\nPBMD is competitive, but as we have longer queries and/or \\nnoisier data, its advantage becomes unassailable. Moreover, \\nPBMD inherits STAMP‚Äôs anytime and incremental \\ncomputability, and is easily parallelizable.  \\nF. Profile-Based Discord Discovery \\nA time series discord  is the subsequence that has the \\nmaximum distance to its nearest neighbor. While this is a \\nsimple definition, time series discords are known to be very \\ncompetitive as novelty/anomaly detectors  [5]. Note that as \\nshown in Figure 13, the time series discord is encoded as the \\nmaximum value in a matrix profile. \\n \\nFigure 13. top) An excerpt from an ECG incorporating a premature ventricular \\ncontraction (red/bold). bottom) The time series profile peaks exactly at the \\nbeginning of the PVC. \\nThe time taken to compute the discord  is obviously just the \\ntime needed to compute the matrix profile ( here, 0.9 seconds). \\nThere are a few dozen discord discovery algorithms in the \\nliterature. Some of them may be competitive in the best case, \\nbut just like motif -discovery algorithms they all degenerate to \\nbrute force search in the worst case, and none allow the anytime \\nproperties that we inherit from using STAMP.  \\nG. Incrementally Maintaining Motifs and Discords \\nWe have demonstrated the ability to detect time series \\nmotifs and discords using the matrix profile in the previous two \\nsections. However, we assumed that the entire time series was \\navailable beforehand. Here we remove this assumption and \\nshow how STAMP I allows us to incrementa lly maintain time \\nseries motifs/ discords in an online fashion. There are other \\nattempts at one [20][2] or both [25] of these tasks, but they are \\nall approximate and allow false dismissals.   \\nIn Section III.E, we introduced the STAMPI algorithm. The \\nability to incrementally maintain the matrix profile implies the \\nability to exactly maintain the time series motif [18] and/or time \\nseries discord [5] in streaming data. We simply need to keep \\ntrack of the extreme values of the incrementally-growing matrix \\nprofile, report a new pair of motifs when a new minimum value \\nis detected, and report a new discord when we see a new \\nmaximum value. \\nWe demonstrate the utility of these ideas on the AMPds \\ndataset [13]. Here the kitchen fridge and the heat pump are both \\nplugged into a single metered power supply. For the first week, \\nonly the refrigerator is running. At the end of the week, the \\nweather gets cold and the heat pump is turned on. The sampling \\nrate is one sample/m inute, and th e subsequence length is 100. \\nWe apply the STAMP algorithm to the first three days of data, \\nthen invoke STAMP I to handle newly arriving data , report an \\nevent when we detect a new extreme value. \\nOur first event occurs at the 9,864 th minute (6 da y 20 hour \\n24 minute). As shown in Figure 14, a new minimum value is \\ndetected, which indicates a new time series motif. The just -\\narrived 100 -minute-long pattern looks v ery similar to another \\npattern that occurred five hours earlier. While there is a lot of \\nregularity in the fridge data in general, the exceptional \\nsimilarity observed here suggested some underlying physical \\nmechanism that caused such a perfectly -conserved pattern, \\nperhaps a mechanical ice-making ‚Äúsubroutine.‚Äù \\n \\nFigure 14. top) The matrix profile of the first 9,864 minutes of data. bottom) \\nThe minimum value of the matrix profile corresponds to a pair of time series \\nmotifs in the power usage data. right) The time series motif detected. \\nOur second event occurs at the 10,473 th minute (7 day 6 \\nhour 33 minute). As shown in Figure 15, a new maximum value \\n‚Ä¶indor house Quidditch team ever since his first ye‚Ä¶\\nHarry had been on the Gryffindor House Quidditch te..\\nsince his first year at Hogwarts and owned a Fire..\\nsince his first year at Hogwarts and owned on..\\nED = 2.9 \\nClosest Match \\n0 100(1.6 seconds) 0 100\\nED = 10.4\\n(1.6 seconds)\\nFurthest Match (Time Series Difference) \\n256 512 1,024 2,048 4,096\\n0\\n40\\n80\\n120\\n160\\n200\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nSubsequence Length  (m)\\nTime Taken (min)\\n80 40 0\\n0\\n70\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nNoise Added (dB)  \\n0 1000 2000 3000\\nECG qtdb\\nSel102\\n(excerpt)\\nPremature Ventricular \\nContraction\\nTime Series Profile\\n0 5,000 10,000\\n0 100\\nnew minimum value\\nnew motifMatrix Profile\\nPower Usage Data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='is detected, which indicates a new time series discord. The time \\nseries discord corresponds to the first occurrence of a heat \\npump pattern in the power usage data.  \\n \\nFigure 15. top) The matrix profile for the first 10,473 minutes. bottom) The \\nmaximum value of the matrix profile corresponds to a time series discord. \\nright) The time series discord detected is the first  heat pump pattern \\noccurrence in the dataset. \\nThe maximum time needed to process a single data point \\nwith STAMP I in this dataset is 0.005 second s, which is less \\nthan 0.01% of the data sampling rate. Thus, on this dataset we \\ncould continue monitoring with the STAMP I algorithm for \\nseveral decades before running out of time or memory. \\nH. Profile-Based Shapelet Discovery \\nShapelets are time series subsequences which are in some \\nsense the maximally representative of a class [23][27]. \\nShapelets can be used to classify time series (essentially, the \\nnearest shapelet algorithm), offering the benefits of speed, \\nintuitiveness and at least on some domains, significantly \\nimproved accuracy [23]. However, these advantages come at \\nthe cost of a very expensive training phase, with O( n2m4) time \\ncomplexity, where m is the length of the longest time series \\nobject in the dataset, and n is the number of objects in the \\ntraining set. In order to mitigate this high ti me complexity, \\nresearchers have proposed various distance pruning techniques \\nand candidate ranking approaches for both the admissible [27] \\nand appro ximate [23] shapelet discovery. Nevertheless, \\nscalability remains the bottleneck.  \\nBecause shapelets are essentially supervised motifs, and we \\nhave shown that STAMP can find motifs very quickly, it is \\nnatural to ask if STAMP has implications for shapelet \\ndiscovery. While space limitations prohibit a detailed \\nconsideration of this question, we briefly sketch out and test \\nthis possibility as follows. \\nAs shown in Figure 16, we can use matrix profile to \\nheuristically ‚Äúsuggest‚Äù candidate shapelets. We consider two \\ntime series TA (green/bold) and TB (pink/light) with class 1 and \\nclass 0 being their corresponding class label, and we take  JAB, \\nJAA, JBA and JBB. Our claim is the differences in the heights of \\nPAB, PAA (or PBA, PBB) are strong indicators of good candidate \\nshapelets. The intuition is that if a discriminative pattern is \\npresent in say, class 1, but not in class 0, then we expect to see a \\n‚Äúbump‚Äù in the PAB (the intuition holds if the order is reversed). \\nA significant difference (quantified by a threshold shown in \\ndashed line) between the heights of PAA and PAB curves \\ntherefore indicates the occurrence of good candidate shapelets, \\npatterns that only occur in one of the two classes. \\n \\nFigure 16. top.left) Two time series TA and TB formed by concatenating \\ninstances of class 1 and 0 respectively of ArrowHead [6]. bottom) The height \\ndifference between PAB (or PBA) and PAA (or PBB) are suggestive of  good \\nshapelets. top.right) An example of good shapelet extracted from class 1. \\nThe time taken to compute  all four matrix profiles is 1.0 \\nseconds and the time to further evaluate the two twelve \\ncandidates selected takes 2.7 seconds. On the same machine, \\nthe brute force shapelet classifier takes 4.2 minutes with 2,364 \\ncandidates. Note that, in this toy demonst ration, the speedup is \\n68X, however, for larger datasets, the speedup is greater [24]. \\nI. Profile-Based Semantic Segmentation \\nThe goal of time series semantic segmentation is to partition \\na dataset containing multiple activities/regimes into atomic \\nbehaviors or conditions. For example, for human activity data \\nthe regimes might later be mapped to { eating, working, \\ncommuting,...}[29]. As this example suggests, most work in this \\narea is highly domain -dependent. In contrast, here we show \\nhow the matrix profile index can be emp loyed for domain \\nagnostic time series segmentation.  \\nThe intuition of our approach is as follows. Within a single \\nregime we might expect that most subsequences will have a \\nnearest neighbor close by (in time). For example, consider the \\ntoy problem shown in Figure 17 which shows two obvious \\nregimes. We would expect that the nearest neighbor to the first \\nrun gait cycle is the second or third or fourth run cycle, but it \\nwill almost certainly not be one of the walk cycles. In general, \\nthis tendency for nearest neighbor pointers not to cross the \\nboundaries corresponding to regime changes may be sufficient \\nto discover these boundaries, and of course, this is precisely the \\ninformation that is encoded in the matrix profile index. \\nThus, for each point we count how many ‚Äúarcs‚Äù connecting \\ntwo nearest neighbors cross it if we connect each subsequence \\nto its nearest neighbor as shown in Figure 17. \\n \\nFigure 17. top) A (toy) time series ( red) and nearest neighbor locations for \\neach subsequence. bottom) The number of arcs crossing above each \\nsubsequence. \\nWe applied the procedure described above to a heavily \\nstudied activity segmentation problem [29], which was derived \\nfrom the CMU Motion Capture data base [16]. The recordings \\nare represented as multi -dimensional time series, and most \\nresearch efforts carefully select the best subset for the \\nsegmentation task. For example, [29] states that ‚Äú we only \\nconsider the 14 most informative joints out of 29 .‚Äù In contrast, \\nwe attempt this with a single dimension. The only parameter we \\nneed to set is sliding window size . In Figure 18 we show the \\nsegmenting results obtained using our approach , with  \\nannotations taken from [29] for context. \\nMuch of the evaluation in this community lacks formal \\nmetrics, preferring instead visual sanity tests like the one in \\nFigure 18. Given this, we can say that our approach is very \\ncompetitive on this dataset, in spite of the fact that we \\nhandicapped ourselves to only consider one dimension.  \\n0 100\\n0 10,0005,000\\nMatrix Profile\\nPower Usage Data\\nnew maximum value\\nnew discord\\n0\\n7\\n0 1,400\\nPABPAA\\n0 1,600\\nTA\\nTB\\n0 300\\nAn example of good shapelet\\n0 1,400\\nPBA\\nPBB\\n8\\n0\\n0 1,400\\nDiff(PAB ,PAA) Threshold\\n-1\\n4\\n-2\\n4\\n0 1,400\\nDiff(PBA ,PBB) Threshold\\n0\\n1\\n2\\n1 2 0\\nwalking slow    walking slow     run       run run run\\nNumber of arcs intersecting each point'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content=\"Figure 18. top) A matrix profile ( blue) obtained for the time series ( red) and \\nnumber of arcs crossing each point ( green). Low values of this green curve \\ncoorespond to candidate split points. bottom) Human annotations of the \\nactivities: w ‚Äì walk, s ‚Äì stretch, p ‚Äì punch, c ‚Äì chop, t ‚Äì turn, d ‚Äì drink. \\nV. CONCLUSION \\nWe have introduced a scalable algorithm for creating time \\nseries subsequences joins. Our algorithm is simple, fast, \\nparallelizable and parameter -free, and can be in crementally \\nupdated for moderately fast data arrival rates. We have shown \\nthat our algorithm has implications for many existing tasks, \\nsuch as motif discovery, discord discovery, shapelet discovery \\nand semantic segmentation, and may open up new avenues for  \\nresearch, including computing various definitions of time series \\nset difference. Our code is freely available for the community to \\nconfirm, extend and exploit our ideas. \\nREFERENCES \\n[1] R. J. Bayardo, Y. Ma and R. Srikant, ‚ÄúScaling up all pairs similarity \\nsearch,‚Äù WWW 2007, pp 131-140. \\n[2] N. Begum and E. Keogh, ‚ÄúRare time series motif discovery from \\nunbounded streams,‚Äù PVLDB 8(2): 149-160, 2014. \\n[3] G. Beroza, ‚ÄúPersonal Correspondence,‚Äù Jan 21th, 2016. \\n[4] T. Bouezmarni and J. Rombouts, ‚ÄúNonparametric density estimation for \\npositive time series,‚Äù CSDA, 54, 245-261, 2010.  \\n[5] V. Chandola, D. Cheboli and V. Kumar, ‚ÄúDetecting anomalies in a time \\nseries database,‚Äù UMN TR09-004. \\n[6] T. Chen  et al ., ‚ÄúThe UCR time series classification archive,‚Äù \\nhttp://www.cs.ucr.edu/~eamonn/time_series_data/. \\n[7] ‚ÄúConvolution - Wikipedia, the free encyclopedia,‚Äù  \\nhttps://en.wikipedia.org/wiki/Convolution, Accessed: 2016-01-19. \\n[8] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang and E. J. Keogh, \\n‚ÄúQuerying and mining of time series data: experimental comparison of \\nrepresentations and distance measures,‚Äù PVLDB 1(2): 1542-1552. 2008. \\n[9] J. Hughes et al. , ‚ÄúChimpanzee and human Y chromosomes are \\nremarkably divergent in structure‚Äù Nature 463, (2010). \\n[10] H. Lee, R. Ng and K. Shim, ‚ÄúSimilarity join size estimation using \\nLocality sensitive hashing,‚Äù PVLDB, 4(6):338‚Äì349, 2011. \\n[11] W. Luo, H. Tan, H. Mao  and L. M.  Ni, ‚ÄúEfficient similarity joins on \\nmassive high-dimensional datasets using mapreduce,‚Äù In MDM'12, IEEE \\nComputer Society, pp. 1-10. \\n[12] Y. Ma, X. Meng and S. Wang, ‚ÄúParallel similarity joins on massive high-\\ndimensional data using MapReduce ,‚Äù Concurrency and Computation , \\nVolume 28, Issue 1 Jan 2016. Pages 166‚Äì183. \\n[13] S. V. Makonin, ‚ÄúAMPds: a public dataset for load disaggregation and \\neco-feedback research,‚Äù EPEC 2013, pp 1-6. \\n[14] MASS: http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html \\n[15] G. D. F. Morales and A. Gionis, ‚Äústreaming similarity self-join,‚Äù Proc. \\nVLDB Endow, 2016. \\n[16] Motion Capture Database, http://mocap.cs.cmu.edu/ \\n[17] A. Mueen, H. Hamooni and T. Estrada, ‚ÄúTime series join on subsequence \\ncorrelation,‚Äù IEEE ICDM 2014, pp. 450-459. \\n[18] A. Mueen, E. Keogh, Q. Zhu, S. Cash and B. Westover, ‚ÄúExact discovery \\nof time series motif,‚Äù SDM 2009. \\n[19] D. Murray et al. , ‚ÄúA data management platform for personalised real-\\ntime energy feedback,‚Äù In EEDAL 2015. \\n[20] V. Niennattrakul et al, ‚ÄúData editing techniques to allow the application \\nof distance-based outlier detection to streams,‚Äù ICDM 2010: 947-952. \\n[21] D. Patnaik, et al, ‚ÄúSustainable operation and management of data center \\nchillers using temporal data mining,‚Äù KDD 2009. \\n[22] T. Rakthanmanon et al., ‚ÄúSearching and Mining Trillions of Time Series \\nSubsequences under Dynamic Time Warping,‚Äù In KDD 2012, 262-270. \\n[23] T. Rakthanmanon and E. Keogh, ‚ÄúFast shapelets: a scalable algorithm for \\ndiscovering time series shapelets,‚Äù SDM, 2013. \\n[24] Supporting page. http://www.cs.ucr.edu/~eamonn/MatrixProfile.html \\n[25] C. D. Truong and D. T.  Anh, ‚ÄúAn Efficient Method for Motif and \\nAnomaly Detection in Time Series‚Äù IJBIDM, Vol. 10, No. 4, 2015. \\n[26] A. Tucker and X. Liu, ‚ÄúA Bayesian  Network Approach to Explaining \\nTime Series with Changing Structure,‚Äù Intell Data Anal, 8 (5) (2004). \\n[27] L. Ye and E. Keogh, ‚ÄúTime series shapelets: a new primitive for data \\nmining,‚Äù ACM SIGKDD, 2009, pp 947-56. \\n[28] C. Yoon, O. O‚ÄôReilly, K. Bergen and G. Beroza, ‚ÄúEarthquake detection \\nthrough computationally efficient similarity search,‚Äù Sci. Adv. 2015. \\n[29] F. Zhou, F. Torre and J.  Hodgins ‚Äú Aligned Cluster Analysis for \\nTemporal Segmentation of Human Motion,‚Äù IEEE FG'2008. \\n[30] S. Zilberstein and S. Russell, ‚ÄúApproximate Reasoning Using Anytime \\nAlgorithms,‚Äù In Imprecise and Approximate Computation , Kluwer \\nAcademic Publishers, 1995. \\n \\n0 5,000 10,000\\nMatrix  Profile\\nSplit Points   \\nPrediction\\nOne-dimension of multi-d time series: Subject 86, recording 4, dimension 30\\nW S P N      W C T           D            P         W\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Matrix Profile I: All Pairs Similarity Joins for Time Series: \\nA Unifying View that Includes Motifs, Discords and Shapelets \\nChin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,  \\nHoang Anh Dau, ‚Ä†Diego Furtado Silva, ‚Ä°Abdullah Mueen, and Eamonn Keogh \\nUniversity of California, Riverside, ‚Ä†Universidade de S√£o Paulo, ‚Ä°University of New Mexico \\n{myeh003, yzhu015, lulan001, nbegu001, yding007, hdau001}@ucr.edu, diegofsilva@icmc.usp.br, mueen@unm.edu, eamonn@cs.ucr.edu \\n \\nAbstract‚Äî The all-pairs-similarity-search (or similarity join) \\nproblem has been extensively studied for text and a handful of \\nother datatypes. However, surprisingly little progress has been  \\nmade on similarity joins for time series subsequences. The lack of  \\nprogress probably stems from the daunting nature of the \\nproblem. For even modest sized data sets the obvious nested -loop \\nalgorithm can take months, and the typical speed -up techniques \\nin this domain (i.e., indexing, lower -bounding, triangular -\\ninequality pruning and early abandoning) at best produce one or \\ntwo orders of magnitude speedup. In this work we introduce a \\nnovel scalable algorithm for time series subsequence all -pairs-\\nsimilarity-search. For exceptionally large datasets, the algorithm \\ncan be trivially cast as an anytime algorithm and produce high -\\nquality approximate solutions in reasonable  time. The exact \\nsimilarity join algorithm computes the answer to the time series \\nmotif and time series discord  problem as a side -effect, and our \\nalgorithm incidentally provides the fastest known algorithm for \\nboth these  extensively-studied problems. We de monstrate the \\nutility of our ideas for many time series data mining problems, \\nincluding motif discovery, novelty discovery,  shapelet discovery,  \\nsemantic segmentation, density estimation, and contrast set \\nmining.    \\nKeywords‚ÄîTime Series; Similarity Joins; Motif Discovery \\nI. INTRODUCTION \\nThe all-pairs-similarity-search (also known as similarity \\njoin) problem comes in several variants. The basic task is this: \\nGiven a collection of data objects, retrieve the nearest neighbor \\nfor each object . In the text domain the  algorithm has \\napplications in a host of problems, including community \\ndiscovery, duplicate detection, collaborative filtering, \\nclustering, and query refinement [1]. While virtually all text \\nprocessing algorithms have analogues in time series data \\nmining, there has been surprisingly little progress on Time \\nSeries subsequences All-Pairs-Similarity-Search (TSAPSS). \\nWe believe that this lack of progress stems not from a lack \\nof interest in this useful primitive, but from the daunting nature \\nof the problem. Consider the following example that reflects the \\nneeds of an industrial collaborator. A boiler at a che mical \\nrefinery reports pressure once a minute. After a year, we have a \\ntime series of length 525,600. A plant manager may wish to do \\na similarity self-join on this data with week -long subsequences \\n(10,080) to discover operating regimes (summer vs. winter o r \\nlight distillate vs. heavy distillate etc.) The obvious nested loop \\nalgorithm requires 132,880,692,960 Euclidean distance \\ncomputations. If we assume each one takes 0.0001 seconds, \\nthen the join will take 153.8 days. The core contribution of this \\nwork is to show that we can reduce this time to 6.3 hours, using \\nan off-the-shelf desktop computer. Moreover, we show that this \\njoin can be computed and/or updated incrementally. Thus we \\ncould maintain this join essentially forever on a standard \\ndesktop, even if the data arrival frequency was much faster than \\nonce a minute. \\nOur algorithm uses an ultra -fast similarity search algorithm \\nunder z -normalized Euclidean distance as a subroutine, \\nexploiting the overlap between subsequences using the classic \\nFast Fourier Transform (FFT) algorithm.  \\nOur method has the following advantages/features: \\n\\uf0b7 It is exact, providing no false positives or false dismissals. \\n\\uf0b7 It is simple and parameter -free. In contrast, the more \\ngeneral metric space APSS algorithms require building and \\ntuning spatial access methods and/or hash functions.  \\n\\uf0b7 Our algorithm requires an inconsequential space overhead, \\njust O(n) with a small constant factor. \\n\\uf0b7 While our exact algorithm is extremely scalable, for \\nextremely large datasets we can compute the results in an \\nanytime fashion, allowing ultra-fast approximate solutions.  \\n\\uf0b7 Having computed the similarity join for a dataset, we can \\nincrementally upd ate it very efficiently. In many domains \\nthis means we can effectively maintain exact joins on \\nstreaming data forever.  \\n\\uf0b7 Our method provides full joins, eliminating the need to \\nspecify a similarity threshold, which as we will show, is a \\nnear impossible task in this domain.  \\n\\uf0b7 Our algorithm is embarrassingly parallelizable, both on \\nmulticore processors and in distributed systems. \\nGiven all these features, our algorithm has implications for \\nmany time series data mining tasks [5][18][28].  \\nThe rest of the paper is organized as follows. Section II \\nreviews related work an d introduces the necessary background \\nmaterials and definitions. In Section III we introduce our \\nalgorithm and its anytime and incremental variants. Section IV \\nsees a detailed empirical evaluation of our algorithm and shows \\nits implications for many data mining tasks. Finally, in Section \\nV we offer conclusions and directions for future work. \\nII. RELATED WORK AND BACKGROUND \\nThe basic variant of  similarity join problem we are \\ninterested in is as follows : Given a collection of data objects, \\nretrieve the nearest neighbor for every object.   \\nOther common variants include retrieving the top-K nearest \\nneighbors or the nearest neighbor for each object if that \\nneighbor is within a user-supplied threshold, œÑ. (Such variations \\nare trivial generalizations of our proposed algorithm, so we \\nomit them from further discussion). The latter variant results in \\na much easier problem, provided that the threshold is small. For \\nexample, [1] notes that virtually all research efforts ‚Äú exploit a \\nsimilarity threshold more aggressively in order to limit the set'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='of candidate pairs that are considered..  [or] ... to reduce the \\namount of information indexed in the first place.‚Äù \\nThis critical dependence on œÑ is a major issue for text joins, \\nas it is known that ‚Äú join size can change dramatically \\ndepending on the input similarity threshold ‚Äù [10]. However, \\nthis issue is even more critical for time series for two reasons. \\nFirst, unlike similarity (which is bounded between zero and \\none), the Euclidean distance is effectively unbounded, and \\ngenerally not intuitive. For example, if two heartbeats have a \\nEuclidean distance of 17.1, are they similar? Even for a domain \\nexpert that knows the sampling rate and the noise level of the \\ndata, this is not obvious. Second, a single threshold can produce \\nradically different output sizes, even for datasets that are very \\nsimilar.  Consider Figure 1 which shows the output size vs. \\nthreshold setting for the first and s econd halves of a ten -day \\nperiod monitoring data center chillers [21]. For the first five \\ndays a threshold of 0.6 would return zero items, but for the \\nsecond five days the same setting would return 108 items.  This \\nshows the difficulty in selecting  an appropriate threshold. Our \\nsolution is to have no threshold, and do a full join. \\n \\nFigure 1.  Output size vs. threshold for data center chillers [21]. Values beyond \\n2.0 are truncated for clarity (but archived at [24]).  \\nA handful of efforts have considered joins on time series, \\nachieving speedup by (in addition to the use of MapReduce) \\nconverting the data to lower -dimensional representations such \\nas PAA [11] or SAX [12] and exploiting lower bounds and/o r \\nLocality Sensitive Hashing (LSH) to prune some calculations. \\nHowever, the methods are very complex, with many (10 -plus) \\nparameters to adjust. As [11] acknowledges with admirable \\ncandor, ‚ÄúReasoning about the optimal settings is not trivial.‚Äù In \\ncontrast, our proposed algorithm has zero parameters to set. \\nA very recent research effort [28] has tackled the scalability \\nissue by converting the real -valued time series into discrete \\n‚Äúfingerprints‚Äù before using a LSH approach, much like the text \\nretrieval community [1]. They produced impressive speedup, \\nbut they also experienced false negatives. Moreover, the \\napproach has several parameters that need to be set; for \\nexample, they need to set the threshold to a very precise 0.818.  \\nAs we shall show, our algorithm allows both anytime and \\nincremental (i.e. streaming) versions. While a streaming join \\nalgorithm for text was recently introduced [15], we are not \\naware of any such algorithms for time series data or general \\nmetric spaces. More generally, there is a large amount of \\nliterature on joins for text processing [1]. Such work is \\ninteresting, but of little utility given our constraints, data type \\nand problem setting. We require full joins, not threshold joins, \\nand we are unwilling to allow the possibility of false negatives. \\nA. Definitions and Notation \\nWe begin by defining the data type of interest, time series: \\nDefinition 1:  A time series T is a sequence of real -valued \\nnumbers ti: T = t1, t2, ..., tn where n is the length of T. \\nWe are not interested in the global properties of time series, \\nbut in the similarity between local subsequences:  \\nDefinition 2:  A subsequence Ti,m of a T is a continuous \\nsubset of the values from T of length m starting from \\nposition i.   Ti,m = ti, ti+1,‚Ä¶, ti+m-1, where 1 ‚â§  i ‚â§  n-m+1.  \\nWe can take any subsequence from a time series and \\ncompute its distance to all sequences. We call an ordered vector \\nof such distances a distance profile: \\nDefinition 3:  A distance profile D is a vector of the \\nEuclidean distances between a given query and each \\nsubsequence in an all-subsequences set (see Definition 4).  \\nNote that we are assuming that the distance is measured \\nusing the Euclidean distance between the z -normalized \\nsubsequences [8]. The distance profile can be considered a meta \\ntime series that annotates the time series T that was used to \\ngenerate it. The first three definitions are illustrated in Figure 2. \\n \\nFigure 2. A subsequence Q extracted from a time series T is used as a query to \\nevery subsequence in T. The vector of all distances is a distance profile. \\nNote that if the query and all-subsequences set belong to the \\nsame time series, the distance profile must be zero at the \\nlocation of the query, and close to zero just before and just \\nafter. Such matches are c alled trivial matches in the literature \\n[18], and are avoided by ignoring an exclusion zone (shown as \\na gray region) of m/2 before and after the location of the query. \\nWe are interested in similarity join of all subsequences of a \\ngiven time series. We define an all-subsequences set of a given \\ntime series as a set that contains all possible subsequences from \\nthe time series. The notion of all-subsequences set is purely for \\nnotational purposes. In our implementation, we do not actually \\nextract the subsequences in this form as it would require \\nsignificant time and space overhead. \\nDefinition 4: An all-subsequences set A of a time series T \\nis an ordered set of all possible subsequences of T obtained \\nby sliding a window of length m across T: A ={T1,m,, \\nT2,m,‚Ä¶, Tn-m+1,m}, where m is a user -defined subsequence \\nlength. We use A[i] to denote Ti,m. \\nWe are interested in the nearest neighbor (i.e., 1NN) relation \\nbetween subsequences; therefore, we define a 1NN-join \\nfunction which indicates the nearest neighbor relation between \\nthe two input subsequences. \\nDefinition 5:  1NN-join function :  given two all -\\nsubsequences sets A and B and two subsequences A[i] and \\nB[j], a 1NN-join function  Œ∏1nn (A[i], B[j]) is a Boolean \\nfunction which returns ‚Äútrue‚Äù only if B[j] is the nearest \\nneighbor of A[i] in the set B. \\nWith the defined join function, a similarity join set  can be \\ngenerated by applying the similarity join operator on two input \\nall-subsequences sets. \\nDefinition 6: Similarity join set : given all -subsequences \\nsets A and B, a similarity join set  JAB of A and B is a set \\ncontaining pairs of each subsequence in A with its nearest \\nneighbor in B: JAB={‚å© A[i], B[j] ‚å™ |Œ∏1nn (A[i], B[j])}.  We \\ndenote this formally as JAB = A‚ãà\\uf0711nnB. \\n0\\n400\\n800\\n1200\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\\nFirst 5 Days\\nSecond 5 Days\\nData Center Chillers\\nT, a snippet of an energy \\nconsumption\\n2,0000 m/2m/2\\nQ, query of length m\\nNote that |D| = |T|-|Q|+1D, a distance profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='We measure the Euclidean distance between each pair \\nwithin a similarity join set and store the resultants into an \\nordered vector. We call the result vector the matrix profile. \\nDefinition 7:  A matrix profile (or just profile) PAB is a \\nvector of the Euclidean distances between each pair in JAB. \\nWe call this vector the matrix profile because one \\n(inefficient) way to compute it would be to compute the full \\ndistance matrix of all the subsequences in one time series with \\nall the subsequence in another time series and extract the \\nsmallest value in each row (the smallest non-diagonal value for \\nthe self-join case).  In Figure 3 we show the matrix profile of \\nour running example. \\n \\nFigure 3. A time series T, and its self-join matrix profile P.  \\nLike the distance profile, the matrix profile can be \\nconsidered a meta time series annotating the time series T if the \\nmatrix profile is generated by joining T with itself. The profile \\nhas a host of interesting and exploitable properties. For \\nexample, the highest point on the profile corresponds to the \\ntime series discord [5], th e ( tied) lowest points correspond to \\nthe locations of the best time series motif pair [18], and the \\nvariance can be seen as a measure of the T‚Äôs com plexity. \\nMoreover, the histogram of the values in the matrix profile is \\nthe exact answer to the time series density estimation [4]. \\nWe name this special case of the similarity join set \\n(Definition 6) as self-similarity join set, and the corresponding \\nprofile as self-similarity join profile. \\nDefinition 8:  A self-similarity join  set JAA is a result of \\nsimilarity join of the set  A with itself . We denote this \\nformally as JAA = A ‚ãà\\uf0711nn A. We denote the corresponding \\nmatrix profile or self-similarity join profile as PAA. \\nNote that we exclude trivial matches when self -similarity \\njoin is performed, i.e., if A[i] and A[j] are subsequences from \\nthe same all -subsequences set A, Œ∏1nn (A[i], B[j]) is ‚Äúfalse‚Äù \\nwhen A[i] and A[j] are a trivially matched pair. \\nThe ith element in the matrix profile tells us the Euclidean \\ndistance to the nearest neighbor of the subsequence of T, \\nstarting at i.  However, it does not tell us where that neighbor is \\nlocated. This information is recorded in matrix profile index. \\nDefinition 9: A matrix profile index IAB of a similarity join \\nset JAB is a vector of integers where IAB[i] = j if {A[i], B[j]} \\n‚àà JAB. \\nBy storing the neighboring information this way, we can \\nefficiently retrieve the nearest neighbor of A[i] by accessing the \\nith element in the matrix profile index. \\nNote that the function which computes the similarity join set \\nof two input time series  is not symmetric; therefore, JAB ‚â† JBA,  \\nPAB ‚â† PBA, and IAB ‚â† IBA. \\nFor ease of presentation, we have confined this work to the \\nsingle dimensional case; however, nothing intrinsically \\nprecludes generalizations to multidimensional data. \\nSummary of the Previous Section \\nThe previous section was rather dense, so before moving on \\nwe summarize the main takeaway points. We can create two \\nmeta time series, the matrix profile and the matrix profile index, \\nto annotate a time series TA with the distance and location of all \\nits subsequences nearest neighbors in itself or another time \\nseries TB. These two data objects explicitly or implicitly contain \\nthe answers to many time series data mining tasks. However, \\nthey appear to be too expensive to compute to be practica l. In \\nthe next section we will show an algorithm that can compute \\nthese efficiently. \\nIII. ALGORITHMS \\nWe are finally in a position to explain our algorithm s. We \\nbegin by stating the fundamental intuition, which stems from \\nthe relationship between distance profiles and the matrix \\nprofile. As Figure 2 and Figure 3 visually suggest, all distance \\nprofiles (excluding the trivial match region) are upper bound \\napproximations to the matrix profile. More critically, if we \\ncompute all the distance profiles, an d take the minimum value \\nat each location, the result is the matrix profile! \\nThis tells us that if we have a fast way to compute the \\ndistance profiles, then we also have a fast way to compute the \\nmatrix profile. As we shall show in the next section, we have an \\nultra-fast way to compute the distance profiles. \\nA. Mueen‚Äôs ultra-fast Algorithm for Similarity Search (MASS) \\nWe begin by introducing a novel Euclidean distance \\nsimilarity search algorithm for time series data. The algorithm \\ndoes not just find the nearest neighbor to a query and return its \\ndistance; it returns the distance to every subsequence. In \\nparticular, it computes the distance profile, as shown in Figure \\n2. The algorithm requires just O( nlogn) time by exploiting the \\nFFT to calculate  the dot products between the query and all \\nsubsequences of the time series.  \\nWe need to carefully qualify the claim of ‚Äúultra-fast‚Äù. There \\nare dozens of algorithms for time series similarity search that \\nutilize index structures to efficiently locate neighbors [8]. While \\nsuch algorithms can be faster in the  best case , all these \\nalgorithms degenerate to brute force search in the worst case 1 \\n(actually, much worse than brute force search due to the \\noverhead of the index). Likewise, there are index -free methods \\nthat achieve speed -up using various early abandoning tricks \\n[22], but they too degrade to brute force search in the worst \\ncase. In contrast, the performance of the algorithms outlined in \\nTABLE I and TABLE II is completely independent of the data. \\nTABLE I. CALCULATION OF SLIDING DOT PRODUCTS \\nProcedure SlidingDotProduct(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: The dot product between Q and all subsequences in T \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nn ‚Üê Length(T), m ‚Üê Length(Q) \\nTa ‚Üê Append T with n zeros   \\nQr ‚Üê Reverse(Q)  \\nQra ‚Üê Append Qr with 2n-m zeros \\nQraf ‚Üê FFT(Qra), Taf ‚Üê FFT(Ta) \\nQT ‚Üê InverseFFT(ElementwiseMultiplication(Qraf, Taf)) \\nreturn QT \\n \\n1 There are many such worse case scenarios, including high levels of noise blurring \\nthe distinction between closest and furthest neighbors, and rendering triangular -\\ninequality pruning and early abandoning worthless. \\n2,0000\\nP, a matrix profile\\nT, a snippet of an energy \\nconsumption\\nNote that |P| = |T|-|Q|+1'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Line 1 determines the length of both the time series T and \\nthe query Q. In line 2, we use that information to append T with \\nan equal number of zeros. In line 3, we obtain the mirror image \\nof the original query. This reversing ensures that a convolution \\n(i.e. ‚Äúcrisscrossed‚Äù multiplication) essentially produces in-order \\nalignment. Because we require both vectors to be the s ame \\nlength, in line 4 we append enough zeros to the (now reversed) \\nquery so that, like Ta, it is also of length 2 n. In line 5, the \\nalgorithm calculates Fourier transforms of the appended -\\nreversed query (Qra) and the appended time series Ta. Note that \\nwe use FFT algorithm which is an O(nlogn) algorithm. The Qraf \\nand the Taf produced in line 5 are vectors of complex numbers \\nrepresenting frequency components of the two time series. The \\nalgorithm calculates the element-wise multiplication of the two \\ncomplex vectors and performs inverse FFT on the product. \\nLines 5-6 are the class ic convolution operation on two vectors \\n[7]. Figure 4 shows a toy example of the sliding dot product \\nfunction in work. The algorithm time complexity does not \\ndepend on the length of the query (m). \\n \\nFigure 4. A toy example of convolution operation being used to calculate  \\nsliding dot products for time series data. Note the reverse and append \\noperation on T and q in the input. Fifteen dot products are calculated for every \\nslide. The cells m = 2 to n = 4 from left ( red/bold arrows) contain valid \\nproducts. TABLE II takes this subroutine and uses it to create a distance \\nprofile (see Definition 3). \\nIn line 1 of TABLE II, we invoke the dot products code \\noutlined in TABLE I. The formula to calculate the z-normalized \\nEuclidean distance D[i] between two time series subsequence Q \\nand Ti,m using their dot product, QT[i] is (see [24] for \\nderivation): \\nùê∑[ùëñ] = ‚àö2ùëö(1‚àíùëÑùëá[ùëñ]‚àíùëöùúáùëÑùëÄùëá[ùëñ]\\nùëöùúéùëÑùõ¥ùëá[ùëñ] ) \\nwhere m is the subsequence length, ŒºQ is the mean of Q, \\nMT[i] is the mean of Ti,m, œÉQ is the standard deviation of Q, and  \\nŒ£T[i] is the standard deviation of Ti,m. Normally, it takes O( m) \\ntime to calculate the mean and standard deviation for every \\nsubsequence of a long time series. However, here we exploit a \\ntechnique noted in [22] in a different context. We cache \\ncumulative sums of the values and square of the values in the \\ntime series. At any stage the two cumulative sum vectors are \\nsufficient to calculate the mean an d the standard deviation of \\nany subsequence of any length.  See [14] for an elaborate \\ndescription and variations of MASS. \\nTABLE II. MUEEN‚ÄôS ALGORITHM FOR SIMILARITY SEARCH (MASS) \\nProcedure MASS(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: A distance profile D of the query Q  \\n1 \\n2 \\n3 \\n4 \\nQT ‚Üê SlidingDotProducts(Q, T) \\nŒºQ, œÉQ, ŒúT, Œ£T  ‚Üê ComputeMeanStd(Q, T)    // see [22] \\nD ‚Üê CalculateDistanceProfile(Q, T, QT, ŒºQ, œÉQ, ŒúT, Œ£T) \\nreturn D \\nUnlike the dozens of time series KNN search algorithms in \\nthe literature [8], this algorithm calculates the distance to every \\nsubsequence, i.e. the distance profile  of time series T. \\nAlternatively, in join nomenclature, the algorithm produces one \\nfull row of the all -pair similarity matrix. Thus, as we show in \\nthe next section, our join algorithm is simply a loop that \\ncomputes each full row of the all -pair similarity matrix and \\nupdates the current ‚Äúbest-so-far‚Äù matrix profile when needed. \\nB. The STAMP Algorithm \\nWe call our join algorithm STAMP, Scalable Time series \\nAnytime Matrix Profile. The algorithm is outlined in TABLE \\nIII. In line 1, we extract the length of TB. In line 2, we allocate \\nmemory and initial matrix profile PAB and matrix profile index \\nIAB. From lines 3 to line 6, we calculate the distance profiles D \\nusing each subsequence B[idx] in the time series TB and the \\ntime series TA. Then, we perform pairwise minimum for each \\nelement in D with the paired element in PAB (i.e., min( D[i], \\nPAB[i]) for i = 0 to length(D) - 1.) We also update IAB[i] with idx \\nwhen D[i] ‚â§ PAB[i] as we perform the  pairwise minimum \\noperation. Finally, we return the result PAB and IAB in line 7.  \\nNote that the algorithm presented in TABLE III computes \\nthe matrix profile for the general similarity join. To modify the \\ncurrent algorithm to compute  the self -similarity join matrix \\nprofile of a time series TA, we simply replace TB in line 1 with \\nTA, replace B with A in line 4, and ignore trivial match in D \\nwhen performing ElementWiseMin in line 5. \\nTABLE III. THE STAMP ALGORITHM \\nProcedure STAMP(TA, TB, m) \\nInput: Two user provided time series, TA and TB, interested \\nsubsequence length m \\nOutput: A matrix profile PAB and associated matrix profile index IAB of \\nTA join TB, JAB = A‚ãà\\uf0711nnB \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nnB ‚Üê Length(TB) \\nPAB ‚Üê infs, IAB ‚Üê zeros, idxes ‚Üê 1:nB-m+1 \\nfor each idx in idxes    // In any order, but random for anytime algorithm \\n          D ‚Üê MASS(B[idx], TA) \\n          PAB, IAB ‚Üê ElementWiseMin(PAB, IAB, D, idx) \\nend for \\nreturn PAB, IAB \\nTo parallelize the STAMP algorithm for multicore \\nmachines, we simply distribute the indexes to secondary \\nprocess run in each core, and the secondary processes use the \\nindexes they received to update their own PAB and IAB. Once the \\nmain process returns from  all secondary processes, we use \\nElementWiseMin to merge the received PAB and IAB.  \\nC. An Anytime Algorithm for TSAPSS \\nWhile the exact algorithm introduced in the previous section \\nis extremely scalable, there will always be datasets for which \\ntime needed for an exact solution is untenable. We can mitigate \\nthis by computing the results in an anytime fashion, allowing \\nfast approximate solutions [30]. To a dd the anytime nature to \\nthe STAMP algorithm, we simply ensure a randomized search \\norder in line 2 of TABLE III.  \\nWe can compute a (post-hoc) measurement of the quality of \\nan anytime solution by measuring the Root -Mean-Squared-\\nError (RMSE) between the true matrix profile and the current \\nbest-so-far mat rix profile. As Figure 5 suggests, with an \\nexperiment on random walk data, the algorithm converges very \\nquickly. \\nQ2T1 0 00 00 00 00 0Q1T4Q2T2+Q1T1 Q2T3+Q1T2 Q2T4+Q1T3\\nOutput\\nInputT2T1 T4T3 00 00 Q1Q2 00 00 00'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Figure 5. main) The decre ase in RMSE as the STAMP algorithm updates \\nmatrix profile with the distance profile calculated at each iteration.  inset) The \\napproximate matrix profile at the 10% mark is visually indistinguishable from \\nthe final matrix profile. \\nZilberstein [30] gives a number of desirable properties of \\nanytime algorithms, including Low Overhead , Interruptibility, \\nMonotonicity, Recognizable Quality, Diminishing Returns and \\nPreemptability (these properties are mostly obvious from their \\nnames, but full definitions are at [30]). \\nBecause each subsequence‚Äôs distance profile is bounded \\nbelow by the exact matrix profile, updating an approximate \\nmatrix profile with a distance profile with pairwise minimum \\noperation either drives the approximate solution closer the exact \\nsolution or retains the current approximate solution. Thus , we \\nhave guaranteed Monotonicity. From Figure 5, the approximate \\nmatrix profile converges to the exact matrix profile \\nsuperlinearly; therefore, we have strong Diminishing Returns. \\nWe can easily achieve Interruptibility and Preemptability by \\nsimply inserting a few lines of code between lines 5 and 6 of \\nTABLE III that read: \\n5new \\n6new \\n7new \\nif CheckForUserInterrupt  = TRUE \\n          Report({PAB, IAB}, ‚ÄòHere is an approximate answer.‚Äô) \\nif GetUserChoice = ‚Äòfurther refine‚Äô, CONTINUE, else BREAK \\nThe space and time overhead for the anytime property is \\neffectively zero, thus we have Low Overhead. This leaves only \\nthe property of Recognizable Quality. Here we must resort to a \\nprobabilistic argument.  The convergence curve shown in  \\nFigure 5 is very typical, so we could use past convergence \\ncurves to predict the quality of solution when interrupted on \\nsimilar data.  \\nD. Time and Space Complexity \\nThe overall complexity of the proposed algorithm is \\nO(n2logn) where n is the length of the time series. However, our \\nexperiments (see Section 4.1) empirically suggest that the \\nruntime of  STAMP‚Äôs growth rate is roughly O( n2) instead of \\nO(n2logn). One possible e xplanation for this is that the nlogn \\nfactor comes from the FFT subroutine. Because FFT is so \\nimportant in many applications, it is extraordinarily well \\noptimized. Thus, the empirical runtime is very close to linear. \\nIn contrast to the above, the brute for ce nested loop \\napproach has a time complexity of O(n2m). Recall the industrial \\nexample in the introduction section. We have  m = 10,080, but \\nlog(n) = 5.7, so we would expect our approach to be about \\n1,768 times faster. In fact, we are empirically even faster. The \\ncomplexity analysis downplays the details of important constant \\nfactors. The nested loop algorithm must also z -normalize the \\nsubsequences. This either requires O( nm) time, but with an \\nuntenable O(nm) space overhead, or an O( n2m) time overhead. \\nAnd r ecall that this is before a single Euclidean distance \\ncalculation is performed.  \\nFinally, we mention one quirk of our algorithm which we \\ninherit from using the highly optimized FFT subroutine. Our \\nalgorithm is fastest when n is an integer power of two, slower \\nfor non -power of two but composite numbers, and slowest \\nwhen n is prime. The difference (for otherwise similar values of \\nn) can approach a factor of 1.6x. Thus, where possible, it is \\nworth contriving the best case by tru ncation or zero-padding to \\nthe nearest power of two. \\nE. Incrementally Maintaining TSAPSS \\nUp to this point we have discussed the batch version of \\nTSAPSS. By batch, we mean that the STAMP algorithm needs \\nto see the entire time series TA and TB (or just TA if we are \\ncalculating the self -similarity join matrix profile) before \\ncreating the matrix profile. However, it would be advantageous \\nif we could build the matrix profile incrementally. Given that \\nwe have performed a batch construction of matrix profile, i f \\nnew data arrives, it would clearly be preferable to incrementally \\nadjust the current profile, rather than start from scratch.  \\nBecause the matrix profile solves both the times series motif \\nand the time series discord problems, an incremental version of \\nSTAMP would automatically provide the first incremental \\nversions of both algorithms. We call such an algorithm the \\nSTAMPI (STAMP Incremental) algorithm. \\nWe will demonstrate our ability to incrementally maintain \\nthe matrix profile in this section. For simpli city and brevity \\nTABLE IV only shows the algorithm to maintain the self -\\nsimilarity join. The generalizations are obvious. \\nTABLE IV. THE STAMPI ALGORITHM \\nProcedure STAMPI(TA, t, PAA, IAA) \\nInput: The original time series TA, a new data point t following TA, the \\nmatrix profile PAA and its associated matrix profile index IAA of TA.  \\nOutput: The incrementally updated matrix profile PAA,new and its matrix \\nprofile index IAA,new of the current time series TA,new= TA, t. \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nTA,new = [TA, t] \\nS ‚Üê last subsequence in TA,new, idx ‚Üê index of S in TA,new \\nD ‚Üê MASS (S, TA) \\nPAA, IAA ‚Üê ElementWiseMin(PAA, IAA, D, idx) \\npAA,last, iAA,last ‚Üê FindMin(D) \\nPAA,new ‚Üê [PAA, pAA,last], IAA,new ‚Üê [IAA, iAA,last] \\nreturn PAA,new, IAA,new \\nFor clarity, we denote the updated time series as TA,new, the \\nupdated matrix profile as PAA,new and the associated matrix \\nprofile index as IAA,new. As each additional data point t arrives, \\nthe size of the time series TA increases by one, and a new \\nsubsequence S is generated at the end of TA,new. In line 3 we \\nobtain the distance profile of S with regard to TA. Then, as in the \\noriginal STAMP algorithm,  in line 4 we perform  a pairwise \\ncomparison between every element in D with the corresponding \\nelement in PAA to see if the corresponding element in PAA needs \\nto be updated. In line 5, we find the nearest neighbor of S and \\nthe associated index by evaluating the minimum value of D. \\nFinally, in line 6, we obtain the new matrix profile and \\nassociated matrix profile index by concatenating the results in \\nline 4 and line 5.  \\nThe time complexity of the STAMP I algorithm is O(nlogn) \\nwhere n is the length of size of th e current time series TA. Note \\nthat as we maintain the profile, each incremental call of \\nSTAMPI requires invoking the FFT subroutine with a slightly \\nlonger time series ( n becomes n+1 ). Thus it gets very slightly \\nslower at each time step. Therefore, the best way to measure the \\nperformance is to ask for the Maximum Time Horizon (MTH), \\nin essence the answer to this question: ‚Äú Given this arrival rate, \\nhow long can we maintain the profile before we can no longer \\nupdate fast enough?‚Äù \\nNote that the subsequence length m is not considered in the \\nMTH evaluation. Recall that overall time complexity of the \\n10,0000\\nRMSE\\niteration1,000\\napproximate matrix \\nprofile at 1,000 iterations.\\nexact matrix profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='algorithm is determined by the efficiency of the FFT \\nsubroutine, which is independent of m. We have computed the \\nMTH for two common scenarios of interest to the community. \\n\\uf0b7 House Electrical Demand  [19]: This dataset is updated \\nevery eight seconds. By iteratively calling the STAMP I \\nalgorithm, we can maintain the profile for 5.8 years.  \\n\\uf0b7 Oil Refinery :  Most telemetry in oil refineries is sampled \\nat once a minute [26]. The relatively low sampling rate \\nreflects the ‚Äúinertia‚Äù of massive boilers/condensers. Even if \\nwe maintain the profile for 40 years, the update time is only \\naround 6.78 seconds. Moreover, th e raw data, matrix \\nprofile and index would only require 0.5 gigabytes of main \\nmemory. Thus the MTH here is forty plus years. Given \\nprojected improvements in hardware, this effectively \\nmeans we can maintain the matrix profile forever. \\nAs impressive as these  numbers are, they are actually quite \\npessimistic. For simplicity we assume that every value in the \\nmatrix profile index will be updated at each time step.  \\nHowever, empirically, much less than 0.1% of them need to be \\nupdated. If it is possible to prove an upper bound on the number \\nof changes to the matrix profile index per update, then we could \\ngreatly extend the MTH  or handle much faster sampling rates. \\nWe leave such considerations for future work. \\nIV. EXPERIMENTAL EVALUATION \\nWe begin by stating our experimen tal philosophy. We have \\ndesigned all experiments such that they are easily reproducible. \\nTo this end, we have built a webpage [24] which contains al l \\ndatasets and code used in this work, together with spreadsheets \\nwhich contain the raw numbers and some supporting videos.  \\nGiven page limits, and the utility of our algorithm for a host \\nof existing (and several new) data mining tasks, we have chosen \\nto c onduct exceptionally broad  but shallow experiments. We \\nhave conducted such deep detailed experiments and placed \\nthem at [24]. Unless otherwise state d we measure wall clock \\ntime on an Intel i7@4GHz with 4 cores. \\nA. Scalability of Profile-Based Self-Join \\nBecause the time performance of STAMP is independent of \\nthe data quality or any user inputs (there are none except the \\nchoice of m, which does not affect the speed), our scalability \\nexperiments are unusually brief.  In TABLE V we show the \\ntime required for a self -join with m fixed to 256, for \\nincreasingly long time series.  \\nTABLE V.  TIME REQUIRED FOR A SELF-JOIN WITH M = 256, VARYING N \\nValue of n 217 218 219 220 221 \\nTime Required 15.1 min 70.4 min 5.4 hours 24.4 hours 4.2 days \\nIn TABLE VI, we show the time required for a self -join \\nwith n fixed to 2 17, for increasing long m. Again recall that \\nunlike virtually all other time series data mining algorithms in \\nthe literature whose performance degrades for longer \\nsubsequences [8][18], the running time of STAMP does not \\ndepend on m. \\nTABLE VI. TIME REQUIRED FOR A SELF-JOIN WITH N= 217, VARYING M \\nValue of m 64 128 256 512 1,024 \\nTime Required 15.1 min 15.1 min 15.1 min 15.0 min 14.5 min \\nFinally, we further exploit the simple parallelizability of the \\nalgorithm by using four 16 -core virtual machines on Microsoft \\nAzure to redo the two -million join ( n = 2 21 and m = 256) \\nexperiment. By scaling up the computational power, we have \\nreduced the running time from 4.2 days to just 14.1 hours. This \\nuse of cloud computing required writing just few dozen lines of \\nsimple additional code [24]. \\nIt is difficult to find good baselines to compare to. We \\nbelieve STAMP is the only algorithm that does full, exact joins \\non time series subsequences. Most algorithms do only threshold \\njoins and/or do approximate joins, and are not specialized for \\ntime series subsequences. However, we searched for the best \\nbaselines and made the following concessions to  the rival \\nalgorithms to allow comparisons. \\n\\uf0b7 TSFRDAA [11]: While STAMP returns all nearest \\nneighbors, we adjust the threshold (the selectively) of \\nTSFRDAA such that only needs to return the top 1% nearest \\nneighbors, a much easier task. \\n\\uf0b7 HDSJI-SAX [12]: This method  allows false negatives ; we \\nignore this. It needs time to build indexes ; we do not count \\nthis time, and as above, we allow it to return  the only the \\ntop 1% nearest neighbors ( as bef ore, not the 100% the \\nSTAMP returns, and therefore a much easier task.). \\n\\uf0b7 Optimized Nested Loop (ONL): Here we use a nested -loop \\njoin optimized in the following manner. We use a state -of-\\nthe-art DFT indexing technique to do the search in the \\ninner loop, and we do not count the time needed to build \\nthe indexes [8]. We carefully adjust  parameters for best \\nperformance.  \\nWe consider the first 2 18 data points of the ECG dataset \\nused in [22], with a query length of 256, about one heartbeat ; \\nTABLE VII shows the results.  \\nTABLE VII. TIME FOR A SELF-JOIN WITH M = 256, VARYING ALGORITHMS \\nAlgorithm TSFRDAA HDSJI-SAX ONL STAMP \\nTime Required 51.7 hours 19.6 min 28.1 hours 1.17 hours \\nAs these results show, even if we ignore the limitations of \\nthe baseline methods, STAMP is still significantly faster. \\nB. Profile-Based Self-Join \\nA recent paper notes that many fundamental problems in \\nseismology can be solved by joining seismometer telemetry \\n[28], including the disc overy of foreshocks, aftershocks, \\ntriggered earthquakes, volcanic activity and induced seismicity. \\nHowever, the paper notes a join with a query length of 200 on a \\ndata stream of length 604,781 requires 9.5 days. Their solution, \\na clever transformation of t he data to allow LSH based \\ntechniques, does achieve significant speedup, but at the cost of \\nfalse negatives and the need for significant parameter tuning.  \\nThe authors kindly shared their data, and, as we hint at in \\nFigure 6, confirmed that STAMP does not have false negatives. \\n \\nFigure 6. top) An excerpt of a seismic time series aligned with its matrix \\nprofile (bottom). The ground truth provided by the authors of [28] requires that \\nthe events occurring at time 4,050 and 7,800 match. \\nWe repeated the n = 604,781, m = 200 experime nt and \\nfound it took just 8.9 hours to finish. As impressive as this is, in \\nthe next section we show that we can do even better.   \\n4,000 5,000 6,000 7,000 8,000 9,000\\n0\\n5\\n10\\n15\\nSeismic time series (excerpt) \\nMatrix Profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='C. The Utility of Anytime STAMP \\nThe seismology dataset offers an excellent opportunity to \\ndemonstrate the utility of the anytime version of our algorithm. \\nThe authors of [28] revealed in their long -term ambition of \\nmining even larger datasets [3]. In Figure 7 we repeated the \\nexperiment with the snippet shown in Figure 6, this time \\nreporting the best-so-far matrix profile reported by the \\nalgorithm at various milestones. Even with just 0.25% of the \\ndistances computed (that is to say, 400 times faster) the correct \\nanswer has emerged. Thus, we can provide the correct answers \\nto the seismologists in just minutes, rather than the 9.5 days.  \\n \\nFigure 7. top) An excerpt of the seismic data that is also shown in Figure 6. \\ntop-to-bottom) The approximations of the matrix profile for increasing \\ninterrupt times. By the time we have computed just 0.25% of the calculations \\nrequired, the minimum of the matrix profile points to the ground truth. \\nTo show the generality of this anytime feature of STAMP, \\nwe consider a very different dataset. As shown in Figure \\n8.inset), it is possible to convert DNA to a time ser ies [22]. We \\nconverted the Y-chromosome of the Chimpanzee this way. The \\nresulting time series is  little over one -million in length. We \\nperformed a self-join with m = 60,000.  Figure 8.bottom shows \\nthe best motif is so well conserved (ignoring the first 20%), that \\nit must correspond to a recent (in evolutionary tim e) gene \\nduplication event. In fact, in a subsequent analysis we \\ndiscovered that ‚Äúmuch of the Y (Chimp chromosome) consists of \\nlengthy, highly similar repeat units, or ‚Äòamplicons‚Äô‚Äù [9]. \\n \\nFigure 8. top) The Y -chromosome of the Chimp in time series space with its \\nmatrix profile. bottom) A zoom -in of the top motif discovered using anytime \\nSTAMP, we believe it to be an amplicon [9]. \\nThis demanding join would take just over a day of CPU \\ntime (see TABLE V). However, using anytime STAMP we \\nhave the result shown above after doing just 0.021% of the \\ncomputations, in about 18 seconds. At [24] we have videos that \\nshow the rapid convergence of the anytime variant of STAMP. \\nD. Profile-Based Similarity Join Set \\nIn this section we show two uses of similarity join set. The \\nfirst use is more familiar to APSS users, quantifying what is \\nsimilar between two time series. The second example, \\nquantifying what is different between two time series, is novel  \\nand can only be supported by threshold -free algorithms that \\nreport the nearest neighbor for all objects. \\nTime Series Set Similarity \\nGiven two (or more) time series collected under different \\nconditions or treatments, a data analyst may wish to know what \\npatterns (if any) are conserved between the two time series. To \\ndemonstrate the utility of automating this, we consider a simple \\nbut in tuitive example. Figure 9 shows the raw audio of two \\npopular songs converted to Mel Frequency Cepstral \\nCoefficients (MFCCs). Specifically, the songs used in this \\nexample are ‚ÄúUnder Pressure‚Äù by Queen an d David Bowie and \\n‚ÄúIce Ice Baby ‚Äù by the American rapper Vanilla Ice. Normally, \\nthere are 13 MFCCs; here, we consider just one for simplicity. \\n \\n \\nFigure 9. Two songs represented by just the 2 nd MFCC at 100Hz. We \\nrecognize that it is difficult to see any structure in these times series; however, \\nthis difficulty is the motivation for this experiment. \\nEven for these two relatively short time series, visual \\ninspection does not offer immediate answers. The problem here \\nis compounded by the size reproduction, but is not significantly \\neasier with large-scale [24] or interactive graphic tools. We ran \\nJAB (Queen-Bowie, Vanilla Ice) on these datasets with m = 500 \\n(five seconds), the best match, corresponding the minimum \\nvalue of the matrix profile PAB, is shown in Figure 10. \\n \\nFigure 10. The result of JAB (Queen-Bowie (red/bold), Vanilla-Ice (green/fine)) \\nproduces a strongly conserved five second region. \\nReaders may know the cause for this highly conserved \\nsubsequence. It corresponds to the famous baseline of ‚ÄúUnder \\nPressure,‚Äù which was sampled (plagiarized) by Vanilla Ice in \\nhis song. The join took 21.9 seconds. The ability to find \\nconserved structure in apparently disparate time series could \\nopen many avenues of research in medicine and industry. \\nTime Series Difference \\nWe introduce the Time Series Diff (TSD) operator, which \\ninformally asks ‚ÄúWhat happens in time series T A, that does not \\nhappen in time series TB?‚Äù Here TA/TB could be an ECG before \\na drug is administered/after a drug is administered, telemetry \\nbefore a successful launch/before a catastrophic launch etc. The \\nTSD is simply the subsequence referred to by the maximum \\nvalue of the JAB join‚Äôs profile PAB. \\nWe begin with a simple intuitive example. The UK and US \\nversions of the Harry Potter audiobook series are performed by \\ndifferent narrators, and have a handful of differences in the text. \\nFor example, the UK version contains: \\nHarry was passionate about Quidditch. He had played as Seeker on the Gryffindor \\nhouse Quidditch team ever since his first year at Hogwarts and owned a Firebolt, one \\nof the best racing brooms in the world... \\nBut the corresponding USA version has: \\nHarry had been on the Gryffindor House Quidditch team ever since his first year at \\nHogwarts and owned one of the best racing brooms in the world, a Firebolt. \\nAs shown in Figure 11, we can convert the audio \\ncorresponding to these snippets into MFCCs and invoke a JAB \\njoin set to produce a matrix profile PAB that represents the \\ndifferences between them. As Figure 11.left shows, the low \\nvalues of this profile correspond to identical spoken phrases \\n(despite having two different narrators). However here we are \\n4,000 5,000 6,000 7,000 8,000 9,000\\ndata\\n0.25%\\n1%\\n100%\\n0 1,000,0000\\n100\\n200\\nPan troglodytes Y-chromosome \\n0 60,000\\n12,749,475 to 14,249,474 bp\\n622,725 to 2,122,724 bp\\nT1 = 0;\\nfor i = 1 to length(chromosome) \\nif chromosomei = A, then Ti+1 = Ti + 2 \\nif chromosomei = G, then Ti+1 = Ti + 1\\nif chromosomei = C, then Ti+1 = Ti - 1 \\nif chromosomei = T, then Ti+1 = Ti - 2 \\nend\\n0\\nQueen-Bowie\\n1,000 2,000\\n-10\\n0\\n10\\nVanilla Ice\\n0 250 500-3\\n-2\\n-1\\n0\\n1\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='interested in the differences, the maximum value of the profile. \\nAs we can see in Figure 11.right, here the profile corresponds \\nto a phrase unique to the USA edition.  \\n \\nFigure 11. The 2 nd MFCC of snippets from the USA ( pink/bold) and UK \\n(green/fine) Harry Potter audiobooks.  The JAB join of the two longer sections \\nin the main text produce s mostly small values in the profile correspond to the \\nsame phrase (left), the largest value in  the profile corresponds to a phrase \\nunique to the USA edition (right). \\nThe time required to do this is just 0.067 seconds, much \\nfaster than real time. While this demonstration is trivial, in [24] \\nwe show an example applied to ECG telemetry.  \\nE. Profile-Based Motif Discovery \\nSince their introduction in 2003, time series motifs have \\nbecome one of the most frequently used primitives in time  \\nseries data mining, with applications in dozens of domains [2]. \\nThere are several proposed definitions for time series motifs, \\nbut in [18] it is argued that if you can solve the most basic \\nvariant, the closest (non -trivial) pair of subsequences, then all \\nother variants only require some minor additional calculations. \\nNote that the locations of the two (tying) minimum values of \\nthe matrix profile are exactly the locations of the 1st motif pair. \\nThe fastest known exact algorithm for computing time \\nseries motifs is the MK algorithm [18]. Note, however, that this \\nalgorithm‚Äôs time performance depends on the time series itself. \\nIn contrast, the Profile -Based Motif Discovery (PBMD) takes \\ntime independent of the data. To see this, we compared the two \\napproaches on an electrocardiogram of length 65,536. In Figure \\n12.left we ask what happens as we search for lon ger and longer \\nmotifs. In Figure 12.right we ask what happens if the motif \\nlength is fixed to m = 512, but the data becomes increasing \\nnoisy.  \\n \\nFigure 12. The time required to find the top -motif pairs in a time series of \\nlength 216 for increasingly long motif lengths ( left), and for a length fixed to \\n512, but in the face of increasing noise levels (right). \\nThese results show that  even in the best case for MK, \\nPBMD is competitive, but as we have longer queries and/or \\nnoisier data, its advantage becomes unassailable. Moreover, \\nPBMD inherits STAMP‚Äôs anytime and incremental \\ncomputability, and is easily parallelizable.  \\nF. Profile-Based Discord Discovery \\nA time series discord  is the subsequence that has the \\nmaximum distance to its nearest neighbor. While this is a \\nsimple definition, time series discords are known to be very \\ncompetitive as novelty/anomaly detectors  [5]. Note that as \\nshown in Figure 13, the time series discord is encoded as the \\nmaximum value in a matrix profile. \\n \\nFigure 13. top) An excerpt from an ECG incorporating a premature ventricular \\ncontraction (red/bold). bottom) The time series profile peaks exactly at the \\nbeginning of the PVC. \\nThe time taken to compute the discord  is obviously just the \\ntime needed to compute the matrix profile ( here, 0.9 seconds). \\nThere are a few dozen discord discovery algorithms in the \\nliterature. Some of them may be competitive in the best case, \\nbut just like motif -discovery algorithms they all degenerate to \\nbrute force search in the worst case, and none allow the anytime \\nproperties that we inherit from using STAMP.  \\nG. Incrementally Maintaining Motifs and Discords \\nWe have demonstrated the ability to detect time series \\nmotifs and discords using the matrix profile in the previous two \\nsections. However, we assumed that the entire time series was \\navailable beforehand. Here we remove this assumption and \\nshow how STAMP I allows us to incrementa lly maintain time \\nseries motifs/ discords in an online fashion. There are other \\nattempts at one [20][2] or both [25] of these tasks, but they are \\nall approximate and allow false dismissals.   \\nIn Section III.E, we introduced the STAMPI algorithm. The \\nability to incrementally maintain the matrix profile implies the \\nability to exactly maintain the time series motif [18] and/or time \\nseries discord [5] in streaming data. We simply need to keep \\ntrack of the extreme values of the incrementally-growing matrix \\nprofile, report a new pair of motifs when a new minimum value \\nis detected, and report a new discord when we see a new \\nmaximum value. \\nWe demonstrate the utility of these ideas on the AMPds \\ndataset [13]. Here the kitchen fridge and the heat pump are both \\nplugged into a single metered power supply. For the first week, \\nonly the refrigerator is running. At the end of the week, the \\nweather gets cold and the heat pump is turned on. The sampling \\nrate is one sample/m inute, and th e subsequence length is 100. \\nWe apply the STAMP algorithm to the first three days of data, \\nthen invoke STAMP I to handle newly arriving data , report an \\nevent when we detect a new extreme value. \\nOur first event occurs at the 9,864 th minute (6 da y 20 hour \\n24 minute). As shown in Figure 14, a new minimum value is \\ndetected, which indicates a new time series motif. The just -\\narrived 100 -minute-long pattern looks v ery similar to another \\npattern that occurred five hours earlier. While there is a lot of \\nregularity in the fridge data in general, the exceptional \\nsimilarity observed here suggested some underlying physical \\nmechanism that caused such a perfectly -conserved pattern, \\nperhaps a mechanical ice-making ‚Äúsubroutine.‚Äù \\n \\nFigure 14. top) The matrix profile of the first 9,864 minutes of data. bottom) \\nThe minimum value of the matrix profile corresponds to a pair of time series \\nmotifs in the power usage data. right) The time series motif detected. \\nOur second event occurs at the 10,473 th minute (7 day 6 \\nhour 33 minute). As shown in Figure 15, a new maximum value \\n‚Ä¶indor house Quidditch team ever since his first ye‚Ä¶\\nHarry had been on the Gryffindor House Quidditch te..\\nsince his first year at Hogwarts and owned a Fire..\\nsince his first year at Hogwarts and owned on..\\nED = 2.9 \\nClosest Match \\n0 100(1.6 seconds) 0 100\\nED = 10.4\\n(1.6 seconds)\\nFurthest Match (Time Series Difference) \\n256 512 1,024 2,048 4,096\\n0\\n40\\n80\\n120\\n160\\n200\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nSubsequence Length  (m)\\nTime Taken (min)\\n80 40 0\\n0\\n70\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nNoise Added (dB)  \\n0 1000 2000 3000\\nECG qtdb\\nSel102\\n(excerpt)\\nPremature Ventricular \\nContraction\\nTime Series Profile\\n0 5,000 10,000\\n0 100\\nnew minimum value\\nnew motifMatrix Profile\\nPower Usage Data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='is detected, which indicates a new time series discord. The time \\nseries discord corresponds to the first occurrence of a heat \\npump pattern in the power usage data.  \\n \\nFigure 15. top) The matrix profile for the first 10,473 minutes. bottom) The \\nmaximum value of the matrix profile corresponds to a time series discord. \\nright) The time series discord detected is the first  heat pump pattern \\noccurrence in the dataset. \\nThe maximum time needed to process a single data point \\nwith STAMP I in this dataset is 0.005 second s, which is less \\nthan 0.01% of the data sampling rate. Thus, on this dataset we \\ncould continue monitoring with the STAMP I algorithm for \\nseveral decades before running out of time or memory. \\nH. Profile-Based Shapelet Discovery \\nShapelets are time series subsequences which are in some \\nsense the maximally representative of a class [23][27]. \\nShapelets can be used to classify time series (essentially, the \\nnearest shapelet algorithm), offering the benefits of speed, \\nintuitiveness and at least on some domains, significantly \\nimproved accuracy [23]. However, these advantages come at \\nthe cost of a very expensive training phase, with O( n2m4) time \\ncomplexity, where m is the length of the longest time series \\nobject in the dataset, and n is the number of objects in the \\ntraining set. In order to mitigate this high ti me complexity, \\nresearchers have proposed various distance pruning techniques \\nand candidate ranking approaches for both the admissible [27] \\nand appro ximate [23] shapelet discovery. Nevertheless, \\nscalability remains the bottleneck.  \\nBecause shapelets are essentially supervised motifs, and we \\nhave shown that STAMP can find motifs very quickly, it is \\nnatural to ask if STAMP has implications for shapelet \\ndiscovery. While space limitations prohibit a detailed \\nconsideration of this question, we briefly sketch out and test \\nthis possibility as follows. \\nAs shown in Figure 16, we can use matrix profile to \\nheuristically ‚Äúsuggest‚Äù candidate shapelets. We consider two \\ntime series TA (green/bold) and TB (pink/light) with class 1 and \\nclass 0 being their corresponding class label, and we take  JAB, \\nJAA, JBA and JBB. Our claim is the differences in the heights of \\nPAB, PAA (or PBA, PBB) are strong indicators of good candidate \\nshapelets. The intuition is that if a discriminative pattern is \\npresent in say, class 1, but not in class 0, then we expect to see a \\n‚Äúbump‚Äù in the PAB (the intuition holds if the order is reversed). \\nA significant difference (quantified by a threshold shown in \\ndashed line) between the heights of PAA and PAB curves \\ntherefore indicates the occurrence of good candidate shapelets, \\npatterns that only occur in one of the two classes. \\n \\nFigure 16. top.left) Two time series TA and TB formed by concatenating \\ninstances of class 1 and 0 respectively of ArrowHead [6]. bottom) The height \\ndifference between PAB (or PBA) and PAA (or PBB) are suggestive of  good \\nshapelets. top.right) An example of good shapelet extracted from class 1. \\nThe time taken to compute  all four matrix profiles is 1.0 \\nseconds and the time to further evaluate the two twelve \\ncandidates selected takes 2.7 seconds. On the same machine, \\nthe brute force shapelet classifier takes 4.2 minutes with 2,364 \\ncandidates. Note that, in this toy demonst ration, the speedup is \\n68X, however, for larger datasets, the speedup is greater [24]. \\nI. Profile-Based Semantic Segmentation \\nThe goal of time series semantic segmentation is to partition \\na dataset containing multiple activities/regimes into atomic \\nbehaviors or conditions. For example, for human activity data \\nthe regimes might later be mapped to { eating, working, \\ncommuting,...}[29]. As this example suggests, most work in this \\narea is highly domain -dependent. In contrast, here we show \\nhow the matrix profile index can be emp loyed for domain \\nagnostic time series segmentation.  \\nThe intuition of our approach is as follows. Within a single \\nregime we might expect that most subsequences will have a \\nnearest neighbor close by (in time). For example, consider the \\ntoy problem shown in Figure 17 which shows two obvious \\nregimes. We would expect that the nearest neighbor to the first \\nrun gait cycle is the second or third or fourth run cycle, but it \\nwill almost certainly not be one of the walk cycles. In general, \\nthis tendency for nearest neighbor pointers not to cross the \\nboundaries corresponding to regime changes may be sufficient \\nto discover these boundaries, and of course, this is precisely the \\ninformation that is encoded in the matrix profile index. \\nThus, for each point we count how many ‚Äúarcs‚Äù connecting \\ntwo nearest neighbors cross it if we connect each subsequence \\nto its nearest neighbor as shown in Figure 17. \\n \\nFigure 17. top) A (toy) time series ( red) and nearest neighbor locations for \\neach subsequence. bottom) The number of arcs crossing above each \\nsubsequence. \\nWe applied the procedure described above to a heavily \\nstudied activity segmentation problem [29], which was derived \\nfrom the CMU Motion Capture data base [16]. The recordings \\nare represented as multi -dimensional time series, and most \\nresearch efforts carefully select the best subset for the \\nsegmentation task. For example, [29] states that ‚Äú we only \\nconsider the 14 most informative joints out of 29 .‚Äù In contrast, \\nwe attempt this with a single dimension. The only parameter we \\nneed to set is sliding window size . In Figure 18 we show the \\nsegmenting results obtained using our approach , with  \\nannotations taken from [29] for context. \\nMuch of the evaluation in this community lacks formal \\nmetrics, preferring instead visual sanity tests like the one in \\nFigure 18. Given this, we can say that our approach is very \\ncompetitive on this dataset, in spite of the fact that we \\nhandicapped ourselves to only consider one dimension.  \\n0 100\\n0 10,0005,000\\nMatrix Profile\\nPower Usage Data\\nnew maximum value\\nnew discord\\n0\\n7\\n0 1,400\\nPABPAA\\n0 1,600\\nTA\\nTB\\n0 300\\nAn example of good shapelet\\n0 1,400\\nPBA\\nPBB\\n8\\n0\\n0 1,400\\nDiff(PAB ,PAA) Threshold\\n-1\\n4\\n-2\\n4\\n0 1,400\\nDiff(PBA ,PBB) Threshold\\n0\\n1\\n2\\n1 2 0\\nwalking slow    walking slow     run       run run run\\nNumber of arcs intersecting each point'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content=\"Figure 18. top) A matrix profile ( blue) obtained for the time series ( red) and \\nnumber of arcs crossing each point ( green). Low values of this green curve \\ncoorespond to candidate split points. bottom) Human annotations of the \\nactivities: w ‚Äì walk, s ‚Äì stretch, p ‚Äì punch, c ‚Äì chop, t ‚Äì turn, d ‚Äì drink. \\nV. CONCLUSION \\nWe have introduced a scalable algorithm for creating time \\nseries subsequences joins. Our algorithm is simple, fast, \\nparallelizable and parameter -free, and can be in crementally \\nupdated for moderately fast data arrival rates. We have shown \\nthat our algorithm has implications for many existing tasks, \\nsuch as motif discovery, discord discovery, shapelet discovery \\nand semantic segmentation, and may open up new avenues for  \\nresearch, including computing various definitions of time series \\nset difference. Our code is freely available for the community to \\nconfirm, extend and exploit our ideas. \\nREFERENCES \\n[1] R. J. Bayardo, Y. Ma and R. Srikant, ‚ÄúScaling up all pairs similarity \\nsearch,‚Äù WWW 2007, pp 131-140. \\n[2] N. Begum and E. Keogh, ‚ÄúRare time series motif discovery from \\nunbounded streams,‚Äù PVLDB 8(2): 149-160, 2014. \\n[3] G. Beroza, ‚ÄúPersonal Correspondence,‚Äù Jan 21th, 2016. \\n[4] T. Bouezmarni and J. Rombouts, ‚ÄúNonparametric density estimation for \\npositive time series,‚Äù CSDA, 54, 245-261, 2010.  \\n[5] V. Chandola, D. Cheboli and V. Kumar, ‚ÄúDetecting anomalies in a time \\nseries database,‚Äù UMN TR09-004. \\n[6] T. Chen  et al ., ‚ÄúThe UCR time series classification archive,‚Äù \\nhttp://www.cs.ucr.edu/~eamonn/time_series_data/. \\n[7] ‚ÄúConvolution - Wikipedia, the free encyclopedia,‚Äù  \\nhttps://en.wikipedia.org/wiki/Convolution, Accessed: 2016-01-19. \\n[8] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang and E. J. Keogh, \\n‚ÄúQuerying and mining of time series data: experimental comparison of \\nrepresentations and distance measures,‚Äù PVLDB 1(2): 1542-1552. 2008. \\n[9] J. Hughes et al. , ‚ÄúChimpanzee and human Y chromosomes are \\nremarkably divergent in structure‚Äù Nature 463, (2010). \\n[10] H. Lee, R. Ng and K. Shim, ‚ÄúSimilarity join size estimation using \\nLocality sensitive hashing,‚Äù PVLDB, 4(6):338‚Äì349, 2011. \\n[11] W. Luo, H. Tan, H. Mao  and L. M.  Ni, ‚ÄúEfficient similarity joins on \\nmassive high-dimensional datasets using mapreduce,‚Äù In MDM'12, IEEE \\nComputer Society, pp. 1-10. \\n[12] Y. Ma, X. Meng and S. Wang, ‚ÄúParallel similarity joins on massive high-\\ndimensional data using MapReduce ,‚Äù Concurrency and Computation , \\nVolume 28, Issue 1 Jan 2016. Pages 166‚Äì183. \\n[13] S. V. Makonin, ‚ÄúAMPds: a public dataset for load disaggregation and \\neco-feedback research,‚Äù EPEC 2013, pp 1-6. \\n[14] MASS: http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html \\n[15] G. D. F. Morales and A. Gionis, ‚Äústreaming similarity self-join,‚Äù Proc. \\nVLDB Endow, 2016. \\n[16] Motion Capture Database, http://mocap.cs.cmu.edu/ \\n[17] A. Mueen, H. Hamooni and T. Estrada, ‚ÄúTime series join on subsequence \\ncorrelation,‚Äù IEEE ICDM 2014, pp. 450-459. \\n[18] A. Mueen, E. Keogh, Q. Zhu, S. Cash and B. Westover, ‚ÄúExact discovery \\nof time series motif,‚Äù SDM 2009. \\n[19] D. Murray et al. , ‚ÄúA data management platform for personalised real-\\ntime energy feedback,‚Äù In EEDAL 2015. \\n[20] V. Niennattrakul et al, ‚ÄúData editing techniques to allow the application \\nof distance-based outlier detection to streams,‚Äù ICDM 2010: 947-952. \\n[21] D. Patnaik, et al, ‚ÄúSustainable operation and management of data center \\nchillers using temporal data mining,‚Äù KDD 2009. \\n[22] T. Rakthanmanon et al., ‚ÄúSearching and Mining Trillions of Time Series \\nSubsequences under Dynamic Time Warping,‚Äù In KDD 2012, 262-270. \\n[23] T. Rakthanmanon and E. Keogh, ‚ÄúFast shapelets: a scalable algorithm for \\ndiscovering time series shapelets,‚Äù SDM, 2013. \\n[24] Supporting page. http://www.cs.ucr.edu/~eamonn/MatrixProfile.html \\n[25] C. D. Truong and D. T.  Anh, ‚ÄúAn Efficient Method for Motif and \\nAnomaly Detection in Time Series‚Äù IJBIDM, Vol. 10, No. 4, 2015. \\n[26] A. Tucker and X. Liu, ‚ÄúA Bayesian  Network Approach to Explaining \\nTime Series with Changing Structure,‚Äù Intell Data Anal, 8 (5) (2004). \\n[27] L. Ye and E. Keogh, ‚ÄúTime series shapelets: a new primitive for data \\nmining,‚Äù ACM SIGKDD, 2009, pp 947-56. \\n[28] C. Yoon, O. O‚ÄôReilly, K. Bergen and G. Beroza, ‚ÄúEarthquake detection \\nthrough computationally efficient similarity search,‚Äù Sci. Adv. 2015. \\n[29] F. Zhou, F. Torre and J.  Hodgins ‚Äú Aligned Cluster Analysis for \\nTemporal Segmentation of Human Motion,‚Äù IEEE FG'2008. \\n[30] S. Zilberstein and S. Russell, ‚ÄúApproximate Reasoning Using Anytime \\nAlgorithms,‚Äù In Imprecise and Approximate Computation , Kluwer \\nAcademic Publishers, 1995. \\n \\n0 5,000 10,000\\nMatrix  Profile\\nSplit Points   \\nPrediction\\nOne-dimension of multi-d time series: Subject 86, recording 4, dimension 30\\nW S P N      W C T           D            P         W\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Matrix Profile I: All Pairs Similarity Joins for Time Series: \\nA Unifying View that Includes Motifs, Discords and Shapelets \\nChin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,  \\nHoang Anh Dau, ‚Ä†Diego Furtado Silva, ‚Ä°Abdullah Mueen, and Eamonn Keogh \\nUniversity of California, Riverside, ‚Ä†Universidade de S√£o Paulo, ‚Ä°University of New Mexico \\n{myeh003, yzhu015, lulan001, nbegu001, yding007, hdau001}@ucr.edu, diegofsilva@icmc.usp.br, mueen@unm.edu, eamonn@cs.ucr.edu \\n \\nAbstract‚Äî The all-pairs-similarity-search (or similarity join) \\nproblem has been extensively studied for text and a handful of \\nother datatypes. However, surprisingly little progress has been  \\nmade on similarity joins for time series subsequences. The lack of  \\nprogress probably stems from the daunting nature of the \\nproblem. For even modest sized data sets the obvious nested -loop \\nalgorithm can take months, and the typical speed -up techniques \\nin this domain (i.e., indexing, lower -bounding, triangular -\\ninequality pruning and early abandoning) at best produce one or \\ntwo orders of magnitude speedup. In this work we introduce a \\nnovel scalable algorithm for time series subsequence all -pairs-\\nsimilarity-search. For exceptionally large datasets, the algorithm \\ncan be trivially cast as an anytime algorithm and produce high -\\nquality approximate solutions in reasonable  time. The exact \\nsimilarity join algorithm computes the answer to the time series \\nmotif and time series discord  problem as a side -effect, and our \\nalgorithm incidentally provides the fastest known algorithm for \\nboth these  extensively-studied problems. We de monstrate the \\nutility of our ideas for many time series data mining problems, \\nincluding motif discovery, novelty discovery,  shapelet discovery,  \\nsemantic segmentation, density estimation, and contrast set \\nmining.    \\nKeywords‚ÄîTime Series; Similarity Joins; Motif Discovery \\nI. INTRODUCTION \\nThe all-pairs-similarity-search (also known as similarity \\njoin) problem comes in several variants. The basic task is this: \\nGiven a collection of data objects, retrieve the nearest neighbor \\nfor each object . In the text domain the  algorithm has \\napplications in a host of problems, including community \\ndiscovery, duplicate detection, collaborative filtering, \\nclustering, and query refinement [1]. While virtually all text \\nprocessing algorithms have analogues in time series data \\nmining, there has been surprisingly little progress on Time \\nSeries subsequences All-Pairs-Similarity-Search (TSAPSS). \\nWe believe that this lack of progress stems not from a lack \\nof interest in this useful primitive, but from the daunting nature \\nof the problem. Consider the following example that reflects the \\nneeds of an industrial collaborator. A boiler at a che mical \\nrefinery reports pressure once a minute. After a year, we have a \\ntime series of length 525,600. A plant manager may wish to do \\na similarity self-join on this data with week -long subsequences \\n(10,080) to discover operating regimes (summer vs. winter o r \\nlight distillate vs. heavy distillate etc.) The obvious nested loop \\nalgorithm requires 132,880,692,960 Euclidean distance \\ncomputations. If we assume each one takes 0.0001 seconds, \\nthen the join will take 153.8 days. The core contribution of this \\nwork is to show that we can reduce this time to 6.3 hours, using \\nan off-the-shelf desktop computer. Moreover, we show that this \\njoin can be computed and/or updated incrementally. Thus we \\ncould maintain this join essentially forever on a standard \\ndesktop, even if the data arrival frequency was much faster than \\nonce a minute. \\nOur algorithm uses an ultra -fast similarity search algorithm \\nunder z -normalized Euclidean distance as a subroutine, \\nexploiting the overlap between subsequences using the classic \\nFast Fourier Transform (FFT) algorithm.  \\nOur method has the following advantages/features: \\n\\uf0b7 It is exact, providing no false positives or false dismissals. \\n\\uf0b7 It is simple and parameter -free. In contrast, the more \\ngeneral metric space APSS algorithms require building and \\ntuning spatial access methods and/or hash functions.  \\n\\uf0b7 Our algorithm requires an inconsequential space overhead, \\njust O(n) with a small constant factor. \\n\\uf0b7 While our exact algorithm is extremely scalable, for \\nextremely large datasets we can compute the results in an \\nanytime fashion, allowing ultra-fast approximate solutions.  \\n\\uf0b7 Having computed the similarity join for a dataset, we can \\nincrementally upd ate it very efficiently. In many domains \\nthis means we can effectively maintain exact joins on \\nstreaming data forever.  \\n\\uf0b7 Our method provides full joins, eliminating the need to \\nspecify a similarity threshold, which as we will show, is a \\nnear impossible task in this domain.  \\n\\uf0b7 Our algorithm is embarrassingly parallelizable, both on \\nmulticore processors and in distributed systems. \\nGiven all these features, our algorithm has implications for \\nmany time series data mining tasks [5][18][28].  \\nThe rest of the paper is organized as follows. Section II \\nreviews related work an d introduces the necessary background \\nmaterials and definitions. In Section III we introduce our \\nalgorithm and its anytime and incremental variants. Section IV \\nsees a detailed empirical evaluation of our algorithm and shows \\nits implications for many data mining tasks. Finally, in Section \\nV we offer conclusions and directions for future work. \\nII. RELATED WORK AND BACKGROUND \\nThe basic variant of  similarity join problem we are \\ninterested in is as follows : Given a collection of data objects, \\nretrieve the nearest neighbor for every object.   \\nOther common variants include retrieving the top-K nearest \\nneighbors or the nearest neighbor for each object if that \\nneighbor is within a user-supplied threshold, œÑ. (Such variations \\nare trivial generalizations of our proposed algorithm, so we \\nomit them from further discussion). The latter variant results in \\na much easier problem, provided that the threshold is small. For \\nexample, [1] notes that virtually all research efforts ‚Äú exploit a \\nsimilarity threshold more aggressively in order to limit the set'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='of candidate pairs that are considered..  [or] ... to reduce the \\namount of information indexed in the first place.‚Äù \\nThis critical dependence on œÑ is a major issue for text joins, \\nas it is known that ‚Äú join size can change dramatically \\ndepending on the input similarity threshold ‚Äù [10]. However, \\nthis issue is even more critical for time series for two reasons. \\nFirst, unlike similarity (which is bounded between zero and \\none), the Euclidean distance is effectively unbounded, and \\ngenerally not intuitive. For example, if two heartbeats have a \\nEuclidean distance of 17.1, are they similar? Even for a domain \\nexpert that knows the sampling rate and the noise level of the \\ndata, this is not obvious. Second, a single threshold can produce \\nradically different output sizes, even for datasets that are very \\nsimilar.  Consider Figure 1 which shows the output size vs. \\nthreshold setting for the first and s econd halves of a ten -day \\nperiod monitoring data center chillers [21]. For the first five \\ndays a threshold of 0.6 would return zero items, but for the \\nsecond five days the same setting would return 108 items.  This \\nshows the difficulty in selecting  an appropriate threshold. Our \\nsolution is to have no threshold, and do a full join. \\n \\nFigure 1.  Output size vs. threshold for data center chillers [21]. Values beyond \\n2.0 are truncated for clarity (but archived at [24]).  \\nA handful of efforts have considered joins on time series, \\nachieving speedup by (in addition to the use of MapReduce) \\nconverting the data to lower -dimensional representations such \\nas PAA [11] or SAX [12] and exploiting lower bounds and/o r \\nLocality Sensitive Hashing (LSH) to prune some calculations. \\nHowever, the methods are very complex, with many (10 -plus) \\nparameters to adjust. As [11] acknowledges with admirable \\ncandor, ‚ÄúReasoning about the optimal settings is not trivial.‚Äù In \\ncontrast, our proposed algorithm has zero parameters to set. \\nA very recent research effort [28] has tackled the scalability \\nissue by converting the real -valued time series into discrete \\n‚Äúfingerprints‚Äù before using a LSH approach, much like the text \\nretrieval community [1]. They produced impressive speedup, \\nbut they also experienced false negatives. Moreover, the \\napproach has several parameters that need to be set; for \\nexample, they need to set the threshold to a very precise 0.818.  \\nAs we shall show, our algorithm allows both anytime and \\nincremental (i.e. streaming) versions. While a streaming join \\nalgorithm for text was recently introduced [15], we are not \\naware of any such algorithms for time series data or general \\nmetric spaces. More generally, there is a large amount of \\nliterature on joins for text processing [1]. Such work is \\ninteresting, but of little utility given our constraints, data type \\nand problem setting. We require full joins, not threshold joins, \\nand we are unwilling to allow the possibility of false negatives. \\nA. Definitions and Notation \\nWe begin by defining the data type of interest, time series: \\nDefinition 1:  A time series T is a sequence of real -valued \\nnumbers ti: T = t1, t2, ..., tn where n is the length of T. \\nWe are not interested in the global properties of time series, \\nbut in the similarity between local subsequences:  \\nDefinition 2:  A subsequence Ti,m of a T is a continuous \\nsubset of the values from T of length m starting from \\nposition i.   Ti,m = ti, ti+1,‚Ä¶, ti+m-1, where 1 ‚â§  i ‚â§  n-m+1.  \\nWe can take any subsequence from a time series and \\ncompute its distance to all sequences. We call an ordered vector \\nof such distances a distance profile: \\nDefinition 3:  A distance profile D is a vector of the \\nEuclidean distances between a given query and each \\nsubsequence in an all-subsequences set (see Definition 4).  \\nNote that we are assuming that the distance is measured \\nusing the Euclidean distance between the z -normalized \\nsubsequences [8]. The distance profile can be considered a meta \\ntime series that annotates the time series T that was used to \\ngenerate it. The first three definitions are illustrated in Figure 2. \\n \\nFigure 2. A subsequence Q extracted from a time series T is used as a query to \\nevery subsequence in T. The vector of all distances is a distance profile. \\nNote that if the query and all-subsequences set belong to the \\nsame time series, the distance profile must be zero at the \\nlocation of the query, and close to zero just before and just \\nafter. Such matches are c alled trivial matches in the literature \\n[18], and are avoided by ignoring an exclusion zone (shown as \\na gray region) of m/2 before and after the location of the query. \\nWe are interested in similarity join of all subsequences of a \\ngiven time series. We define an all-subsequences set of a given \\ntime series as a set that contains all possible subsequences from \\nthe time series. The notion of all-subsequences set is purely for \\nnotational purposes. In our implementation, we do not actually \\nextract the subsequences in this form as it would require \\nsignificant time and space overhead. \\nDefinition 4: An all-subsequences set A of a time series T \\nis an ordered set of all possible subsequences of T obtained \\nby sliding a window of length m across T: A ={T1,m,, \\nT2,m,‚Ä¶, Tn-m+1,m}, where m is a user -defined subsequence \\nlength. We use A[i] to denote Ti,m. \\nWe are interested in the nearest neighbor (i.e., 1NN) relation \\nbetween subsequences; therefore, we define a 1NN-join \\nfunction which indicates the nearest neighbor relation between \\nthe two input subsequences. \\nDefinition 5:  1NN-join function :  given two all -\\nsubsequences sets A and B and two subsequences A[i] and \\nB[j], a 1NN-join function  Œ∏1nn (A[i], B[j]) is a Boolean \\nfunction which returns ‚Äútrue‚Äù only if B[j] is the nearest \\nneighbor of A[i] in the set B. \\nWith the defined join function, a similarity join set  can be \\ngenerated by applying the similarity join operator on two input \\nall-subsequences sets. \\nDefinition 6: Similarity join set : given all -subsequences \\nsets A and B, a similarity join set  JAB of A and B is a set \\ncontaining pairs of each subsequence in A with its nearest \\nneighbor in B: JAB={‚å© A[i], B[j] ‚å™ |Œ∏1nn (A[i], B[j])}.  We \\ndenote this formally as JAB = A‚ãà\\uf0711nnB. \\n0\\n400\\n800\\n1200\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\\nFirst 5 Days\\nSecond 5 Days\\nData Center Chillers\\nT, a snippet of an energy \\nconsumption\\n2,0000 m/2m/2\\nQ, query of length m\\nNote that |D| = |T|-|Q|+1D, a distance profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='We measure the Euclidean distance between each pair \\nwithin a similarity join set and store the resultants into an \\nordered vector. We call the result vector the matrix profile. \\nDefinition 7:  A matrix profile (or just profile) PAB is a \\nvector of the Euclidean distances between each pair in JAB. \\nWe call this vector the matrix profile because one \\n(inefficient) way to compute it would be to compute the full \\ndistance matrix of all the subsequences in one time series with \\nall the subsequence in another time series and extract the \\nsmallest value in each row (the smallest non-diagonal value for \\nthe self-join case).  In Figure 3 we show the matrix profile of \\nour running example. \\n \\nFigure 3. A time series T, and its self-join matrix profile P.  \\nLike the distance profile, the matrix profile can be \\nconsidered a meta time series annotating the time series T if the \\nmatrix profile is generated by joining T with itself. The profile \\nhas a host of interesting and exploitable properties. For \\nexample, the highest point on the profile corresponds to the \\ntime series discord [5], th e ( tied) lowest points correspond to \\nthe locations of the best time series motif pair [18], and the \\nvariance can be seen as a measure of the T‚Äôs com plexity. \\nMoreover, the histogram of the values in the matrix profile is \\nthe exact answer to the time series density estimation [4]. \\nWe name this special case of the similarity join set \\n(Definition 6) as self-similarity join set, and the corresponding \\nprofile as self-similarity join profile. \\nDefinition 8:  A self-similarity join  set JAA is a result of \\nsimilarity join of the set  A with itself . We denote this \\nformally as JAA = A ‚ãà\\uf0711nn A. We denote the corresponding \\nmatrix profile or self-similarity join profile as PAA. \\nNote that we exclude trivial matches when self -similarity \\njoin is performed, i.e., if A[i] and A[j] are subsequences from \\nthe same all -subsequences set A, Œ∏1nn (A[i], B[j]) is ‚Äúfalse‚Äù \\nwhen A[i] and A[j] are a trivially matched pair. \\nThe ith element in the matrix profile tells us the Euclidean \\ndistance to the nearest neighbor of the subsequence of T, \\nstarting at i.  However, it does not tell us where that neighbor is \\nlocated. This information is recorded in matrix profile index. \\nDefinition 9: A matrix profile index IAB of a similarity join \\nset JAB is a vector of integers where IAB[i] = j if {A[i], B[j]} \\n‚àà JAB. \\nBy storing the neighboring information this way, we can \\nefficiently retrieve the nearest neighbor of A[i] by accessing the \\nith element in the matrix profile index. \\nNote that the function which computes the similarity join set \\nof two input time series  is not symmetric; therefore, JAB ‚â† JBA,  \\nPAB ‚â† PBA, and IAB ‚â† IBA. \\nFor ease of presentation, we have confined this work to the \\nsingle dimensional case; however, nothing intrinsically \\nprecludes generalizations to multidimensional data. \\nSummary of the Previous Section \\nThe previous section was rather dense, so before moving on \\nwe summarize the main takeaway points. We can create two \\nmeta time series, the matrix profile and the matrix profile index, \\nto annotate a time series TA with the distance and location of all \\nits subsequences nearest neighbors in itself or another time \\nseries TB. These two data objects explicitly or implicitly contain \\nthe answers to many time series data mining tasks. However, \\nthey appear to be too expensive to compute to be practica l. In \\nthe next section we will show an algorithm that can compute \\nthese efficiently. \\nIII. ALGORITHMS \\nWe are finally in a position to explain our algorithm s. We \\nbegin by stating the fundamental intuition, which stems from \\nthe relationship between distance profiles and the matrix \\nprofile. As Figure 2 and Figure 3 visually suggest, all distance \\nprofiles (excluding the trivial match region) are upper bound \\napproximations to the matrix profile. More critically, if we \\ncompute all the distance profiles, an d take the minimum value \\nat each location, the result is the matrix profile! \\nThis tells us that if we have a fast way to compute the \\ndistance profiles, then we also have a fast way to compute the \\nmatrix profile. As we shall show in the next section, we have an \\nultra-fast way to compute the distance profiles. \\nA. Mueen‚Äôs ultra-fast Algorithm for Similarity Search (MASS) \\nWe begin by introducing a novel Euclidean distance \\nsimilarity search algorithm for time series data. The algorithm \\ndoes not just find the nearest neighbor to a query and return its \\ndistance; it returns the distance to every subsequence. In \\nparticular, it computes the distance profile, as shown in Figure \\n2. The algorithm requires just O( nlogn) time by exploiting the \\nFFT to calculate  the dot products between the query and all \\nsubsequences of the time series.  \\nWe need to carefully qualify the claim of ‚Äúultra-fast‚Äù. There \\nare dozens of algorithms for time series similarity search that \\nutilize index structures to efficiently locate neighbors [8]. While \\nsuch algorithms can be faster in the  best case , all these \\nalgorithms degenerate to brute force search in the worst case 1 \\n(actually, much worse than brute force search due to the \\noverhead of the index). Likewise, there are index -free methods \\nthat achieve speed -up using various early abandoning tricks \\n[22], but they too degrade to brute force search in the worst \\ncase. In contrast, the performance of the algorithms outlined in \\nTABLE I and TABLE II is completely independent of the data. \\nTABLE I. CALCULATION OF SLIDING DOT PRODUCTS \\nProcedure SlidingDotProduct(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: The dot product between Q and all subsequences in T \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nn ‚Üê Length(T), m ‚Üê Length(Q) \\nTa ‚Üê Append T with n zeros   \\nQr ‚Üê Reverse(Q)  \\nQra ‚Üê Append Qr with 2n-m zeros \\nQraf ‚Üê FFT(Qra), Taf ‚Üê FFT(Ta) \\nQT ‚Üê InverseFFT(ElementwiseMultiplication(Qraf, Taf)) \\nreturn QT \\n \\n1 There are many such worse case scenarios, including high levels of noise blurring \\nthe distinction between closest and furthest neighbors, and rendering triangular -\\ninequality pruning and early abandoning worthless. \\n2,0000\\nP, a matrix profile\\nT, a snippet of an energy \\nconsumption\\nNote that |P| = |T|-|Q|+1'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Line 1 determines the length of both the time series T and \\nthe query Q. In line 2, we use that information to append T with \\nan equal number of zeros. In line 3, we obtain the mirror image \\nof the original query. This reversing ensures that a convolution \\n(i.e. ‚Äúcrisscrossed‚Äù multiplication) essentially produces in-order \\nalignment. Because we require both vectors to be the s ame \\nlength, in line 4 we append enough zeros to the (now reversed) \\nquery so that, like Ta, it is also of length 2 n. In line 5, the \\nalgorithm calculates Fourier transforms of the appended -\\nreversed query (Qra) and the appended time series Ta. Note that \\nwe use FFT algorithm which is an O(nlogn) algorithm. The Qraf \\nand the Taf produced in line 5 are vectors of complex numbers \\nrepresenting frequency components of the two time series. The \\nalgorithm calculates the element-wise multiplication of the two \\ncomplex vectors and performs inverse FFT on the product. \\nLines 5-6 are the class ic convolution operation on two vectors \\n[7]. Figure 4 shows a toy example of the sliding dot product \\nfunction in work. The algorithm time complexity does not \\ndepend on the length of the query (m). \\n \\nFigure 4. A toy example of convolution operation being used to calculate  \\nsliding dot products for time series data. Note the reverse and append \\noperation on T and q in the input. Fifteen dot products are calculated for every \\nslide. The cells m = 2 to n = 4 from left ( red/bold arrows) contain valid \\nproducts. TABLE II takes this subroutine and uses it to create a distance \\nprofile (see Definition 3). \\nIn line 1 of TABLE II, we invoke the dot products code \\noutlined in TABLE I. The formula to calculate the z-normalized \\nEuclidean distance D[i] between two time series subsequence Q \\nand Ti,m using their dot product, QT[i] is (see [24] for \\nderivation): \\nùê∑[ùëñ] = ‚àö2ùëö(1‚àíùëÑùëá[ùëñ]‚àíùëöùúáùëÑùëÄùëá[ùëñ]\\nùëöùúéùëÑùõ¥ùëá[ùëñ] ) \\nwhere m is the subsequence length, ŒºQ is the mean of Q, \\nMT[i] is the mean of Ti,m, œÉQ is the standard deviation of Q, and  \\nŒ£T[i] is the standard deviation of Ti,m. Normally, it takes O( m) \\ntime to calculate the mean and standard deviation for every \\nsubsequence of a long time series. However, here we exploit a \\ntechnique noted in [22] in a different context. We cache \\ncumulative sums of the values and square of the values in the \\ntime series. At any stage the two cumulative sum vectors are \\nsufficient to calculate the mean an d the standard deviation of \\nany subsequence of any length.  See [14] for an elaborate \\ndescription and variations of MASS. \\nTABLE II. MUEEN‚ÄôS ALGORITHM FOR SIMILARITY SEARCH (MASS) \\nProcedure MASS(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: A distance profile D of the query Q  \\n1 \\n2 \\n3 \\n4 \\nQT ‚Üê SlidingDotProducts(Q, T) \\nŒºQ, œÉQ, ŒúT, Œ£T  ‚Üê ComputeMeanStd(Q, T)    // see [22] \\nD ‚Üê CalculateDistanceProfile(Q, T, QT, ŒºQ, œÉQ, ŒúT, Œ£T) \\nreturn D \\nUnlike the dozens of time series KNN search algorithms in \\nthe literature [8], this algorithm calculates the distance to every \\nsubsequence, i.e. the distance profile  of time series T. \\nAlternatively, in join nomenclature, the algorithm produces one \\nfull row of the all -pair similarity matrix. Thus, as we show in \\nthe next section, our join algorithm is simply a loop that \\ncomputes each full row of the all -pair similarity matrix and \\nupdates the current ‚Äúbest-so-far‚Äù matrix profile when needed. \\nB. The STAMP Algorithm \\nWe call our join algorithm STAMP, Scalable Time series \\nAnytime Matrix Profile. The algorithm is outlined in TABLE \\nIII. In line 1, we extract the length of TB. In line 2, we allocate \\nmemory and initial matrix profile PAB and matrix profile index \\nIAB. From lines 3 to line 6, we calculate the distance profiles D \\nusing each subsequence B[idx] in the time series TB and the \\ntime series TA. Then, we perform pairwise minimum for each \\nelement in D with the paired element in PAB (i.e., min( D[i], \\nPAB[i]) for i = 0 to length(D) - 1.) We also update IAB[i] with idx \\nwhen D[i] ‚â§ PAB[i] as we perform the  pairwise minimum \\noperation. Finally, we return the result PAB and IAB in line 7.  \\nNote that the algorithm presented in TABLE III computes \\nthe matrix profile for the general similarity join. To modify the \\ncurrent algorithm to compute  the self -similarity join matrix \\nprofile of a time series TA, we simply replace TB in line 1 with \\nTA, replace B with A in line 4, and ignore trivial match in D \\nwhen performing ElementWiseMin in line 5. \\nTABLE III. THE STAMP ALGORITHM \\nProcedure STAMP(TA, TB, m) \\nInput: Two user provided time series, TA and TB, interested \\nsubsequence length m \\nOutput: A matrix profile PAB and associated matrix profile index IAB of \\nTA join TB, JAB = A‚ãà\\uf0711nnB \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nnB ‚Üê Length(TB) \\nPAB ‚Üê infs, IAB ‚Üê zeros, idxes ‚Üê 1:nB-m+1 \\nfor each idx in idxes    // In any order, but random for anytime algorithm \\n          D ‚Üê MASS(B[idx], TA) \\n          PAB, IAB ‚Üê ElementWiseMin(PAB, IAB, D, idx) \\nend for \\nreturn PAB, IAB \\nTo parallelize the STAMP algorithm for multicore \\nmachines, we simply distribute the indexes to secondary \\nprocess run in each core, and the secondary processes use the \\nindexes they received to update their own PAB and IAB. Once the \\nmain process returns from  all secondary processes, we use \\nElementWiseMin to merge the received PAB and IAB.  \\nC. An Anytime Algorithm for TSAPSS \\nWhile the exact algorithm introduced in the previous section \\nis extremely scalable, there will always be datasets for which \\ntime needed for an exact solution is untenable. We can mitigate \\nthis by computing the results in an anytime fashion, allowing \\nfast approximate solutions [30]. To a dd the anytime nature to \\nthe STAMP algorithm, we simply ensure a randomized search \\norder in line 2 of TABLE III.  \\nWe can compute a (post-hoc) measurement of the quality of \\nan anytime solution by measuring the Root -Mean-Squared-\\nError (RMSE) between the true matrix profile and the current \\nbest-so-far mat rix profile. As Figure 5 suggests, with an \\nexperiment on random walk data, the algorithm converges very \\nquickly. \\nQ2T1 0 00 00 00 00 0Q1T4Q2T2+Q1T1 Q2T3+Q1T2 Q2T4+Q1T3\\nOutput\\nInputT2T1 T4T3 00 00 Q1Q2 00 00 00'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Figure 5. main) The decre ase in RMSE as the STAMP algorithm updates \\nmatrix profile with the distance profile calculated at each iteration.  inset) The \\napproximate matrix profile at the 10% mark is visually indistinguishable from \\nthe final matrix profile. \\nZilberstein [30] gives a number of desirable properties of \\nanytime algorithms, including Low Overhead , Interruptibility, \\nMonotonicity, Recognizable Quality, Diminishing Returns and \\nPreemptability (these properties are mostly obvious from their \\nnames, but full definitions are at [30]). \\nBecause each subsequence‚Äôs distance profile is bounded \\nbelow by the exact matrix profile, updating an approximate \\nmatrix profile with a distance profile with pairwise minimum \\noperation either drives the approximate solution closer the exact \\nsolution or retains the current approximate solution. Thus , we \\nhave guaranteed Monotonicity. From Figure 5, the approximate \\nmatrix profile converges to the exact matrix profile \\nsuperlinearly; therefore, we have strong Diminishing Returns. \\nWe can easily achieve Interruptibility and Preemptability by \\nsimply inserting a few lines of code between lines 5 and 6 of \\nTABLE III that read: \\n5new \\n6new \\n7new \\nif CheckForUserInterrupt  = TRUE \\n          Report({PAB, IAB}, ‚ÄòHere is an approximate answer.‚Äô) \\nif GetUserChoice = ‚Äòfurther refine‚Äô, CONTINUE, else BREAK \\nThe space and time overhead for the anytime property is \\neffectively zero, thus we have Low Overhead. This leaves only \\nthe property of Recognizable Quality. Here we must resort to a \\nprobabilistic argument.  The convergence curve shown in  \\nFigure 5 is very typical, so we could use past convergence \\ncurves to predict the quality of solution when interrupted on \\nsimilar data.  \\nD. Time and Space Complexity \\nThe overall complexity of the proposed algorithm is \\nO(n2logn) where n is the length of the time series. However, our \\nexperiments (see Section 4.1) empirically suggest that the \\nruntime of  STAMP‚Äôs growth rate is roughly O( n2) instead of \\nO(n2logn). One possible e xplanation for this is that the nlogn \\nfactor comes from the FFT subroutine. Because FFT is so \\nimportant in many applications, it is extraordinarily well \\noptimized. Thus, the empirical runtime is very close to linear. \\nIn contrast to the above, the brute for ce nested loop \\napproach has a time complexity of O(n2m). Recall the industrial \\nexample in the introduction section. We have  m = 10,080, but \\nlog(n) = 5.7, so we would expect our approach to be about \\n1,768 times faster. In fact, we are empirically even faster. The \\ncomplexity analysis downplays the details of important constant \\nfactors. The nested loop algorithm must also z -normalize the \\nsubsequences. This either requires O( nm) time, but with an \\nuntenable O(nm) space overhead, or an O( n2m) time overhead. \\nAnd r ecall that this is before a single Euclidean distance \\ncalculation is performed.  \\nFinally, we mention one quirk of our algorithm which we \\ninherit from using the highly optimized FFT subroutine. Our \\nalgorithm is fastest when n is an integer power of two, slower \\nfor non -power of two but composite numbers, and slowest \\nwhen n is prime. The difference (for otherwise similar values of \\nn) can approach a factor of 1.6x. Thus, where possible, it is \\nworth contriving the best case by tru ncation or zero-padding to \\nthe nearest power of two. \\nE. Incrementally Maintaining TSAPSS \\nUp to this point we have discussed the batch version of \\nTSAPSS. By batch, we mean that the STAMP algorithm needs \\nto see the entire time series TA and TB (or just TA if we are \\ncalculating the self -similarity join matrix profile) before \\ncreating the matrix profile. However, it would be advantageous \\nif we could build the matrix profile incrementally. Given that \\nwe have performed a batch construction of matrix profile, i f \\nnew data arrives, it would clearly be preferable to incrementally \\nadjust the current profile, rather than start from scratch.  \\nBecause the matrix profile solves both the times series motif \\nand the time series discord problems, an incremental version of \\nSTAMP would automatically provide the first incremental \\nversions of both algorithms. We call such an algorithm the \\nSTAMPI (STAMP Incremental) algorithm. \\nWe will demonstrate our ability to incrementally maintain \\nthe matrix profile in this section. For simpli city and brevity \\nTABLE IV only shows the algorithm to maintain the self -\\nsimilarity join. The generalizations are obvious. \\nTABLE IV. THE STAMPI ALGORITHM \\nProcedure STAMPI(TA, t, PAA, IAA) \\nInput: The original time series TA, a new data point t following TA, the \\nmatrix profile PAA and its associated matrix profile index IAA of TA.  \\nOutput: The incrementally updated matrix profile PAA,new and its matrix \\nprofile index IAA,new of the current time series TA,new= TA, t. \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nTA,new = [TA, t] \\nS ‚Üê last subsequence in TA,new, idx ‚Üê index of S in TA,new \\nD ‚Üê MASS (S, TA) \\nPAA, IAA ‚Üê ElementWiseMin(PAA, IAA, D, idx) \\npAA,last, iAA,last ‚Üê FindMin(D) \\nPAA,new ‚Üê [PAA, pAA,last], IAA,new ‚Üê [IAA, iAA,last] \\nreturn PAA,new, IAA,new \\nFor clarity, we denote the updated time series as TA,new, the \\nupdated matrix profile as PAA,new and the associated matrix \\nprofile index as IAA,new. As each additional data point t arrives, \\nthe size of the time series TA increases by one, and a new \\nsubsequence S is generated at the end of TA,new. In line 3 we \\nobtain the distance profile of S with regard to TA. Then, as in the \\noriginal STAMP algorithm,  in line 4 we perform  a pairwise \\ncomparison between every element in D with the corresponding \\nelement in PAA to see if the corresponding element in PAA needs \\nto be updated. In line 5, we find the nearest neighbor of S and \\nthe associated index by evaluating the minimum value of D. \\nFinally, in line 6, we obtain the new matrix profile and \\nassociated matrix profile index by concatenating the results in \\nline 4 and line 5.  \\nThe time complexity of the STAMP I algorithm is O(nlogn) \\nwhere n is the length of size of th e current time series TA. Note \\nthat as we maintain the profile, each incremental call of \\nSTAMPI requires invoking the FFT subroutine with a slightly \\nlonger time series ( n becomes n+1 ). Thus it gets very slightly \\nslower at each time step. Therefore, the best way to measure the \\nperformance is to ask for the Maximum Time Horizon (MTH), \\nin essence the answer to this question: ‚Äú Given this arrival rate, \\nhow long can we maintain the profile before we can no longer \\nupdate fast enough?‚Äù \\nNote that the subsequence length m is not considered in the \\nMTH evaluation. Recall that overall time complexity of the \\n10,0000\\nRMSE\\niteration1,000\\napproximate matrix \\nprofile at 1,000 iterations.\\nexact matrix profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='algorithm is determined by the efficiency of the FFT \\nsubroutine, which is independent of m. We have computed the \\nMTH for two common scenarios of interest to the community. \\n\\uf0b7 House Electrical Demand  [19]: This dataset is updated \\nevery eight seconds. By iteratively calling the STAMP I \\nalgorithm, we can maintain the profile for 5.8 years.  \\n\\uf0b7 Oil Refinery :  Most telemetry in oil refineries is sampled \\nat once a minute [26]. The relatively low sampling rate \\nreflects the ‚Äúinertia‚Äù of massive boilers/condensers. Even if \\nwe maintain the profile for 40 years, the update time is only \\naround 6.78 seconds. Moreover, th e raw data, matrix \\nprofile and index would only require 0.5 gigabytes of main \\nmemory. Thus the MTH here is forty plus years. Given \\nprojected improvements in hardware, this effectively \\nmeans we can maintain the matrix profile forever. \\nAs impressive as these  numbers are, they are actually quite \\npessimistic. For simplicity we assume that every value in the \\nmatrix profile index will be updated at each time step.  \\nHowever, empirically, much less than 0.1% of them need to be \\nupdated. If it is possible to prove an upper bound on the number \\nof changes to the matrix profile index per update, then we could \\ngreatly extend the MTH  or handle much faster sampling rates. \\nWe leave such considerations for future work. \\nIV. EXPERIMENTAL EVALUATION \\nWe begin by stating our experimen tal philosophy. We have \\ndesigned all experiments such that they are easily reproducible. \\nTo this end, we have built a webpage [24] which contains al l \\ndatasets and code used in this work, together with spreadsheets \\nwhich contain the raw numbers and some supporting videos.  \\nGiven page limits, and the utility of our algorithm for a host \\nof existing (and several new) data mining tasks, we have chosen \\nto c onduct exceptionally broad  but shallow experiments. We \\nhave conducted such deep detailed experiments and placed \\nthem at [24]. Unless otherwise state d we measure wall clock \\ntime on an Intel i7@4GHz with 4 cores. \\nA. Scalability of Profile-Based Self-Join \\nBecause the time performance of STAMP is independent of \\nthe data quality or any user inputs (there are none except the \\nchoice of m, which does not affect the speed), our scalability \\nexperiments are unusually brief.  In TABLE V we show the \\ntime required for a self -join with m fixed to 256, for \\nincreasingly long time series.  \\nTABLE V.  TIME REQUIRED FOR A SELF-JOIN WITH M = 256, VARYING N \\nValue of n 217 218 219 220 221 \\nTime Required 15.1 min 70.4 min 5.4 hours 24.4 hours 4.2 days \\nIn TABLE VI, we show the time required for a self -join \\nwith n fixed to 2 17, for increasing long m. Again recall that \\nunlike virtually all other time series data mining algorithms in \\nthe literature whose performance degrades for longer \\nsubsequences [8][18], the running time of STAMP does not \\ndepend on m. \\nTABLE VI. TIME REQUIRED FOR A SELF-JOIN WITH N= 217, VARYING M \\nValue of m 64 128 256 512 1,024 \\nTime Required 15.1 min 15.1 min 15.1 min 15.0 min 14.5 min \\nFinally, we further exploit the simple parallelizability of the \\nalgorithm by using four 16 -core virtual machines on Microsoft \\nAzure to redo the two -million join ( n = 2 21 and m = 256) \\nexperiment. By scaling up the computational power, we have \\nreduced the running time from 4.2 days to just 14.1 hours. This \\nuse of cloud computing required writing just few dozen lines of \\nsimple additional code [24]. \\nIt is difficult to find good baselines to compare to. We \\nbelieve STAMP is the only algorithm that does full, exact joins \\non time series subsequences. Most algorithms do only threshold \\njoins and/or do approximate joins, and are not specialized for \\ntime series subsequences. However, we searched for the best \\nbaselines and made the following concessions to  the rival \\nalgorithms to allow comparisons. \\n\\uf0b7 TSFRDAA [11]: While STAMP returns all nearest \\nneighbors, we adjust the threshold (the selectively) of \\nTSFRDAA such that only needs to return the top 1% nearest \\nneighbors, a much easier task. \\n\\uf0b7 HDSJI-SAX [12]: This method  allows false negatives ; we \\nignore this. It needs time to build indexes ; we do not count \\nthis time, and as above, we allow it to return  the only the \\ntop 1% nearest neighbors ( as bef ore, not the 100% the \\nSTAMP returns, and therefore a much easier task.). \\n\\uf0b7 Optimized Nested Loop (ONL): Here we use a nested -loop \\njoin optimized in the following manner. We use a state -of-\\nthe-art DFT indexing technique to do the search in the \\ninner loop, and we do not count the time needed to build \\nthe indexes [8]. We carefully adjust  parameters for best \\nperformance.  \\nWe consider the first 2 18 data points of the ECG dataset \\nused in [22], with a query length of 256, about one heartbeat ; \\nTABLE VII shows the results.  \\nTABLE VII. TIME FOR A SELF-JOIN WITH M = 256, VARYING ALGORITHMS \\nAlgorithm TSFRDAA HDSJI-SAX ONL STAMP \\nTime Required 51.7 hours 19.6 min 28.1 hours 1.17 hours \\nAs these results show, even if we ignore the limitations of \\nthe baseline methods, STAMP is still significantly faster. \\nB. Profile-Based Self-Join \\nA recent paper notes that many fundamental problems in \\nseismology can be solved by joining seismometer telemetry \\n[28], including the disc overy of foreshocks, aftershocks, \\ntriggered earthquakes, volcanic activity and induced seismicity. \\nHowever, the paper notes a join with a query length of 200 on a \\ndata stream of length 604,781 requires 9.5 days. Their solution, \\na clever transformation of t he data to allow LSH based \\ntechniques, does achieve significant speedup, but at the cost of \\nfalse negatives and the need for significant parameter tuning.  \\nThe authors kindly shared their data, and, as we hint at in \\nFigure 6, confirmed that STAMP does not have false negatives. \\n \\nFigure 6. top) An excerpt of a seismic time series aligned with its matrix \\nprofile (bottom). The ground truth provided by the authors of [28] requires that \\nthe events occurring at time 4,050 and 7,800 match. \\nWe repeated the n = 604,781, m = 200 experime nt and \\nfound it took just 8.9 hours to finish. As impressive as this is, in \\nthe next section we show that we can do even better.   \\n4,000 5,000 6,000 7,000 8,000 9,000\\n0\\n5\\n10\\n15\\nSeismic time series (excerpt) \\nMatrix Profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='C. The Utility of Anytime STAMP \\nThe seismology dataset offers an excellent opportunity to \\ndemonstrate the utility of the anytime version of our algorithm. \\nThe authors of [28] revealed in their long -term ambition of \\nmining even larger datasets [3]. In Figure 7 we repeated the \\nexperiment with the snippet shown in Figure 6, this time \\nreporting the best-so-far matrix profile reported by the \\nalgorithm at various milestones. Even with just 0.25% of the \\ndistances computed (that is to say, 400 times faster) the correct \\nanswer has emerged. Thus, we can provide the correct answers \\nto the seismologists in just minutes, rather than the 9.5 days.  \\n \\nFigure 7. top) An excerpt of the seismic data that is also shown in Figure 6. \\ntop-to-bottom) The approximations of the matrix profile for increasing \\ninterrupt times. By the time we have computed just 0.25% of the calculations \\nrequired, the minimum of the matrix profile points to the ground truth. \\nTo show the generality of this anytime feature of STAMP, \\nwe consider a very different dataset. As shown in Figure \\n8.inset), it is possible to convert DNA to a time ser ies [22]. We \\nconverted the Y-chromosome of the Chimpanzee this way. The \\nresulting time series is  little over one -million in length. We \\nperformed a self-join with m = 60,000.  Figure 8.bottom shows \\nthe best motif is so well conserved (ignoring the first 20%), that \\nit must correspond to a recent (in evolutionary tim e) gene \\nduplication event. In fact, in a subsequent analysis we \\ndiscovered that ‚Äúmuch of the Y (Chimp chromosome) consists of \\nlengthy, highly similar repeat units, or ‚Äòamplicons‚Äô‚Äù [9]. \\n \\nFigure 8. top) The Y -chromosome of the Chimp in time series space with its \\nmatrix profile. bottom) A zoom -in of the top motif discovered using anytime \\nSTAMP, we believe it to be an amplicon [9]. \\nThis demanding join would take just over a day of CPU \\ntime (see TABLE V). However, using anytime STAMP we \\nhave the result shown above after doing just 0.021% of the \\ncomputations, in about 18 seconds. At [24] we have videos that \\nshow the rapid convergence of the anytime variant of STAMP. \\nD. Profile-Based Similarity Join Set \\nIn this section we show two uses of similarity join set. The \\nfirst use is more familiar to APSS users, quantifying what is \\nsimilar between two time series. The second example, \\nquantifying what is different between two time series, is novel  \\nand can only be supported by threshold -free algorithms that \\nreport the nearest neighbor for all objects. \\nTime Series Set Similarity \\nGiven two (or more) time series collected under different \\nconditions or treatments, a data analyst may wish to know what \\npatterns (if any) are conserved between the two time series. To \\ndemonstrate the utility of automating this, we consider a simple \\nbut in tuitive example. Figure 9 shows the raw audio of two \\npopular songs converted to Mel Frequency Cepstral \\nCoefficients (MFCCs). Specifically, the songs used in this \\nexample are ‚ÄúUnder Pressure‚Äù by Queen an d David Bowie and \\n‚ÄúIce Ice Baby ‚Äù by the American rapper Vanilla Ice. Normally, \\nthere are 13 MFCCs; here, we consider just one for simplicity. \\n \\n \\nFigure 9. Two songs represented by just the 2 nd MFCC at 100Hz. We \\nrecognize that it is difficult to see any structure in these times series; however, \\nthis difficulty is the motivation for this experiment. \\nEven for these two relatively short time series, visual \\ninspection does not offer immediate answers. The problem here \\nis compounded by the size reproduction, but is not significantly \\neasier with large-scale [24] or interactive graphic tools. We ran \\nJAB (Queen-Bowie, Vanilla Ice) on these datasets with m = 500 \\n(five seconds), the best match, corresponding the minimum \\nvalue of the matrix profile PAB, is shown in Figure 10. \\n \\nFigure 10. The result of JAB (Queen-Bowie (red/bold), Vanilla-Ice (green/fine)) \\nproduces a strongly conserved five second region. \\nReaders may know the cause for this highly conserved \\nsubsequence. It corresponds to the famous baseline of ‚ÄúUnder \\nPressure,‚Äù which was sampled (plagiarized) by Vanilla Ice in \\nhis song. The join took 21.9 seconds. The ability to find \\nconserved structure in apparently disparate time series could \\nopen many avenues of research in medicine and industry. \\nTime Series Difference \\nWe introduce the Time Series Diff (TSD) operator, which \\ninformally asks ‚ÄúWhat happens in time series T A, that does not \\nhappen in time series TB?‚Äù Here TA/TB could be an ECG before \\na drug is administered/after a drug is administered, telemetry \\nbefore a successful launch/before a catastrophic launch etc. The \\nTSD is simply the subsequence referred to by the maximum \\nvalue of the JAB join‚Äôs profile PAB. \\nWe begin with a simple intuitive example. The UK and US \\nversions of the Harry Potter audiobook series are performed by \\ndifferent narrators, and have a handful of differences in the text. \\nFor example, the UK version contains: \\nHarry was passionate about Quidditch. He had played as Seeker on the Gryffindor \\nhouse Quidditch team ever since his first year at Hogwarts and owned a Firebolt, one \\nof the best racing brooms in the world... \\nBut the corresponding USA version has: \\nHarry had been on the Gryffindor House Quidditch team ever since his first year at \\nHogwarts and owned one of the best racing brooms in the world, a Firebolt. \\nAs shown in Figure 11, we can convert the audio \\ncorresponding to these snippets into MFCCs and invoke a JAB \\njoin set to produce a matrix profile PAB that represents the \\ndifferences between them. As Figure 11.left shows, the low \\nvalues of this profile correspond to identical spoken phrases \\n(despite having two different narrators). However here we are \\n4,000 5,000 6,000 7,000 8,000 9,000\\ndata\\n0.25%\\n1%\\n100%\\n0 1,000,0000\\n100\\n200\\nPan troglodytes Y-chromosome \\n0 60,000\\n12,749,475 to 14,249,474 bp\\n622,725 to 2,122,724 bp\\nT1 = 0;\\nfor i = 1 to length(chromosome) \\nif chromosomei = A, then Ti+1 = Ti + 2 \\nif chromosomei = G, then Ti+1 = Ti + 1\\nif chromosomei = C, then Ti+1 = Ti - 1 \\nif chromosomei = T, then Ti+1 = Ti - 2 \\nend\\n0\\nQueen-Bowie\\n1,000 2,000\\n-10\\n0\\n10\\nVanilla Ice\\n0 250 500-3\\n-2\\n-1\\n0\\n1\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='interested in the differences, the maximum value of the profile. \\nAs we can see in Figure 11.right, here the profile corresponds \\nto a phrase unique to the USA edition.  \\n \\nFigure 11. The 2 nd MFCC of snippets from the USA ( pink/bold) and UK \\n(green/fine) Harry Potter audiobooks.  The JAB join of the two longer sections \\nin the main text produce s mostly small values in the profile correspond to the \\nsame phrase (left), the largest value in  the profile corresponds to a phrase \\nunique to the USA edition (right). \\nThe time required to do this is just 0.067 seconds, much \\nfaster than real time. While this demonstration is trivial, in [24] \\nwe show an example applied to ECG telemetry.  \\nE. Profile-Based Motif Discovery \\nSince their introduction in 2003, time series motifs have \\nbecome one of the most frequently used primitives in time  \\nseries data mining, with applications in dozens of domains [2]. \\nThere are several proposed definitions for time series motifs, \\nbut in [18] it is argued that if you can solve the most basic \\nvariant, the closest (non -trivial) pair of subsequences, then all \\nother variants only require some minor additional calculations. \\nNote that the locations of the two (tying) minimum values of \\nthe matrix profile are exactly the locations of the 1st motif pair. \\nThe fastest known exact algorithm for computing time \\nseries motifs is the MK algorithm [18]. Note, however, that this \\nalgorithm‚Äôs time performance depends on the time series itself. \\nIn contrast, the Profile -Based Motif Discovery (PBMD) takes \\ntime independent of the data. To see this, we compared the two \\napproaches on an electrocardiogram of length 65,536. In Figure \\n12.left we ask what happens as we search for lon ger and longer \\nmotifs. In Figure 12.right we ask what happens if the motif \\nlength is fixed to m = 512, but the data becomes increasing \\nnoisy.  \\n \\nFigure 12. The time required to find the top -motif pairs in a time series of \\nlength 216 for increasingly long motif lengths ( left), and for a length fixed to \\n512, but in the face of increasing noise levels (right). \\nThese results show that  even in the best case for MK, \\nPBMD is competitive, but as we have longer queries and/or \\nnoisier data, its advantage becomes unassailable. Moreover, \\nPBMD inherits STAMP‚Äôs anytime and incremental \\ncomputability, and is easily parallelizable.  \\nF. Profile-Based Discord Discovery \\nA time series discord  is the subsequence that has the \\nmaximum distance to its nearest neighbor. While this is a \\nsimple definition, time series discords are known to be very \\ncompetitive as novelty/anomaly detectors  [5]. Note that as \\nshown in Figure 13, the time series discord is encoded as the \\nmaximum value in a matrix profile. \\n \\nFigure 13. top) An excerpt from an ECG incorporating a premature ventricular \\ncontraction (red/bold). bottom) The time series profile peaks exactly at the \\nbeginning of the PVC. \\nThe time taken to compute the discord  is obviously just the \\ntime needed to compute the matrix profile ( here, 0.9 seconds). \\nThere are a few dozen discord discovery algorithms in the \\nliterature. Some of them may be competitive in the best case, \\nbut just like motif -discovery algorithms they all degenerate to \\nbrute force search in the worst case, and none allow the anytime \\nproperties that we inherit from using STAMP.  \\nG. Incrementally Maintaining Motifs and Discords \\nWe have demonstrated the ability to detect time series \\nmotifs and discords using the matrix profile in the previous two \\nsections. However, we assumed that the entire time series was \\navailable beforehand. Here we remove this assumption and \\nshow how STAMP I allows us to incrementa lly maintain time \\nseries motifs/ discords in an online fashion. There are other \\nattempts at one [20][2] or both [25] of these tasks, but they are \\nall approximate and allow false dismissals.   \\nIn Section III.E, we introduced the STAMPI algorithm. The \\nability to incrementally maintain the matrix profile implies the \\nability to exactly maintain the time series motif [18] and/or time \\nseries discord [5] in streaming data. We simply need to keep \\ntrack of the extreme values of the incrementally-growing matrix \\nprofile, report a new pair of motifs when a new minimum value \\nis detected, and report a new discord when we see a new \\nmaximum value. \\nWe demonstrate the utility of these ideas on the AMPds \\ndataset [13]. Here the kitchen fridge and the heat pump are both \\nplugged into a single metered power supply. For the first week, \\nonly the refrigerator is running. At the end of the week, the \\nweather gets cold and the heat pump is turned on. The sampling \\nrate is one sample/m inute, and th e subsequence length is 100. \\nWe apply the STAMP algorithm to the first three days of data, \\nthen invoke STAMP I to handle newly arriving data , report an \\nevent when we detect a new extreme value. \\nOur first event occurs at the 9,864 th minute (6 da y 20 hour \\n24 minute). As shown in Figure 14, a new minimum value is \\ndetected, which indicates a new time series motif. The just -\\narrived 100 -minute-long pattern looks v ery similar to another \\npattern that occurred five hours earlier. While there is a lot of \\nregularity in the fridge data in general, the exceptional \\nsimilarity observed here suggested some underlying physical \\nmechanism that caused such a perfectly -conserved pattern, \\nperhaps a mechanical ice-making ‚Äúsubroutine.‚Äù \\n \\nFigure 14. top) The matrix profile of the first 9,864 minutes of data. bottom) \\nThe minimum value of the matrix profile corresponds to a pair of time series \\nmotifs in the power usage data. right) The time series motif detected. \\nOur second event occurs at the 10,473 th minute (7 day 6 \\nhour 33 minute). As shown in Figure 15, a new maximum value \\n‚Ä¶indor house Quidditch team ever since his first ye‚Ä¶\\nHarry had been on the Gryffindor House Quidditch te..\\nsince his first year at Hogwarts and owned a Fire..\\nsince his first year at Hogwarts and owned on..\\nED = 2.9 \\nClosest Match \\n0 100(1.6 seconds) 0 100\\nED = 10.4\\n(1.6 seconds)\\nFurthest Match (Time Series Difference) \\n256 512 1,024 2,048 4,096\\n0\\n40\\n80\\n120\\n160\\n200\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nSubsequence Length  (m)\\nTime Taken (min)\\n80 40 0\\n0\\n70\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nNoise Added (dB)  \\n0 1000 2000 3000\\nECG qtdb\\nSel102\\n(excerpt)\\nPremature Ventricular \\nContraction\\nTime Series Profile\\n0 5,000 10,000\\n0 100\\nnew minimum value\\nnew motifMatrix Profile\\nPower Usage Data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='is detected, which indicates a new time series discord. The time \\nseries discord corresponds to the first occurrence of a heat \\npump pattern in the power usage data.  \\n \\nFigure 15. top) The matrix profile for the first 10,473 minutes. bottom) The \\nmaximum value of the matrix profile corresponds to a time series discord. \\nright) The time series discord detected is the first  heat pump pattern \\noccurrence in the dataset. \\nThe maximum time needed to process a single data point \\nwith STAMP I in this dataset is 0.005 second s, which is less \\nthan 0.01% of the data sampling rate. Thus, on this dataset we \\ncould continue monitoring with the STAMP I algorithm for \\nseveral decades before running out of time or memory. \\nH. Profile-Based Shapelet Discovery \\nShapelets are time series subsequences which are in some \\nsense the maximally representative of a class [23][27]. \\nShapelets can be used to classify time series (essentially, the \\nnearest shapelet algorithm), offering the benefits of speed, \\nintuitiveness and at least on some domains, significantly \\nimproved accuracy [23]. However, these advantages come at \\nthe cost of a very expensive training phase, with O( n2m4) time \\ncomplexity, where m is the length of the longest time series \\nobject in the dataset, and n is the number of objects in the \\ntraining set. In order to mitigate this high ti me complexity, \\nresearchers have proposed various distance pruning techniques \\nand candidate ranking approaches for both the admissible [27] \\nand appro ximate [23] shapelet discovery. Nevertheless, \\nscalability remains the bottleneck.  \\nBecause shapelets are essentially supervised motifs, and we \\nhave shown that STAMP can find motifs very quickly, it is \\nnatural to ask if STAMP has implications for shapelet \\ndiscovery. While space limitations prohibit a detailed \\nconsideration of this question, we briefly sketch out and test \\nthis possibility as follows. \\nAs shown in Figure 16, we can use matrix profile to \\nheuristically ‚Äúsuggest‚Äù candidate shapelets. We consider two \\ntime series TA (green/bold) and TB (pink/light) with class 1 and \\nclass 0 being their corresponding class label, and we take  JAB, \\nJAA, JBA and JBB. Our claim is the differences in the heights of \\nPAB, PAA (or PBA, PBB) are strong indicators of good candidate \\nshapelets. The intuition is that if a discriminative pattern is \\npresent in say, class 1, but not in class 0, then we expect to see a \\n‚Äúbump‚Äù in the PAB (the intuition holds if the order is reversed). \\nA significant difference (quantified by a threshold shown in \\ndashed line) between the heights of PAA and PAB curves \\ntherefore indicates the occurrence of good candidate shapelets, \\npatterns that only occur in one of the two classes. \\n \\nFigure 16. top.left) Two time series TA and TB formed by concatenating \\ninstances of class 1 and 0 respectively of ArrowHead [6]. bottom) The height \\ndifference between PAB (or PBA) and PAA (or PBB) are suggestive of  good \\nshapelets. top.right) An example of good shapelet extracted from class 1. \\nThe time taken to compute  all four matrix profiles is 1.0 \\nseconds and the time to further evaluate the two twelve \\ncandidates selected takes 2.7 seconds. On the same machine, \\nthe brute force shapelet classifier takes 4.2 minutes with 2,364 \\ncandidates. Note that, in this toy demonst ration, the speedup is \\n68X, however, for larger datasets, the speedup is greater [24]. \\nI. Profile-Based Semantic Segmentation \\nThe goal of time series semantic segmentation is to partition \\na dataset containing multiple activities/regimes into atomic \\nbehaviors or conditions. For example, for human activity data \\nthe regimes might later be mapped to { eating, working, \\ncommuting,...}[29]. As this example suggests, most work in this \\narea is highly domain -dependent. In contrast, here we show \\nhow the matrix profile index can be emp loyed for domain \\nagnostic time series segmentation.  \\nThe intuition of our approach is as follows. Within a single \\nregime we might expect that most subsequences will have a \\nnearest neighbor close by (in time). For example, consider the \\ntoy problem shown in Figure 17 which shows two obvious \\nregimes. We would expect that the nearest neighbor to the first \\nrun gait cycle is the second or third or fourth run cycle, but it \\nwill almost certainly not be one of the walk cycles. In general, \\nthis tendency for nearest neighbor pointers not to cross the \\nboundaries corresponding to regime changes may be sufficient \\nto discover these boundaries, and of course, this is precisely the \\ninformation that is encoded in the matrix profile index. \\nThus, for each point we count how many ‚Äúarcs‚Äù connecting \\ntwo nearest neighbors cross it if we connect each subsequence \\nto its nearest neighbor as shown in Figure 17. \\n \\nFigure 17. top) A (toy) time series ( red) and nearest neighbor locations for \\neach subsequence. bottom) The number of arcs crossing above each \\nsubsequence. \\nWe applied the procedure described above to a heavily \\nstudied activity segmentation problem [29], which was derived \\nfrom the CMU Motion Capture data base [16]. The recordings \\nare represented as multi -dimensional time series, and most \\nresearch efforts carefully select the best subset for the \\nsegmentation task. For example, [29] states that ‚Äú we only \\nconsider the 14 most informative joints out of 29 .‚Äù In contrast, \\nwe attempt this with a single dimension. The only parameter we \\nneed to set is sliding window size . In Figure 18 we show the \\nsegmenting results obtained using our approach , with  \\nannotations taken from [29] for context. \\nMuch of the evaluation in this community lacks formal \\nmetrics, preferring instead visual sanity tests like the one in \\nFigure 18. Given this, we can say that our approach is very \\ncompetitive on this dataset, in spite of the fact that we \\nhandicapped ourselves to only consider one dimension.  \\n0 100\\n0 10,0005,000\\nMatrix Profile\\nPower Usage Data\\nnew maximum value\\nnew discord\\n0\\n7\\n0 1,400\\nPABPAA\\n0 1,600\\nTA\\nTB\\n0 300\\nAn example of good shapelet\\n0 1,400\\nPBA\\nPBB\\n8\\n0\\n0 1,400\\nDiff(PAB ,PAA) Threshold\\n-1\\n4\\n-2\\n4\\n0 1,400\\nDiff(PBA ,PBB) Threshold\\n0\\n1\\n2\\n1 2 0\\nwalking slow    walking slow     run       run run run\\nNumber of arcs intersecting each point'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content=\"Figure 18. top) A matrix profile ( blue) obtained for the time series ( red) and \\nnumber of arcs crossing each point ( green). Low values of this green curve \\ncoorespond to candidate split points. bottom) Human annotations of the \\nactivities: w ‚Äì walk, s ‚Äì stretch, p ‚Äì punch, c ‚Äì chop, t ‚Äì turn, d ‚Äì drink. \\nV. CONCLUSION \\nWe have introduced a scalable algorithm for creating time \\nseries subsequences joins. Our algorithm is simple, fast, \\nparallelizable and parameter -free, and can be in crementally \\nupdated for moderately fast data arrival rates. We have shown \\nthat our algorithm has implications for many existing tasks, \\nsuch as motif discovery, discord discovery, shapelet discovery \\nand semantic segmentation, and may open up new avenues for  \\nresearch, including computing various definitions of time series \\nset difference. Our code is freely available for the community to \\nconfirm, extend and exploit our ideas. \\nREFERENCES \\n[1] R. J. Bayardo, Y. Ma and R. Srikant, ‚ÄúScaling up all pairs similarity \\nsearch,‚Äù WWW 2007, pp 131-140. \\n[2] N. Begum and E. Keogh, ‚ÄúRare time series motif discovery from \\nunbounded streams,‚Äù PVLDB 8(2): 149-160, 2014. \\n[3] G. Beroza, ‚ÄúPersonal Correspondence,‚Äù Jan 21th, 2016. \\n[4] T. Bouezmarni and J. Rombouts, ‚ÄúNonparametric density estimation for \\npositive time series,‚Äù CSDA, 54, 245-261, 2010.  \\n[5] V. Chandola, D. Cheboli and V. Kumar, ‚ÄúDetecting anomalies in a time \\nseries database,‚Äù UMN TR09-004. \\n[6] T. Chen  et al ., ‚ÄúThe UCR time series classification archive,‚Äù \\nhttp://www.cs.ucr.edu/~eamonn/time_series_data/. \\n[7] ‚ÄúConvolution - Wikipedia, the free encyclopedia,‚Äù  \\nhttps://en.wikipedia.org/wiki/Convolution, Accessed: 2016-01-19. \\n[8] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang and E. J. Keogh, \\n‚ÄúQuerying and mining of time series data: experimental comparison of \\nrepresentations and distance measures,‚Äù PVLDB 1(2): 1542-1552. 2008. \\n[9] J. Hughes et al. , ‚ÄúChimpanzee and human Y chromosomes are \\nremarkably divergent in structure‚Äù Nature 463, (2010). \\n[10] H. Lee, R. Ng and K. Shim, ‚ÄúSimilarity join size estimation using \\nLocality sensitive hashing,‚Äù PVLDB, 4(6):338‚Äì349, 2011. \\n[11] W. Luo, H. Tan, H. Mao  and L. M.  Ni, ‚ÄúEfficient similarity joins on \\nmassive high-dimensional datasets using mapreduce,‚Äù In MDM'12, IEEE \\nComputer Society, pp. 1-10. \\n[12] Y. Ma, X. Meng and S. Wang, ‚ÄúParallel similarity joins on massive high-\\ndimensional data using MapReduce ,‚Äù Concurrency and Computation , \\nVolume 28, Issue 1 Jan 2016. Pages 166‚Äì183. \\n[13] S. V. Makonin, ‚ÄúAMPds: a public dataset for load disaggregation and \\neco-feedback research,‚Äù EPEC 2013, pp 1-6. \\n[14] MASS: http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html \\n[15] G. D. F. Morales and A. Gionis, ‚Äústreaming similarity self-join,‚Äù Proc. \\nVLDB Endow, 2016. \\n[16] Motion Capture Database, http://mocap.cs.cmu.edu/ \\n[17] A. Mueen, H. Hamooni and T. Estrada, ‚ÄúTime series join on subsequence \\ncorrelation,‚Äù IEEE ICDM 2014, pp. 450-459. \\n[18] A. Mueen, E. Keogh, Q. Zhu, S. Cash and B. Westover, ‚ÄúExact discovery \\nof time series motif,‚Äù SDM 2009. \\n[19] D. Murray et al. , ‚ÄúA data management platform for personalised real-\\ntime energy feedback,‚Äù In EEDAL 2015. \\n[20] V. Niennattrakul et al, ‚ÄúData editing techniques to allow the application \\nof distance-based outlier detection to streams,‚Äù ICDM 2010: 947-952. \\n[21] D. Patnaik, et al, ‚ÄúSustainable operation and management of data center \\nchillers using temporal data mining,‚Äù KDD 2009. \\n[22] T. Rakthanmanon et al., ‚ÄúSearching and Mining Trillions of Time Series \\nSubsequences under Dynamic Time Warping,‚Äù In KDD 2012, 262-270. \\n[23] T. Rakthanmanon and E. Keogh, ‚ÄúFast shapelets: a scalable algorithm for \\ndiscovering time series shapelets,‚Äù SDM, 2013. \\n[24] Supporting page. http://www.cs.ucr.edu/~eamonn/MatrixProfile.html \\n[25] C. D. Truong and D. T.  Anh, ‚ÄúAn Efficient Method for Motif and \\nAnomaly Detection in Time Series‚Äù IJBIDM, Vol. 10, No. 4, 2015. \\n[26] A. Tucker and X. Liu, ‚ÄúA Bayesian  Network Approach to Explaining \\nTime Series with Changing Structure,‚Äù Intell Data Anal, 8 (5) (2004). \\n[27] L. Ye and E. Keogh, ‚ÄúTime series shapelets: a new primitive for data \\nmining,‚Äù ACM SIGKDD, 2009, pp 947-56. \\n[28] C. Yoon, O. O‚ÄôReilly, K. Bergen and G. Beroza, ‚ÄúEarthquake detection \\nthrough computationally efficient similarity search,‚Äù Sci. Adv. 2015. \\n[29] F. Zhou, F. Torre and J.  Hodgins ‚Äú Aligned Cluster Analysis for \\nTemporal Segmentation of Human Motion,‚Äù IEEE FG'2008. \\n[30] S. Zilberstein and S. Russell, ‚ÄúApproximate Reasoning Using Anytime \\nAlgorithms,‚Äù In Imprecise and Approximate Computation , Kluwer \\nAcademic Publishers, 1995. \\n \\n0 5,000 10,000\\nMatrix  Profile\\nSplit Points   \\nPrediction\\nOne-dimension of multi-d time series: Subject 86, recording 4, dimension 30\\nW S P N      W C T           D            P         W\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Matrix Profile I: All Pairs Similarity Joins for Time Series: \\nA Unifying View that Includes Motifs, Discords and Shapelets \\nChin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,  \\nHoang Anh Dau, ‚Ä†Diego Furtado Silva, ‚Ä°Abdullah Mueen, and Eamonn Keogh \\nUniversity of California, Riverside, ‚Ä†Universidade de S√£o Paulo, ‚Ä°University of New Mexico \\n{myeh003, yzhu015, lulan001, nbegu001, yding007, hdau001}@ucr.edu, diegofsilva@icmc.usp.br, mueen@unm.edu, eamonn@cs.ucr.edu \\n \\nAbstract‚Äî The all-pairs-similarity-search (or similarity join) \\nproblem has been extensively studied for text and a handful of \\nother datatypes. However, surprisingly little progress has been  \\nmade on similarity joins for time series subsequences. The lack of  \\nprogress probably stems from the daunting nature of the \\nproblem. For even modest sized data sets the obvious nested -loop \\nalgorithm can take months, and the typical speed -up techniques \\nin this domain (i.e., indexing, lower -bounding, triangular -\\ninequality pruning and early abandoning) at best produce one or \\ntwo orders of magnitude speedup. In this work we introduce a \\nnovel scalable algorithm for time series subsequence all -pairs-\\nsimilarity-search. For exceptionally large datasets, the algorithm \\ncan be trivially cast as an anytime algorithm and produce high -\\nquality approximate solutions in reasonable  time. The exact \\nsimilarity join algorithm computes the answer to the time series \\nmotif and time series discord  problem as a side -effect, and our \\nalgorithm incidentally provides the fastest known algorithm for \\nboth these  extensively-studied problems. We de monstrate the \\nutility of our ideas for many time series data mining problems, \\nincluding motif discovery, novelty discovery,  shapelet discovery,  \\nsemantic segmentation, density estimation, and contrast set \\nmining.    \\nKeywords‚ÄîTime Series; Similarity Joins; Motif Discovery \\nI. INTRODUCTION \\nThe all-pairs-similarity-search (also known as similarity \\njoin) problem comes in several variants. The basic task is this: \\nGiven a collection of data objects, retrieve the nearest neighbor \\nfor each object . In the text domain the  algorithm has \\napplications in a host of problems, including community \\ndiscovery, duplicate detection, collaborative filtering, \\nclustering, and query refinement [1]. While virtually all text \\nprocessing algorithms have analogues in time series data \\nmining, there has been surprisingly little progress on Time \\nSeries subsequences All-Pairs-Similarity-Search (TSAPSS). \\nWe believe that this lack of progress stems not from a lack \\nof interest in this useful primitive, but from the daunting nature \\nof the problem. Consider the following example that reflects the \\nneeds of an industrial collaborator. A boiler at a che mical \\nrefinery reports pressure once a minute. After a year, we have a \\ntime series of length 525,600. A plant manager may wish to do \\na similarity self-join on this data with week -long subsequences \\n(10,080) to discover operating regimes (summer vs. winter o r \\nlight distillate vs. heavy distillate etc.) The obvious nested loop \\nalgorithm requires 132,880,692,960 Euclidean distance \\ncomputations. If we assume each one takes 0.0001 seconds, \\nthen the join will take 153.8 days. The core contribution of this \\nwork is to show that we can reduce this time to 6.3 hours, using \\nan off-the-shelf desktop computer. Moreover, we show that this \\njoin can be computed and/or updated incrementally. Thus we \\ncould maintain this join essentially forever on a standard \\ndesktop, even if the data arrival frequency was much faster than \\nonce a minute. \\nOur algorithm uses an ultra -fast similarity search algorithm \\nunder z -normalized Euclidean distance as a subroutine, \\nexploiting the overlap between subsequences using the classic \\nFast Fourier Transform (FFT) algorithm.  \\nOur method has the following advantages/features: \\n\\uf0b7 It is exact, providing no false positives or false dismissals. \\n\\uf0b7 It is simple and parameter -free. In contrast, the more \\ngeneral metric space APSS algorithms require building and \\ntuning spatial access methods and/or hash functions.  \\n\\uf0b7 Our algorithm requires an inconsequential space overhead, \\njust O(n) with a small constant factor. \\n\\uf0b7 While our exact algorithm is extremely scalable, for \\nextremely large datasets we can compute the results in an \\nanytime fashion, allowing ultra-fast approximate solutions.  \\n\\uf0b7 Having computed the similarity join for a dataset, we can \\nincrementally upd ate it very efficiently. In many domains \\nthis means we can effectively maintain exact joins on \\nstreaming data forever.  \\n\\uf0b7 Our method provides full joins, eliminating the need to \\nspecify a similarity threshold, which as we will show, is a \\nnear impossible task in this domain.  \\n\\uf0b7 Our algorithm is embarrassingly parallelizable, both on \\nmulticore processors and in distributed systems. \\nGiven all these features, our algorithm has implications for \\nmany time series data mining tasks [5][18][28].  \\nThe rest of the paper is organized as follows. Section II \\nreviews related work an d introduces the necessary background \\nmaterials and definitions. In Section III we introduce our \\nalgorithm and its anytime and incremental variants. Section IV \\nsees a detailed empirical evaluation of our algorithm and shows \\nits implications for many data mining tasks. Finally, in Section \\nV we offer conclusions and directions for future work. \\nII. RELATED WORK AND BACKGROUND \\nThe basic variant of  similarity join problem we are \\ninterested in is as follows : Given a collection of data objects, \\nretrieve the nearest neighbor for every object.   \\nOther common variants include retrieving the top-K nearest \\nneighbors or the nearest neighbor for each object if that \\nneighbor is within a user-supplied threshold, œÑ. (Such variations \\nare trivial generalizations of our proposed algorithm, so we \\nomit them from further discussion). The latter variant results in \\na much easier problem, provided that the threshold is small. For \\nexample, [1] notes that virtually all research efforts ‚Äú exploit a \\nsimilarity threshold more aggressively in order to limit the set'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='of candidate pairs that are considered..  [or] ... to reduce the \\namount of information indexed in the first place.‚Äù \\nThis critical dependence on œÑ is a major issue for text joins, \\nas it is known that ‚Äú join size can change dramatically \\ndepending on the input similarity threshold ‚Äù [10]. However, \\nthis issue is even more critical for time series for two reasons. \\nFirst, unlike similarity (which is bounded between zero and \\none), the Euclidean distance is effectively unbounded, and \\ngenerally not intuitive. For example, if two heartbeats have a \\nEuclidean distance of 17.1, are they similar? Even for a domain \\nexpert that knows the sampling rate and the noise level of the \\ndata, this is not obvious. Second, a single threshold can produce \\nradically different output sizes, even for datasets that are very \\nsimilar.  Consider Figure 1 which shows the output size vs. \\nthreshold setting for the first and s econd halves of a ten -day \\nperiod monitoring data center chillers [21]. For the first five \\ndays a threshold of 0.6 would return zero items, but for the \\nsecond five days the same setting would return 108 items.  This \\nshows the difficulty in selecting  an appropriate threshold. Our \\nsolution is to have no threshold, and do a full join. \\n \\nFigure 1.  Output size vs. threshold for data center chillers [21]. Values beyond \\n2.0 are truncated for clarity (but archived at [24]).  \\nA handful of efforts have considered joins on time series, \\nachieving speedup by (in addition to the use of MapReduce) \\nconverting the data to lower -dimensional representations such \\nas PAA [11] or SAX [12] and exploiting lower bounds and/o r \\nLocality Sensitive Hashing (LSH) to prune some calculations. \\nHowever, the methods are very complex, with many (10 -plus) \\nparameters to adjust. As [11] acknowledges with admirable \\ncandor, ‚ÄúReasoning about the optimal settings is not trivial.‚Äù In \\ncontrast, our proposed algorithm has zero parameters to set. \\nA very recent research effort [28] has tackled the scalability \\nissue by converting the real -valued time series into discrete \\n‚Äúfingerprints‚Äù before using a LSH approach, much like the text \\nretrieval community [1]. They produced impressive speedup, \\nbut they also experienced false negatives. Moreover, the \\napproach has several parameters that need to be set; for \\nexample, they need to set the threshold to a very precise 0.818.  \\nAs we shall show, our algorithm allows both anytime and \\nincremental (i.e. streaming) versions. While a streaming join \\nalgorithm for text was recently introduced [15], we are not \\naware of any such algorithms for time series data or general \\nmetric spaces. More generally, there is a large amount of \\nliterature on joins for text processing [1]. Such work is \\ninteresting, but of little utility given our constraints, data type \\nand problem setting. We require full joins, not threshold joins, \\nand we are unwilling to allow the possibility of false negatives. \\nA. Definitions and Notation \\nWe begin by defining the data type of interest, time series: \\nDefinition 1:  A time series T is a sequence of real -valued \\nnumbers ti: T = t1, t2, ..., tn where n is the length of T. \\nWe are not interested in the global properties of time series, \\nbut in the similarity between local subsequences:  \\nDefinition 2:  A subsequence Ti,m of a T is a continuous \\nsubset of the values from T of length m starting from \\nposition i.   Ti,m = ti, ti+1,‚Ä¶, ti+m-1, where 1 ‚â§  i ‚â§  n-m+1.  \\nWe can take any subsequence from a time series and \\ncompute its distance to all sequences. We call an ordered vector \\nof such distances a distance profile: \\nDefinition 3:  A distance profile D is a vector of the \\nEuclidean distances between a given query and each \\nsubsequence in an all-subsequences set (see Definition 4).  \\nNote that we are assuming that the distance is measured \\nusing the Euclidean distance between the z -normalized \\nsubsequences [8]. The distance profile can be considered a meta \\ntime series that annotates the time series T that was used to \\ngenerate it. The first three definitions are illustrated in Figure 2. \\n \\nFigure 2. A subsequence Q extracted from a time series T is used as a query to \\nevery subsequence in T. The vector of all distances is a distance profile. \\nNote that if the query and all-subsequences set belong to the \\nsame time series, the distance profile must be zero at the \\nlocation of the query, and close to zero just before and just \\nafter. Such matches are c alled trivial matches in the literature \\n[18], and are avoided by ignoring an exclusion zone (shown as \\na gray region) of m/2 before and after the location of the query. \\nWe are interested in similarity join of all subsequences of a \\ngiven time series. We define an all-subsequences set of a given \\ntime series as a set that contains all possible subsequences from \\nthe time series. The notion of all-subsequences set is purely for \\nnotational purposes. In our implementation, we do not actually \\nextract the subsequences in this form as it would require \\nsignificant time and space overhead. \\nDefinition 4: An all-subsequences set A of a time series T \\nis an ordered set of all possible subsequences of T obtained \\nby sliding a window of length m across T: A ={T1,m,, \\nT2,m,‚Ä¶, Tn-m+1,m}, where m is a user -defined subsequence \\nlength. We use A[i] to denote Ti,m. \\nWe are interested in the nearest neighbor (i.e., 1NN) relation \\nbetween subsequences; therefore, we define a 1NN-join \\nfunction which indicates the nearest neighbor relation between \\nthe two input subsequences. \\nDefinition 5:  1NN-join function :  given two all -\\nsubsequences sets A and B and two subsequences A[i] and \\nB[j], a 1NN-join function  Œ∏1nn (A[i], B[j]) is a Boolean \\nfunction which returns ‚Äútrue‚Äù only if B[j] is the nearest \\nneighbor of A[i] in the set B. \\nWith the defined join function, a similarity join set  can be \\ngenerated by applying the similarity join operator on two input \\nall-subsequences sets. \\nDefinition 6: Similarity join set : given all -subsequences \\nsets A and B, a similarity join set  JAB of A and B is a set \\ncontaining pairs of each subsequence in A with its nearest \\nneighbor in B: JAB={‚å© A[i], B[j] ‚å™ |Œ∏1nn (A[i], B[j])}.  We \\ndenote this formally as JAB = A‚ãà\\uf0711nnB. \\n0\\n400\\n800\\n1200\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\\nFirst 5 Days\\nSecond 5 Days\\nData Center Chillers\\nT, a snippet of an energy \\nconsumption\\n2,0000 m/2m/2\\nQ, query of length m\\nNote that |D| = |T|-|Q|+1D, a distance profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='We measure the Euclidean distance between each pair \\nwithin a similarity join set and store the resultants into an \\nordered vector. We call the result vector the matrix profile. \\nDefinition 7:  A matrix profile (or just profile) PAB is a \\nvector of the Euclidean distances between each pair in JAB. \\nWe call this vector the matrix profile because one \\n(inefficient) way to compute it would be to compute the full \\ndistance matrix of all the subsequences in one time series with \\nall the subsequence in another time series and extract the \\nsmallest value in each row (the smallest non-diagonal value for \\nthe self-join case).  In Figure 3 we show the matrix profile of \\nour running example. \\n \\nFigure 3. A time series T, and its self-join matrix profile P.  \\nLike the distance profile, the matrix profile can be \\nconsidered a meta time series annotating the time series T if the \\nmatrix profile is generated by joining T with itself. The profile \\nhas a host of interesting and exploitable properties. For \\nexample, the highest point on the profile corresponds to the \\ntime series discord [5], th e ( tied) lowest points correspond to \\nthe locations of the best time series motif pair [18], and the \\nvariance can be seen as a measure of the T‚Äôs com plexity. \\nMoreover, the histogram of the values in the matrix profile is \\nthe exact answer to the time series density estimation [4]. \\nWe name this special case of the similarity join set \\n(Definition 6) as self-similarity join set, and the corresponding \\nprofile as self-similarity join profile. \\nDefinition 8:  A self-similarity join  set JAA is a result of \\nsimilarity join of the set  A with itself . We denote this \\nformally as JAA = A ‚ãà\\uf0711nn A. We denote the corresponding \\nmatrix profile or self-similarity join profile as PAA. \\nNote that we exclude trivial matches when self -similarity \\njoin is performed, i.e., if A[i] and A[j] are subsequences from \\nthe same all -subsequences set A, Œ∏1nn (A[i], B[j]) is ‚Äúfalse‚Äù \\nwhen A[i] and A[j] are a trivially matched pair. \\nThe ith element in the matrix profile tells us the Euclidean \\ndistance to the nearest neighbor of the subsequence of T, \\nstarting at i.  However, it does not tell us where that neighbor is \\nlocated. This information is recorded in matrix profile index. \\nDefinition 9: A matrix profile index IAB of a similarity join \\nset JAB is a vector of integers where IAB[i] = j if {A[i], B[j]} \\n‚àà JAB. \\nBy storing the neighboring information this way, we can \\nefficiently retrieve the nearest neighbor of A[i] by accessing the \\nith element in the matrix profile index. \\nNote that the function which computes the similarity join set \\nof two input time series  is not symmetric; therefore, JAB ‚â† JBA,  \\nPAB ‚â† PBA, and IAB ‚â† IBA. \\nFor ease of presentation, we have confined this work to the \\nsingle dimensional case; however, nothing intrinsically \\nprecludes generalizations to multidimensional data. \\nSummary of the Previous Section \\nThe previous section was rather dense, so before moving on \\nwe summarize the main takeaway points. We can create two \\nmeta time series, the matrix profile and the matrix profile index, \\nto annotate a time series TA with the distance and location of all \\nits subsequences nearest neighbors in itself or another time \\nseries TB. These two data objects explicitly or implicitly contain \\nthe answers to many time series data mining tasks. However, \\nthey appear to be too expensive to compute to be practica l. In \\nthe next section we will show an algorithm that can compute \\nthese efficiently. \\nIII. ALGORITHMS \\nWe are finally in a position to explain our algorithm s. We \\nbegin by stating the fundamental intuition, which stems from \\nthe relationship between distance profiles and the matrix \\nprofile. As Figure 2 and Figure 3 visually suggest, all distance \\nprofiles (excluding the trivial match region) are upper bound \\napproximations to the matrix profile. More critically, if we \\ncompute all the distance profiles, an d take the minimum value \\nat each location, the result is the matrix profile! \\nThis tells us that if we have a fast way to compute the \\ndistance profiles, then we also have a fast way to compute the \\nmatrix profile. As we shall show in the next section, we have an \\nultra-fast way to compute the distance profiles. \\nA. Mueen‚Äôs ultra-fast Algorithm for Similarity Search (MASS) \\nWe begin by introducing a novel Euclidean distance \\nsimilarity search algorithm for time series data. The algorithm \\ndoes not just find the nearest neighbor to a query and return its \\ndistance; it returns the distance to every subsequence. In \\nparticular, it computes the distance profile, as shown in Figure \\n2. The algorithm requires just O( nlogn) time by exploiting the \\nFFT to calculate  the dot products between the query and all \\nsubsequences of the time series.  \\nWe need to carefully qualify the claim of ‚Äúultra-fast‚Äù. There \\nare dozens of algorithms for time series similarity search that \\nutilize index structures to efficiently locate neighbors [8]. While \\nsuch algorithms can be faster in the  best case , all these \\nalgorithms degenerate to brute force search in the worst case 1 \\n(actually, much worse than brute force search due to the \\noverhead of the index). Likewise, there are index -free methods \\nthat achieve speed -up using various early abandoning tricks \\n[22], but they too degrade to brute force search in the worst \\ncase. In contrast, the performance of the algorithms outlined in \\nTABLE I and TABLE II is completely independent of the data. \\nTABLE I. CALCULATION OF SLIDING DOT PRODUCTS \\nProcedure SlidingDotProduct(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: The dot product between Q and all subsequences in T \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nn ‚Üê Length(T), m ‚Üê Length(Q) \\nTa ‚Üê Append T with n zeros   \\nQr ‚Üê Reverse(Q)  \\nQra ‚Üê Append Qr with 2n-m zeros \\nQraf ‚Üê FFT(Qra), Taf ‚Üê FFT(Ta) \\nQT ‚Üê InverseFFT(ElementwiseMultiplication(Qraf, Taf)) \\nreturn QT \\n \\n1 There are many such worse case scenarios, including high levels of noise blurring \\nthe distinction between closest and furthest neighbors, and rendering triangular -\\ninequality pruning and early abandoning worthless. \\n2,0000\\nP, a matrix profile\\nT, a snippet of an energy \\nconsumption\\nNote that |P| = |T|-|Q|+1'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Line 1 determines the length of both the time series T and \\nthe query Q. In line 2, we use that information to append T with \\nan equal number of zeros. In line 3, we obtain the mirror image \\nof the original query. This reversing ensures that a convolution \\n(i.e. ‚Äúcrisscrossed‚Äù multiplication) essentially produces in-order \\nalignment. Because we require both vectors to be the s ame \\nlength, in line 4 we append enough zeros to the (now reversed) \\nquery so that, like Ta, it is also of length 2 n. In line 5, the \\nalgorithm calculates Fourier transforms of the appended -\\nreversed query (Qra) and the appended time series Ta. Note that \\nwe use FFT algorithm which is an O(nlogn) algorithm. The Qraf \\nand the Taf produced in line 5 are vectors of complex numbers \\nrepresenting frequency components of the two time series. The \\nalgorithm calculates the element-wise multiplication of the two \\ncomplex vectors and performs inverse FFT on the product. \\nLines 5-6 are the class ic convolution operation on two vectors \\n[7]. Figure 4 shows a toy example of the sliding dot product \\nfunction in work. The algorithm time complexity does not \\ndepend on the length of the query (m). \\n \\nFigure 4. A toy example of convolution operation being used to calculate  \\nsliding dot products for time series data. Note the reverse and append \\noperation on T and q in the input. Fifteen dot products are calculated for every \\nslide. The cells m = 2 to n = 4 from left ( red/bold arrows) contain valid \\nproducts. TABLE II takes this subroutine and uses it to create a distance \\nprofile (see Definition 3). \\nIn line 1 of TABLE II, we invoke the dot products code \\noutlined in TABLE I. The formula to calculate the z-normalized \\nEuclidean distance D[i] between two time series subsequence Q \\nand Ti,m using their dot product, QT[i] is (see [24] for \\nderivation): \\nùê∑[ùëñ] = ‚àö2ùëö(1‚àíùëÑùëá[ùëñ]‚àíùëöùúáùëÑùëÄùëá[ùëñ]\\nùëöùúéùëÑùõ¥ùëá[ùëñ] ) \\nwhere m is the subsequence length, ŒºQ is the mean of Q, \\nMT[i] is the mean of Ti,m, œÉQ is the standard deviation of Q, and  \\nŒ£T[i] is the standard deviation of Ti,m. Normally, it takes O( m) \\ntime to calculate the mean and standard deviation for every \\nsubsequence of a long time series. However, here we exploit a \\ntechnique noted in [22] in a different context. We cache \\ncumulative sums of the values and square of the values in the \\ntime series. At any stage the two cumulative sum vectors are \\nsufficient to calculate the mean an d the standard deviation of \\nany subsequence of any length.  See [14] for an elaborate \\ndescription and variations of MASS. \\nTABLE II. MUEEN‚ÄôS ALGORITHM FOR SIMILARITY SEARCH (MASS) \\nProcedure MASS(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: A distance profile D of the query Q  \\n1 \\n2 \\n3 \\n4 \\nQT ‚Üê SlidingDotProducts(Q, T) \\nŒºQ, œÉQ, ŒúT, Œ£T  ‚Üê ComputeMeanStd(Q, T)    // see [22] \\nD ‚Üê CalculateDistanceProfile(Q, T, QT, ŒºQ, œÉQ, ŒúT, Œ£T) \\nreturn D \\nUnlike the dozens of time series KNN search algorithms in \\nthe literature [8], this algorithm calculates the distance to every \\nsubsequence, i.e. the distance profile  of time series T. \\nAlternatively, in join nomenclature, the algorithm produces one \\nfull row of the all -pair similarity matrix. Thus, as we show in \\nthe next section, our join algorithm is simply a loop that \\ncomputes each full row of the all -pair similarity matrix and \\nupdates the current ‚Äúbest-so-far‚Äù matrix profile when needed. \\nB. The STAMP Algorithm \\nWe call our join algorithm STAMP, Scalable Time series \\nAnytime Matrix Profile. The algorithm is outlined in TABLE \\nIII. In line 1, we extract the length of TB. In line 2, we allocate \\nmemory and initial matrix profile PAB and matrix profile index \\nIAB. From lines 3 to line 6, we calculate the distance profiles D \\nusing each subsequence B[idx] in the time series TB and the \\ntime series TA. Then, we perform pairwise minimum for each \\nelement in D with the paired element in PAB (i.e., min( D[i], \\nPAB[i]) for i = 0 to length(D) - 1.) We also update IAB[i] with idx \\nwhen D[i] ‚â§ PAB[i] as we perform the  pairwise minimum \\noperation. Finally, we return the result PAB and IAB in line 7.  \\nNote that the algorithm presented in TABLE III computes \\nthe matrix profile for the general similarity join. To modify the \\ncurrent algorithm to compute  the self -similarity join matrix \\nprofile of a time series TA, we simply replace TB in line 1 with \\nTA, replace B with A in line 4, and ignore trivial match in D \\nwhen performing ElementWiseMin in line 5. \\nTABLE III. THE STAMP ALGORITHM \\nProcedure STAMP(TA, TB, m) \\nInput: Two user provided time series, TA and TB, interested \\nsubsequence length m \\nOutput: A matrix profile PAB and associated matrix profile index IAB of \\nTA join TB, JAB = A‚ãà\\uf0711nnB \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nnB ‚Üê Length(TB) \\nPAB ‚Üê infs, IAB ‚Üê zeros, idxes ‚Üê 1:nB-m+1 \\nfor each idx in idxes    // In any order, but random for anytime algorithm \\n          D ‚Üê MASS(B[idx], TA) \\n          PAB, IAB ‚Üê ElementWiseMin(PAB, IAB, D, idx) \\nend for \\nreturn PAB, IAB \\nTo parallelize the STAMP algorithm for multicore \\nmachines, we simply distribute the indexes to secondary \\nprocess run in each core, and the secondary processes use the \\nindexes they received to update their own PAB and IAB. Once the \\nmain process returns from  all secondary processes, we use \\nElementWiseMin to merge the received PAB and IAB.  \\nC. An Anytime Algorithm for TSAPSS \\nWhile the exact algorithm introduced in the previous section \\nis extremely scalable, there will always be datasets for which \\ntime needed for an exact solution is untenable. We can mitigate \\nthis by computing the results in an anytime fashion, allowing \\nfast approximate solutions [30]. To a dd the anytime nature to \\nthe STAMP algorithm, we simply ensure a randomized search \\norder in line 2 of TABLE III.  \\nWe can compute a (post-hoc) measurement of the quality of \\nan anytime solution by measuring the Root -Mean-Squared-\\nError (RMSE) between the true matrix profile and the current \\nbest-so-far mat rix profile. As Figure 5 suggests, with an \\nexperiment on random walk data, the algorithm converges very \\nquickly. \\nQ2T1 0 00 00 00 00 0Q1T4Q2T2+Q1T1 Q2T3+Q1T2 Q2T4+Q1T3\\nOutput\\nInputT2T1 T4T3 00 00 Q1Q2 00 00 00'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Figure 5. main) The decre ase in RMSE as the STAMP algorithm updates \\nmatrix profile with the distance profile calculated at each iteration.  inset) The \\napproximate matrix profile at the 10% mark is visually indistinguishable from \\nthe final matrix profile. \\nZilberstein [30] gives a number of desirable properties of \\nanytime algorithms, including Low Overhead , Interruptibility, \\nMonotonicity, Recognizable Quality, Diminishing Returns and \\nPreemptability (these properties are mostly obvious from their \\nnames, but full definitions are at [30]). \\nBecause each subsequence‚Äôs distance profile is bounded \\nbelow by the exact matrix profile, updating an approximate \\nmatrix profile with a distance profile with pairwise minimum \\noperation either drives the approximate solution closer the exact \\nsolution or retains the current approximate solution. Thus , we \\nhave guaranteed Monotonicity. From Figure 5, the approximate \\nmatrix profile converges to the exact matrix profile \\nsuperlinearly; therefore, we have strong Diminishing Returns. \\nWe can easily achieve Interruptibility and Preemptability by \\nsimply inserting a few lines of code between lines 5 and 6 of \\nTABLE III that read: \\n5new \\n6new \\n7new \\nif CheckForUserInterrupt  = TRUE \\n          Report({PAB, IAB}, ‚ÄòHere is an approximate answer.‚Äô) \\nif GetUserChoice = ‚Äòfurther refine‚Äô, CONTINUE, else BREAK \\nThe space and time overhead for the anytime property is \\neffectively zero, thus we have Low Overhead. This leaves only \\nthe property of Recognizable Quality. Here we must resort to a \\nprobabilistic argument.  The convergence curve shown in  \\nFigure 5 is very typical, so we could use past convergence \\ncurves to predict the quality of solution when interrupted on \\nsimilar data.  \\nD. Time and Space Complexity \\nThe overall complexity of the proposed algorithm is \\nO(n2logn) where n is the length of the time series. However, our \\nexperiments (see Section 4.1) empirically suggest that the \\nruntime of  STAMP‚Äôs growth rate is roughly O( n2) instead of \\nO(n2logn). One possible e xplanation for this is that the nlogn \\nfactor comes from the FFT subroutine. Because FFT is so \\nimportant in many applications, it is extraordinarily well \\noptimized. Thus, the empirical runtime is very close to linear. \\nIn contrast to the above, the brute for ce nested loop \\napproach has a time complexity of O(n2m). Recall the industrial \\nexample in the introduction section. We have  m = 10,080, but \\nlog(n) = 5.7, so we would expect our approach to be about \\n1,768 times faster. In fact, we are empirically even faster. The \\ncomplexity analysis downplays the details of important constant \\nfactors. The nested loop algorithm must also z -normalize the \\nsubsequences. This either requires O( nm) time, but with an \\nuntenable O(nm) space overhead, or an O( n2m) time overhead. \\nAnd r ecall that this is before a single Euclidean distance \\ncalculation is performed.  \\nFinally, we mention one quirk of our algorithm which we \\ninherit from using the highly optimized FFT subroutine. Our \\nalgorithm is fastest when n is an integer power of two, slower \\nfor non -power of two but composite numbers, and slowest \\nwhen n is prime. The difference (for otherwise similar values of \\nn) can approach a factor of 1.6x. Thus, where possible, it is \\nworth contriving the best case by tru ncation or zero-padding to \\nthe nearest power of two. \\nE. Incrementally Maintaining TSAPSS \\nUp to this point we have discussed the batch version of \\nTSAPSS. By batch, we mean that the STAMP algorithm needs \\nto see the entire time series TA and TB (or just TA if we are \\ncalculating the self -similarity join matrix profile) before \\ncreating the matrix profile. However, it would be advantageous \\nif we could build the matrix profile incrementally. Given that \\nwe have performed a batch construction of matrix profile, i f \\nnew data arrives, it would clearly be preferable to incrementally \\nadjust the current profile, rather than start from scratch.  \\nBecause the matrix profile solves both the times series motif \\nand the time series discord problems, an incremental version of \\nSTAMP would automatically provide the first incremental \\nversions of both algorithms. We call such an algorithm the \\nSTAMPI (STAMP Incremental) algorithm. \\nWe will demonstrate our ability to incrementally maintain \\nthe matrix profile in this section. For simpli city and brevity \\nTABLE IV only shows the algorithm to maintain the self -\\nsimilarity join. The generalizations are obvious. \\nTABLE IV. THE STAMPI ALGORITHM \\nProcedure STAMPI(TA, t, PAA, IAA) \\nInput: The original time series TA, a new data point t following TA, the \\nmatrix profile PAA and its associated matrix profile index IAA of TA.  \\nOutput: The incrementally updated matrix profile PAA,new and its matrix \\nprofile index IAA,new of the current time series TA,new= TA, t. \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nTA,new = [TA, t] \\nS ‚Üê last subsequence in TA,new, idx ‚Üê index of S in TA,new \\nD ‚Üê MASS (S, TA) \\nPAA, IAA ‚Üê ElementWiseMin(PAA, IAA, D, idx) \\npAA,last, iAA,last ‚Üê FindMin(D) \\nPAA,new ‚Üê [PAA, pAA,last], IAA,new ‚Üê [IAA, iAA,last] \\nreturn PAA,new, IAA,new \\nFor clarity, we denote the updated time series as TA,new, the \\nupdated matrix profile as PAA,new and the associated matrix \\nprofile index as IAA,new. As each additional data point t arrives, \\nthe size of the time series TA increases by one, and a new \\nsubsequence S is generated at the end of TA,new. In line 3 we \\nobtain the distance profile of S with regard to TA. Then, as in the \\noriginal STAMP algorithm,  in line 4 we perform  a pairwise \\ncomparison between every element in D with the corresponding \\nelement in PAA to see if the corresponding element in PAA needs \\nto be updated. In line 5, we find the nearest neighbor of S and \\nthe associated index by evaluating the minimum value of D. \\nFinally, in line 6, we obtain the new matrix profile and \\nassociated matrix profile index by concatenating the results in \\nline 4 and line 5.  \\nThe time complexity of the STAMP I algorithm is O(nlogn) \\nwhere n is the length of size of th e current time series TA. Note \\nthat as we maintain the profile, each incremental call of \\nSTAMPI requires invoking the FFT subroutine with a slightly \\nlonger time series ( n becomes n+1 ). Thus it gets very slightly \\nslower at each time step. Therefore, the best way to measure the \\nperformance is to ask for the Maximum Time Horizon (MTH), \\nin essence the answer to this question: ‚Äú Given this arrival rate, \\nhow long can we maintain the profile before we can no longer \\nupdate fast enough?‚Äù \\nNote that the subsequence length m is not considered in the \\nMTH evaluation. Recall that overall time complexity of the \\n10,0000\\nRMSE\\niteration1,000\\napproximate matrix \\nprofile at 1,000 iterations.\\nexact matrix profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='algorithm is determined by the efficiency of the FFT \\nsubroutine, which is independent of m. We have computed the \\nMTH for two common scenarios of interest to the community. \\n\\uf0b7 House Electrical Demand  [19]: This dataset is updated \\nevery eight seconds. By iteratively calling the STAMP I \\nalgorithm, we can maintain the profile for 5.8 years.  \\n\\uf0b7 Oil Refinery :  Most telemetry in oil refineries is sampled \\nat once a minute [26]. The relatively low sampling rate \\nreflects the ‚Äúinertia‚Äù of massive boilers/condensers. Even if \\nwe maintain the profile for 40 years, the update time is only \\naround 6.78 seconds. Moreover, th e raw data, matrix \\nprofile and index would only require 0.5 gigabytes of main \\nmemory. Thus the MTH here is forty plus years. Given \\nprojected improvements in hardware, this effectively \\nmeans we can maintain the matrix profile forever. \\nAs impressive as these  numbers are, they are actually quite \\npessimistic. For simplicity we assume that every value in the \\nmatrix profile index will be updated at each time step.  \\nHowever, empirically, much less than 0.1% of them need to be \\nupdated. If it is possible to prove an upper bound on the number \\nof changes to the matrix profile index per update, then we could \\ngreatly extend the MTH  or handle much faster sampling rates. \\nWe leave such considerations for future work. \\nIV. EXPERIMENTAL EVALUATION \\nWe begin by stating our experimen tal philosophy. We have \\ndesigned all experiments such that they are easily reproducible. \\nTo this end, we have built a webpage [24] which contains al l \\ndatasets and code used in this work, together with spreadsheets \\nwhich contain the raw numbers and some supporting videos.  \\nGiven page limits, and the utility of our algorithm for a host \\nof existing (and several new) data mining tasks, we have chosen \\nto c onduct exceptionally broad  but shallow experiments. We \\nhave conducted such deep detailed experiments and placed \\nthem at [24]. Unless otherwise state d we measure wall clock \\ntime on an Intel i7@4GHz with 4 cores. \\nA. Scalability of Profile-Based Self-Join \\nBecause the time performance of STAMP is independent of \\nthe data quality or any user inputs (there are none except the \\nchoice of m, which does not affect the speed), our scalability \\nexperiments are unusually brief.  In TABLE V we show the \\ntime required for a self -join with m fixed to 256, for \\nincreasingly long time series.  \\nTABLE V.  TIME REQUIRED FOR A SELF-JOIN WITH M = 256, VARYING N \\nValue of n 217 218 219 220 221 \\nTime Required 15.1 min 70.4 min 5.4 hours 24.4 hours 4.2 days \\nIn TABLE VI, we show the time required for a self -join \\nwith n fixed to 2 17, for increasing long m. Again recall that \\nunlike virtually all other time series data mining algorithms in \\nthe literature whose performance degrades for longer \\nsubsequences [8][18], the running time of STAMP does not \\ndepend on m. \\nTABLE VI. TIME REQUIRED FOR A SELF-JOIN WITH N= 217, VARYING M \\nValue of m 64 128 256 512 1,024 \\nTime Required 15.1 min 15.1 min 15.1 min 15.0 min 14.5 min \\nFinally, we further exploit the simple parallelizability of the \\nalgorithm by using four 16 -core virtual machines on Microsoft \\nAzure to redo the two -million join ( n = 2 21 and m = 256) \\nexperiment. By scaling up the computational power, we have \\nreduced the running time from 4.2 days to just 14.1 hours. This \\nuse of cloud computing required writing just few dozen lines of \\nsimple additional code [24]. \\nIt is difficult to find good baselines to compare to. We \\nbelieve STAMP is the only algorithm that does full, exact joins \\non time series subsequences. Most algorithms do only threshold \\njoins and/or do approximate joins, and are not specialized for \\ntime series subsequences. However, we searched for the best \\nbaselines and made the following concessions to  the rival \\nalgorithms to allow comparisons. \\n\\uf0b7 TSFRDAA [11]: While STAMP returns all nearest \\nneighbors, we adjust the threshold (the selectively) of \\nTSFRDAA such that only needs to return the top 1% nearest \\nneighbors, a much easier task. \\n\\uf0b7 HDSJI-SAX [12]: This method  allows false negatives ; we \\nignore this. It needs time to build indexes ; we do not count \\nthis time, and as above, we allow it to return  the only the \\ntop 1% nearest neighbors ( as bef ore, not the 100% the \\nSTAMP returns, and therefore a much easier task.). \\n\\uf0b7 Optimized Nested Loop (ONL): Here we use a nested -loop \\njoin optimized in the following manner. We use a state -of-\\nthe-art DFT indexing technique to do the search in the \\ninner loop, and we do not count the time needed to build \\nthe indexes [8]. We carefully adjust  parameters for best \\nperformance.  \\nWe consider the first 2 18 data points of the ECG dataset \\nused in [22], with a query length of 256, about one heartbeat ; \\nTABLE VII shows the results.  \\nTABLE VII. TIME FOR A SELF-JOIN WITH M = 256, VARYING ALGORITHMS \\nAlgorithm TSFRDAA HDSJI-SAX ONL STAMP \\nTime Required 51.7 hours 19.6 min 28.1 hours 1.17 hours \\nAs these results show, even if we ignore the limitations of \\nthe baseline methods, STAMP is still significantly faster. \\nB. Profile-Based Self-Join \\nA recent paper notes that many fundamental problems in \\nseismology can be solved by joining seismometer telemetry \\n[28], including the disc overy of foreshocks, aftershocks, \\ntriggered earthquakes, volcanic activity and induced seismicity. \\nHowever, the paper notes a join with a query length of 200 on a \\ndata stream of length 604,781 requires 9.5 days. Their solution, \\na clever transformation of t he data to allow LSH based \\ntechniques, does achieve significant speedup, but at the cost of \\nfalse negatives and the need for significant parameter tuning.  \\nThe authors kindly shared their data, and, as we hint at in \\nFigure 6, confirmed that STAMP does not have false negatives. \\n \\nFigure 6. top) An excerpt of a seismic time series aligned with its matrix \\nprofile (bottom). The ground truth provided by the authors of [28] requires that \\nthe events occurring at time 4,050 and 7,800 match. \\nWe repeated the n = 604,781, m = 200 experime nt and \\nfound it took just 8.9 hours to finish. As impressive as this is, in \\nthe next section we show that we can do even better.   \\n4,000 5,000 6,000 7,000 8,000 9,000\\n0\\n5\\n10\\n15\\nSeismic time series (excerpt) \\nMatrix Profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='C. The Utility of Anytime STAMP \\nThe seismology dataset offers an excellent opportunity to \\ndemonstrate the utility of the anytime version of our algorithm. \\nThe authors of [28] revealed in their long -term ambition of \\nmining even larger datasets [3]. In Figure 7 we repeated the \\nexperiment with the snippet shown in Figure 6, this time \\nreporting the best-so-far matrix profile reported by the \\nalgorithm at various milestones. Even with just 0.25% of the \\ndistances computed (that is to say, 400 times faster) the correct \\nanswer has emerged. Thus, we can provide the correct answers \\nto the seismologists in just minutes, rather than the 9.5 days.  \\n \\nFigure 7. top) An excerpt of the seismic data that is also shown in Figure 6. \\ntop-to-bottom) The approximations of the matrix profile for increasing \\ninterrupt times. By the time we have computed just 0.25% of the calculations \\nrequired, the minimum of the matrix profile points to the ground truth. \\nTo show the generality of this anytime feature of STAMP, \\nwe consider a very different dataset. As shown in Figure \\n8.inset), it is possible to convert DNA to a time ser ies [22]. We \\nconverted the Y-chromosome of the Chimpanzee this way. The \\nresulting time series is  little over one -million in length. We \\nperformed a self-join with m = 60,000.  Figure 8.bottom shows \\nthe best motif is so well conserved (ignoring the first 20%), that \\nit must correspond to a recent (in evolutionary tim e) gene \\nduplication event. In fact, in a subsequent analysis we \\ndiscovered that ‚Äúmuch of the Y (Chimp chromosome) consists of \\nlengthy, highly similar repeat units, or ‚Äòamplicons‚Äô‚Äù [9]. \\n \\nFigure 8. top) The Y -chromosome of the Chimp in time series space with its \\nmatrix profile. bottom) A zoom -in of the top motif discovered using anytime \\nSTAMP, we believe it to be an amplicon [9]. \\nThis demanding join would take just over a day of CPU \\ntime (see TABLE V). However, using anytime STAMP we \\nhave the result shown above after doing just 0.021% of the \\ncomputations, in about 18 seconds. At [24] we have videos that \\nshow the rapid convergence of the anytime variant of STAMP. \\nD. Profile-Based Similarity Join Set \\nIn this section we show two uses of similarity join set. The \\nfirst use is more familiar to APSS users, quantifying what is \\nsimilar between two time series. The second example, \\nquantifying what is different between two time series, is novel  \\nand can only be supported by threshold -free algorithms that \\nreport the nearest neighbor for all objects. \\nTime Series Set Similarity \\nGiven two (or more) time series collected under different \\nconditions or treatments, a data analyst may wish to know what \\npatterns (if any) are conserved between the two time series. To \\ndemonstrate the utility of automating this, we consider a simple \\nbut in tuitive example. Figure 9 shows the raw audio of two \\npopular songs converted to Mel Frequency Cepstral \\nCoefficients (MFCCs). Specifically, the songs used in this \\nexample are ‚ÄúUnder Pressure‚Äù by Queen an d David Bowie and \\n‚ÄúIce Ice Baby ‚Äù by the American rapper Vanilla Ice. Normally, \\nthere are 13 MFCCs; here, we consider just one for simplicity. \\n \\n \\nFigure 9. Two songs represented by just the 2 nd MFCC at 100Hz. We \\nrecognize that it is difficult to see any structure in these times series; however, \\nthis difficulty is the motivation for this experiment. \\nEven for these two relatively short time series, visual \\ninspection does not offer immediate answers. The problem here \\nis compounded by the size reproduction, but is not significantly \\neasier with large-scale [24] or interactive graphic tools. We ran \\nJAB (Queen-Bowie, Vanilla Ice) on these datasets with m = 500 \\n(five seconds), the best match, corresponding the minimum \\nvalue of the matrix profile PAB, is shown in Figure 10. \\n \\nFigure 10. The result of JAB (Queen-Bowie (red/bold), Vanilla-Ice (green/fine)) \\nproduces a strongly conserved five second region. \\nReaders may know the cause for this highly conserved \\nsubsequence. It corresponds to the famous baseline of ‚ÄúUnder \\nPressure,‚Äù which was sampled (plagiarized) by Vanilla Ice in \\nhis song. The join took 21.9 seconds. The ability to find \\nconserved structure in apparently disparate time series could \\nopen many avenues of research in medicine and industry. \\nTime Series Difference \\nWe introduce the Time Series Diff (TSD) operator, which \\ninformally asks ‚ÄúWhat happens in time series T A, that does not \\nhappen in time series TB?‚Äù Here TA/TB could be an ECG before \\na drug is administered/after a drug is administered, telemetry \\nbefore a successful launch/before a catastrophic launch etc. The \\nTSD is simply the subsequence referred to by the maximum \\nvalue of the JAB join‚Äôs profile PAB. \\nWe begin with a simple intuitive example. The UK and US \\nversions of the Harry Potter audiobook series are performed by \\ndifferent narrators, and have a handful of differences in the text. \\nFor example, the UK version contains: \\nHarry was passionate about Quidditch. He had played as Seeker on the Gryffindor \\nhouse Quidditch team ever since his first year at Hogwarts and owned a Firebolt, one \\nof the best racing brooms in the world... \\nBut the corresponding USA version has: \\nHarry had been on the Gryffindor House Quidditch team ever since his first year at \\nHogwarts and owned one of the best racing brooms in the world, a Firebolt. \\nAs shown in Figure 11, we can convert the audio \\ncorresponding to these snippets into MFCCs and invoke a JAB \\njoin set to produce a matrix profile PAB that represents the \\ndifferences between them. As Figure 11.left shows, the low \\nvalues of this profile correspond to identical spoken phrases \\n(despite having two different narrators). However here we are \\n4,000 5,000 6,000 7,000 8,000 9,000\\ndata\\n0.25%\\n1%\\n100%\\n0 1,000,0000\\n100\\n200\\nPan troglodytes Y-chromosome \\n0 60,000\\n12,749,475 to 14,249,474 bp\\n622,725 to 2,122,724 bp\\nT1 = 0;\\nfor i = 1 to length(chromosome) \\nif chromosomei = A, then Ti+1 = Ti + 2 \\nif chromosomei = G, then Ti+1 = Ti + 1\\nif chromosomei = C, then Ti+1 = Ti - 1 \\nif chromosomei = T, then Ti+1 = Ti - 2 \\nend\\n0\\nQueen-Bowie\\n1,000 2,000\\n-10\\n0\\n10\\nVanilla Ice\\n0 250 500-3\\n-2\\n-1\\n0\\n1\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='interested in the differences, the maximum value of the profile. \\nAs we can see in Figure 11.right, here the profile corresponds \\nto a phrase unique to the USA edition.  \\n \\nFigure 11. The 2 nd MFCC of snippets from the USA ( pink/bold) and UK \\n(green/fine) Harry Potter audiobooks.  The JAB join of the two longer sections \\nin the main text produce s mostly small values in the profile correspond to the \\nsame phrase (left), the largest value in  the profile corresponds to a phrase \\nunique to the USA edition (right). \\nThe time required to do this is just 0.067 seconds, much \\nfaster than real time. While this demonstration is trivial, in [24] \\nwe show an example applied to ECG telemetry.  \\nE. Profile-Based Motif Discovery \\nSince their introduction in 2003, time series motifs have \\nbecome one of the most frequently used primitives in time  \\nseries data mining, with applications in dozens of domains [2]. \\nThere are several proposed definitions for time series motifs, \\nbut in [18] it is argued that if you can solve the most basic \\nvariant, the closest (non -trivial) pair of subsequences, then all \\nother variants only require some minor additional calculations. \\nNote that the locations of the two (tying) minimum values of \\nthe matrix profile are exactly the locations of the 1st motif pair. \\nThe fastest known exact algorithm for computing time \\nseries motifs is the MK algorithm [18]. Note, however, that this \\nalgorithm‚Äôs time performance depends on the time series itself. \\nIn contrast, the Profile -Based Motif Discovery (PBMD) takes \\ntime independent of the data. To see this, we compared the two \\napproaches on an electrocardiogram of length 65,536. In Figure \\n12.left we ask what happens as we search for lon ger and longer \\nmotifs. In Figure 12.right we ask what happens if the motif \\nlength is fixed to m = 512, but the data becomes increasing \\nnoisy.  \\n \\nFigure 12. The time required to find the top -motif pairs in a time series of \\nlength 216 for increasingly long motif lengths ( left), and for a length fixed to \\n512, but in the face of increasing noise levels (right). \\nThese results show that  even in the best case for MK, \\nPBMD is competitive, but as we have longer queries and/or \\nnoisier data, its advantage becomes unassailable. Moreover, \\nPBMD inherits STAMP‚Äôs anytime and incremental \\ncomputability, and is easily parallelizable.  \\nF. Profile-Based Discord Discovery \\nA time series discord  is the subsequence that has the \\nmaximum distance to its nearest neighbor. While this is a \\nsimple definition, time series discords are known to be very \\ncompetitive as novelty/anomaly detectors  [5]. Note that as \\nshown in Figure 13, the time series discord is encoded as the \\nmaximum value in a matrix profile. \\n \\nFigure 13. top) An excerpt from an ECG incorporating a premature ventricular \\ncontraction (red/bold). bottom) The time series profile peaks exactly at the \\nbeginning of the PVC. \\nThe time taken to compute the discord  is obviously just the \\ntime needed to compute the matrix profile ( here, 0.9 seconds). \\nThere are a few dozen discord discovery algorithms in the \\nliterature. Some of them may be competitive in the best case, \\nbut just like motif -discovery algorithms they all degenerate to \\nbrute force search in the worst case, and none allow the anytime \\nproperties that we inherit from using STAMP.  \\nG. Incrementally Maintaining Motifs and Discords \\nWe have demonstrated the ability to detect time series \\nmotifs and discords using the matrix profile in the previous two \\nsections. However, we assumed that the entire time series was \\navailable beforehand. Here we remove this assumption and \\nshow how STAMP I allows us to incrementa lly maintain time \\nseries motifs/ discords in an online fashion. There are other \\nattempts at one [20][2] or both [25] of these tasks, but they are \\nall approximate and allow false dismissals.   \\nIn Section III.E, we introduced the STAMPI algorithm. The \\nability to incrementally maintain the matrix profile implies the \\nability to exactly maintain the time series motif [18] and/or time \\nseries discord [5] in streaming data. We simply need to keep \\ntrack of the extreme values of the incrementally-growing matrix \\nprofile, report a new pair of motifs when a new minimum value \\nis detected, and report a new discord when we see a new \\nmaximum value. \\nWe demonstrate the utility of these ideas on the AMPds \\ndataset [13]. Here the kitchen fridge and the heat pump are both \\nplugged into a single metered power supply. For the first week, \\nonly the refrigerator is running. At the end of the week, the \\nweather gets cold and the heat pump is turned on. The sampling \\nrate is one sample/m inute, and th e subsequence length is 100. \\nWe apply the STAMP algorithm to the first three days of data, \\nthen invoke STAMP I to handle newly arriving data , report an \\nevent when we detect a new extreme value. \\nOur first event occurs at the 9,864 th minute (6 da y 20 hour \\n24 minute). As shown in Figure 14, a new minimum value is \\ndetected, which indicates a new time series motif. The just -\\narrived 100 -minute-long pattern looks v ery similar to another \\npattern that occurred five hours earlier. While there is a lot of \\nregularity in the fridge data in general, the exceptional \\nsimilarity observed here suggested some underlying physical \\nmechanism that caused such a perfectly -conserved pattern, \\nperhaps a mechanical ice-making ‚Äúsubroutine.‚Äù \\n \\nFigure 14. top) The matrix profile of the first 9,864 minutes of data. bottom) \\nThe minimum value of the matrix profile corresponds to a pair of time series \\nmotifs in the power usage data. right) The time series motif detected. \\nOur second event occurs at the 10,473 th minute (7 day 6 \\nhour 33 minute). As shown in Figure 15, a new maximum value \\n‚Ä¶indor house Quidditch team ever since his first ye‚Ä¶\\nHarry had been on the Gryffindor House Quidditch te..\\nsince his first year at Hogwarts and owned a Fire..\\nsince his first year at Hogwarts and owned on..\\nED = 2.9 \\nClosest Match \\n0 100(1.6 seconds) 0 100\\nED = 10.4\\n(1.6 seconds)\\nFurthest Match (Time Series Difference) \\n256 512 1,024 2,048 4,096\\n0\\n40\\n80\\n120\\n160\\n200\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nSubsequence Length  (m)\\nTime Taken (min)\\n80 40 0\\n0\\n70\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nNoise Added (dB)  \\n0 1000 2000 3000\\nECG qtdb\\nSel102\\n(excerpt)\\nPremature Ventricular \\nContraction\\nTime Series Profile\\n0 5,000 10,000\\n0 100\\nnew minimum value\\nnew motifMatrix Profile\\nPower Usage Data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='is detected, which indicates a new time series discord. The time \\nseries discord corresponds to the first occurrence of a heat \\npump pattern in the power usage data.  \\n \\nFigure 15. top) The matrix profile for the first 10,473 minutes. bottom) The \\nmaximum value of the matrix profile corresponds to a time series discord. \\nright) The time series discord detected is the first  heat pump pattern \\noccurrence in the dataset. \\nThe maximum time needed to process a single data point \\nwith STAMP I in this dataset is 0.005 second s, which is less \\nthan 0.01% of the data sampling rate. Thus, on this dataset we \\ncould continue monitoring with the STAMP I algorithm for \\nseveral decades before running out of time or memory. \\nH. Profile-Based Shapelet Discovery \\nShapelets are time series subsequences which are in some \\nsense the maximally representative of a class [23][27]. \\nShapelets can be used to classify time series (essentially, the \\nnearest shapelet algorithm), offering the benefits of speed, \\nintuitiveness and at least on some domains, significantly \\nimproved accuracy [23]. However, these advantages come at \\nthe cost of a very expensive training phase, with O( n2m4) time \\ncomplexity, where m is the length of the longest time series \\nobject in the dataset, and n is the number of objects in the \\ntraining set. In order to mitigate this high ti me complexity, \\nresearchers have proposed various distance pruning techniques \\nand candidate ranking approaches for both the admissible [27] \\nand appro ximate [23] shapelet discovery. Nevertheless, \\nscalability remains the bottleneck.  \\nBecause shapelets are essentially supervised motifs, and we \\nhave shown that STAMP can find motifs very quickly, it is \\nnatural to ask if STAMP has implications for shapelet \\ndiscovery. While space limitations prohibit a detailed \\nconsideration of this question, we briefly sketch out and test \\nthis possibility as follows. \\nAs shown in Figure 16, we can use matrix profile to \\nheuristically ‚Äúsuggest‚Äù candidate shapelets. We consider two \\ntime series TA (green/bold) and TB (pink/light) with class 1 and \\nclass 0 being their corresponding class label, and we take  JAB, \\nJAA, JBA and JBB. Our claim is the differences in the heights of \\nPAB, PAA (or PBA, PBB) are strong indicators of good candidate \\nshapelets. The intuition is that if a discriminative pattern is \\npresent in say, class 1, but not in class 0, then we expect to see a \\n‚Äúbump‚Äù in the PAB (the intuition holds if the order is reversed). \\nA significant difference (quantified by a threshold shown in \\ndashed line) between the heights of PAA and PAB curves \\ntherefore indicates the occurrence of good candidate shapelets, \\npatterns that only occur in one of the two classes. \\n \\nFigure 16. top.left) Two time series TA and TB formed by concatenating \\ninstances of class 1 and 0 respectively of ArrowHead [6]. bottom) The height \\ndifference between PAB (or PBA) and PAA (or PBB) are suggestive of  good \\nshapelets. top.right) An example of good shapelet extracted from class 1. \\nThe time taken to compute  all four matrix profiles is 1.0 \\nseconds and the time to further evaluate the two twelve \\ncandidates selected takes 2.7 seconds. On the same machine, \\nthe brute force shapelet classifier takes 4.2 minutes with 2,364 \\ncandidates. Note that, in this toy demonst ration, the speedup is \\n68X, however, for larger datasets, the speedup is greater [24]. \\nI. Profile-Based Semantic Segmentation \\nThe goal of time series semantic segmentation is to partition \\na dataset containing multiple activities/regimes into atomic \\nbehaviors or conditions. For example, for human activity data \\nthe regimes might later be mapped to { eating, working, \\ncommuting,...}[29]. As this example suggests, most work in this \\narea is highly domain -dependent. In contrast, here we show \\nhow the matrix profile index can be emp loyed for domain \\nagnostic time series segmentation.  \\nThe intuition of our approach is as follows. Within a single \\nregime we might expect that most subsequences will have a \\nnearest neighbor close by (in time). For example, consider the \\ntoy problem shown in Figure 17 which shows two obvious \\nregimes. We would expect that the nearest neighbor to the first \\nrun gait cycle is the second or third or fourth run cycle, but it \\nwill almost certainly not be one of the walk cycles. In general, \\nthis tendency for nearest neighbor pointers not to cross the \\nboundaries corresponding to regime changes may be sufficient \\nto discover these boundaries, and of course, this is precisely the \\ninformation that is encoded in the matrix profile index. \\nThus, for each point we count how many ‚Äúarcs‚Äù connecting \\ntwo nearest neighbors cross it if we connect each subsequence \\nto its nearest neighbor as shown in Figure 17. \\n \\nFigure 17. top) A (toy) time series ( red) and nearest neighbor locations for \\neach subsequence. bottom) The number of arcs crossing above each \\nsubsequence. \\nWe applied the procedure described above to a heavily \\nstudied activity segmentation problem [29], which was derived \\nfrom the CMU Motion Capture data base [16]. The recordings \\nare represented as multi -dimensional time series, and most \\nresearch efforts carefully select the best subset for the \\nsegmentation task. For example, [29] states that ‚Äú we only \\nconsider the 14 most informative joints out of 29 .‚Äù In contrast, \\nwe attempt this with a single dimension. The only parameter we \\nneed to set is sliding window size . In Figure 18 we show the \\nsegmenting results obtained using our approach , with  \\nannotations taken from [29] for context. \\nMuch of the evaluation in this community lacks formal \\nmetrics, preferring instead visual sanity tests like the one in \\nFigure 18. Given this, we can say that our approach is very \\ncompetitive on this dataset, in spite of the fact that we \\nhandicapped ourselves to only consider one dimension.  \\n0 100\\n0 10,0005,000\\nMatrix Profile\\nPower Usage Data\\nnew maximum value\\nnew discord\\n0\\n7\\n0 1,400\\nPABPAA\\n0 1,600\\nTA\\nTB\\n0 300\\nAn example of good shapelet\\n0 1,400\\nPBA\\nPBB\\n8\\n0\\n0 1,400\\nDiff(PAB ,PAA) Threshold\\n-1\\n4\\n-2\\n4\\n0 1,400\\nDiff(PBA ,PBB) Threshold\\n0\\n1\\n2\\n1 2 0\\nwalking slow    walking slow     run       run run run\\nNumber of arcs intersecting each point'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content=\"Figure 18. top) A matrix profile ( blue) obtained for the time series ( red) and \\nnumber of arcs crossing each point ( green). Low values of this green curve \\ncoorespond to candidate split points. bottom) Human annotations of the \\nactivities: w ‚Äì walk, s ‚Äì stretch, p ‚Äì punch, c ‚Äì chop, t ‚Äì turn, d ‚Äì drink. \\nV. CONCLUSION \\nWe have introduced a scalable algorithm for creating time \\nseries subsequences joins. Our algorithm is simple, fast, \\nparallelizable and parameter -free, and can be in crementally \\nupdated for moderately fast data arrival rates. We have shown \\nthat our algorithm has implications for many existing tasks, \\nsuch as motif discovery, discord discovery, shapelet discovery \\nand semantic segmentation, and may open up new avenues for  \\nresearch, including computing various definitions of time series \\nset difference. Our code is freely available for the community to \\nconfirm, extend and exploit our ideas. \\nREFERENCES \\n[1] R. J. Bayardo, Y. Ma and R. Srikant, ‚ÄúScaling up all pairs similarity \\nsearch,‚Äù WWW 2007, pp 131-140. \\n[2] N. Begum and E. Keogh, ‚ÄúRare time series motif discovery from \\nunbounded streams,‚Äù PVLDB 8(2): 149-160, 2014. \\n[3] G. Beroza, ‚ÄúPersonal Correspondence,‚Äù Jan 21th, 2016. \\n[4] T. Bouezmarni and J. Rombouts, ‚ÄúNonparametric density estimation for \\npositive time series,‚Äù CSDA, 54, 245-261, 2010.  \\n[5] V. Chandola, D. Cheboli and V. Kumar, ‚ÄúDetecting anomalies in a time \\nseries database,‚Äù UMN TR09-004. \\n[6] T. Chen  et al ., ‚ÄúThe UCR time series classification archive,‚Äù \\nhttp://www.cs.ucr.edu/~eamonn/time_series_data/. \\n[7] ‚ÄúConvolution - Wikipedia, the free encyclopedia,‚Äù  \\nhttps://en.wikipedia.org/wiki/Convolution, Accessed: 2016-01-19. \\n[8] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang and E. J. Keogh, \\n‚ÄúQuerying and mining of time series data: experimental comparison of \\nrepresentations and distance measures,‚Äù PVLDB 1(2): 1542-1552. 2008. \\n[9] J. Hughes et al. , ‚ÄúChimpanzee and human Y chromosomes are \\nremarkably divergent in structure‚Äù Nature 463, (2010). \\n[10] H. Lee, R. Ng and K. Shim, ‚ÄúSimilarity join size estimation using \\nLocality sensitive hashing,‚Äù PVLDB, 4(6):338‚Äì349, 2011. \\n[11] W. Luo, H. Tan, H. Mao  and L. M.  Ni, ‚ÄúEfficient similarity joins on \\nmassive high-dimensional datasets using mapreduce,‚Äù In MDM'12, IEEE \\nComputer Society, pp. 1-10. \\n[12] Y. Ma, X. Meng and S. Wang, ‚ÄúParallel similarity joins on massive high-\\ndimensional data using MapReduce ,‚Äù Concurrency and Computation , \\nVolume 28, Issue 1 Jan 2016. Pages 166‚Äì183. \\n[13] S. V. Makonin, ‚ÄúAMPds: a public dataset for load disaggregation and \\neco-feedback research,‚Äù EPEC 2013, pp 1-6. \\n[14] MASS: http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html \\n[15] G. D. F. Morales and A. Gionis, ‚Äústreaming similarity self-join,‚Äù Proc. \\nVLDB Endow, 2016. \\n[16] Motion Capture Database, http://mocap.cs.cmu.edu/ \\n[17] A. Mueen, H. Hamooni and T. Estrada, ‚ÄúTime series join on subsequence \\ncorrelation,‚Äù IEEE ICDM 2014, pp. 450-459. \\n[18] A. Mueen, E. Keogh, Q. Zhu, S. Cash and B. Westover, ‚ÄúExact discovery \\nof time series motif,‚Äù SDM 2009. \\n[19] D. Murray et al. , ‚ÄúA data management platform for personalised real-\\ntime energy feedback,‚Äù In EEDAL 2015. \\n[20] V. Niennattrakul et al, ‚ÄúData editing techniques to allow the application \\nof distance-based outlier detection to streams,‚Äù ICDM 2010: 947-952. \\n[21] D. Patnaik, et al, ‚ÄúSustainable operation and management of data center \\nchillers using temporal data mining,‚Äù KDD 2009. \\n[22] T. Rakthanmanon et al., ‚ÄúSearching and Mining Trillions of Time Series \\nSubsequences under Dynamic Time Warping,‚Äù In KDD 2012, 262-270. \\n[23] T. Rakthanmanon and E. Keogh, ‚ÄúFast shapelets: a scalable algorithm for \\ndiscovering time series shapelets,‚Äù SDM, 2013. \\n[24] Supporting page. http://www.cs.ucr.edu/~eamonn/MatrixProfile.html \\n[25] C. D. Truong and D. T.  Anh, ‚ÄúAn Efficient Method for Motif and \\nAnomaly Detection in Time Series‚Äù IJBIDM, Vol. 10, No. 4, 2015. \\n[26] A. Tucker and X. Liu, ‚ÄúA Bayesian  Network Approach to Explaining \\nTime Series with Changing Structure,‚Äù Intell Data Anal, 8 (5) (2004). \\n[27] L. Ye and E. Keogh, ‚ÄúTime series shapelets: a new primitive for data \\nmining,‚Äù ACM SIGKDD, 2009, pp 947-56. \\n[28] C. Yoon, O. O‚ÄôReilly, K. Bergen and G. Beroza, ‚ÄúEarthquake detection \\nthrough computationally efficient similarity search,‚Äù Sci. Adv. 2015. \\n[29] F. Zhou, F. Torre and J.  Hodgins ‚Äú Aligned Cluster Analysis for \\nTemporal Segmentation of Human Motion,‚Äù IEEE FG'2008. \\n[30] S. Zilberstein and S. Russell, ‚ÄúApproximate Reasoning Using Anytime \\nAlgorithms,‚Äù In Imprecise and Approximate Computation , Kluwer \\nAcademic Publishers, 1995. \\n \\n0 5,000 10,000\\nMatrix  Profile\\nSplit Points   \\nPrediction\\nOne-dimension of multi-d time series: Subject 86, recording 4, dimension 30\\nW S P N      W C T           D            P         W\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Matrix Profile I: All Pairs Similarity Joins for Time Series: \\nA Unifying View that Includes Motifs, Discords and Shapelets \\nChin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,  \\nHoang Anh Dau, ‚Ä†Diego Furtado Silva, ‚Ä°Abdullah Mueen, and Eamonn Keogh \\nUniversity of California, Riverside, ‚Ä†Universidade de S√£o Paulo, ‚Ä°University of New Mexico \\n{myeh003, yzhu015, lulan001, nbegu001, yding007, hdau001}@ucr.edu, diegofsilva@icmc.usp.br, mueen@unm.edu, eamonn@cs.ucr.edu \\n \\nAbstract‚Äî The all-pairs-similarity-search (or similarity join) \\nproblem has been extensively studied for text and a handful of \\nother datatypes. However, surprisingly little progress has been  \\nmade on similarity joins for time series subsequences. The lack of  \\nprogress probably stems from the daunting nature of the \\nproblem. For even modest sized data sets the obvious nested -loop \\nalgorithm can take months, and the typical speed -up techniques \\nin this domain (i.e., indexing, lower -bounding, triangular -\\ninequality pruning and early abandoning) at best produce one or \\ntwo orders of magnitude speedup. In this work we introduce a \\nnovel scalable algorithm for time series subsequence all -pairs-\\nsimilarity-search. For exceptionally large datasets, the algorithm \\ncan be trivially cast as an anytime algorithm and produce high -\\nquality approximate solutions in reasonable  time. The exact \\nsimilarity join algorithm computes the answer to the time series \\nmotif and time series discord  problem as a side -effect, and our \\nalgorithm incidentally provides the fastest known algorithm for \\nboth these  extensively-studied problems. We de monstrate the \\nutility of our ideas for many time series data mining problems, \\nincluding motif discovery, novelty discovery,  shapelet discovery,  \\nsemantic segmentation, density estimation, and contrast set \\nmining.    \\nKeywords‚ÄîTime Series; Similarity Joins; Motif Discovery \\nI. INTRODUCTION \\nThe all-pairs-similarity-search (also known as similarity \\njoin) problem comes in several variants. The basic task is this: \\nGiven a collection of data objects, retrieve the nearest neighbor \\nfor each object . In the text domain the  algorithm has \\napplications in a host of problems, including community \\ndiscovery, duplicate detection, collaborative filtering, \\nclustering, and query refinement [1]. While virtually all text \\nprocessing algorithms have analogues in time series data \\nmining, there has been surprisingly little progress on Time \\nSeries subsequences All-Pairs-Similarity-Search (TSAPSS). \\nWe believe that this lack of progress stems not from a lack \\nof interest in this useful primitive, but from the daunting nature \\nof the problem. Consider the following example that reflects the \\nneeds of an industrial collaborator. A boiler at a che mical \\nrefinery reports pressure once a minute. After a year, we have a \\ntime series of length 525,600. A plant manager may wish to do \\na similarity self-join on this data with week -long subsequences \\n(10,080) to discover operating regimes (summer vs. winter o r \\nlight distillate vs. heavy distillate etc.) The obvious nested loop \\nalgorithm requires 132,880,692,960 Euclidean distance \\ncomputations. If we assume each one takes 0.0001 seconds, \\nthen the join will take 153.8 days. The core contribution of this \\nwork is to show that we can reduce this time to 6.3 hours, using \\nan off-the-shelf desktop computer. Moreover, we show that this \\njoin can be computed and/or updated incrementally. Thus we \\ncould maintain this join essentially forever on a standard \\ndesktop, even if the data arrival frequency was much faster than \\nonce a minute. \\nOur algorithm uses an ultra -fast similarity search algorithm \\nunder z -normalized Euclidean distance as a subroutine, \\nexploiting the overlap between subsequences using the classic \\nFast Fourier Transform (FFT) algorithm.  \\nOur method has the following advantages/features: \\n\\uf0b7 It is exact, providing no false positives or false dismissals. \\n\\uf0b7 It is simple and parameter -free. In contrast, the more \\ngeneral metric space APSS algorithms require building and \\ntuning spatial access methods and/or hash functions.  \\n\\uf0b7 Our algorithm requires an inconsequential space overhead, \\njust O(n) with a small constant factor. \\n\\uf0b7 While our exact algorithm is extremely scalable, for \\nextremely large datasets we can compute the results in an \\nanytime fashion, allowing ultra-fast approximate solutions.  \\n\\uf0b7 Having computed the similarity join for a dataset, we can \\nincrementally upd ate it very efficiently. In many domains \\nthis means we can effectively maintain exact joins on \\nstreaming data forever.  \\n\\uf0b7 Our method provides full joins, eliminating the need to \\nspecify a similarity threshold, which as we will show, is a \\nnear impossible task in this domain.  \\n\\uf0b7 Our algorithm is embarrassingly parallelizable, both on \\nmulticore processors and in distributed systems. \\nGiven all these features, our algorithm has implications for \\nmany time series data mining tasks [5][18][28].  \\nThe rest of the paper is organized as follows. Section II \\nreviews related work an d introduces the necessary background \\nmaterials and definitions. In Section III we introduce our \\nalgorithm and its anytime and incremental variants. Section IV \\nsees a detailed empirical evaluation of our algorithm and shows \\nits implications for many data mining tasks. Finally, in Section \\nV we offer conclusions and directions for future work. \\nII. RELATED WORK AND BACKGROUND \\nThe basic variant of  similarity join problem we are \\ninterested in is as follows : Given a collection of data objects, \\nretrieve the nearest neighbor for every object.   \\nOther common variants include retrieving the top-K nearest \\nneighbors or the nearest neighbor for each object if that \\nneighbor is within a user-supplied threshold, œÑ. (Such variations \\nare trivial generalizations of our proposed algorithm, so we \\nomit them from further discussion). The latter variant results in \\na much easier problem, provided that the threshold is small. For \\nexample, [1] notes that virtually all research efforts ‚Äú exploit a \\nsimilarity threshold more aggressively in order to limit the set'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='of candidate pairs that are considered..  [or] ... to reduce the \\namount of information indexed in the first place.‚Äù \\nThis critical dependence on œÑ is a major issue for text joins, \\nas it is known that ‚Äú join size can change dramatically \\ndepending on the input similarity threshold ‚Äù [10]. However, \\nthis issue is even more critical for time series for two reasons. \\nFirst, unlike similarity (which is bounded between zero and \\none), the Euclidean distance is effectively unbounded, and \\ngenerally not intuitive. For example, if two heartbeats have a \\nEuclidean distance of 17.1, are they similar? Even for a domain \\nexpert that knows the sampling rate and the noise level of the \\ndata, this is not obvious. Second, a single threshold can produce \\nradically different output sizes, even for datasets that are very \\nsimilar.  Consider Figure 1 which shows the output size vs. \\nthreshold setting for the first and s econd halves of a ten -day \\nperiod monitoring data center chillers [21]. For the first five \\ndays a threshold of 0.6 would return zero items, but for the \\nsecond five days the same setting would return 108 items.  This \\nshows the difficulty in selecting  an appropriate threshold. Our \\nsolution is to have no threshold, and do a full join. \\n \\nFigure 1.  Output size vs. threshold for data center chillers [21]. Values beyond \\n2.0 are truncated for clarity (but archived at [24]).  \\nA handful of efforts have considered joins on time series, \\nachieving speedup by (in addition to the use of MapReduce) \\nconverting the data to lower -dimensional representations such \\nas PAA [11] or SAX [12] and exploiting lower bounds and/o r \\nLocality Sensitive Hashing (LSH) to prune some calculations. \\nHowever, the methods are very complex, with many (10 -plus) \\nparameters to adjust. As [11] acknowledges with admirable \\ncandor, ‚ÄúReasoning about the optimal settings is not trivial.‚Äù In \\ncontrast, our proposed algorithm has zero parameters to set. \\nA very recent research effort [28] has tackled the scalability \\nissue by converting the real -valued time series into discrete \\n‚Äúfingerprints‚Äù before using a LSH approach, much like the text \\nretrieval community [1]. They produced impressive speedup, \\nbut they also experienced false negatives. Moreover, the \\napproach has several parameters that need to be set; for \\nexample, they need to set the threshold to a very precise 0.818.  \\nAs we shall show, our algorithm allows both anytime and \\nincremental (i.e. streaming) versions. While a streaming join \\nalgorithm for text was recently introduced [15], we are not \\naware of any such algorithms for time series data or general \\nmetric spaces. More generally, there is a large amount of \\nliterature on joins for text processing [1]. Such work is \\ninteresting, but of little utility given our constraints, data type \\nand problem setting. We require full joins, not threshold joins, \\nand we are unwilling to allow the possibility of false negatives. \\nA. Definitions and Notation \\nWe begin by defining the data type of interest, time series: \\nDefinition 1:  A time series T is a sequence of real -valued \\nnumbers ti: T = t1, t2, ..., tn where n is the length of T. \\nWe are not interested in the global properties of time series, \\nbut in the similarity between local subsequences:  \\nDefinition 2:  A subsequence Ti,m of a T is a continuous \\nsubset of the values from T of length m starting from \\nposition i.   Ti,m = ti, ti+1,‚Ä¶, ti+m-1, where 1 ‚â§  i ‚â§  n-m+1.  \\nWe can take any subsequence from a time series and \\ncompute its distance to all sequences. We call an ordered vector \\nof such distances a distance profile: \\nDefinition 3:  A distance profile D is a vector of the \\nEuclidean distances between a given query and each \\nsubsequence in an all-subsequences set (see Definition 4).  \\nNote that we are assuming that the distance is measured \\nusing the Euclidean distance between the z -normalized \\nsubsequences [8]. The distance profile can be considered a meta \\ntime series that annotates the time series T that was used to \\ngenerate it. The first three definitions are illustrated in Figure 2. \\n \\nFigure 2. A subsequence Q extracted from a time series T is used as a query to \\nevery subsequence in T. The vector of all distances is a distance profile. \\nNote that if the query and all-subsequences set belong to the \\nsame time series, the distance profile must be zero at the \\nlocation of the query, and close to zero just before and just \\nafter. Such matches are c alled trivial matches in the literature \\n[18], and are avoided by ignoring an exclusion zone (shown as \\na gray region) of m/2 before and after the location of the query. \\nWe are interested in similarity join of all subsequences of a \\ngiven time series. We define an all-subsequences set of a given \\ntime series as a set that contains all possible subsequences from \\nthe time series. The notion of all-subsequences set is purely for \\nnotational purposes. In our implementation, we do not actually \\nextract the subsequences in this form as it would require \\nsignificant time and space overhead. \\nDefinition 4: An all-subsequences set A of a time series T \\nis an ordered set of all possible subsequences of T obtained \\nby sliding a window of length m across T: A ={T1,m,, \\nT2,m,‚Ä¶, Tn-m+1,m}, where m is a user -defined subsequence \\nlength. We use A[i] to denote Ti,m. \\nWe are interested in the nearest neighbor (i.e., 1NN) relation \\nbetween subsequences; therefore, we define a 1NN-join \\nfunction which indicates the nearest neighbor relation between \\nthe two input subsequences. \\nDefinition 5:  1NN-join function :  given two all -\\nsubsequences sets A and B and two subsequences A[i] and \\nB[j], a 1NN-join function  Œ∏1nn (A[i], B[j]) is a Boolean \\nfunction which returns ‚Äútrue‚Äù only if B[j] is the nearest \\nneighbor of A[i] in the set B. \\nWith the defined join function, a similarity join set  can be \\ngenerated by applying the similarity join operator on two input \\nall-subsequences sets. \\nDefinition 6: Similarity join set : given all -subsequences \\nsets A and B, a similarity join set  JAB of A and B is a set \\ncontaining pairs of each subsequence in A with its nearest \\nneighbor in B: JAB={‚å© A[i], B[j] ‚å™ |Œ∏1nn (A[i], B[j])}.  We \\ndenote this formally as JAB = A‚ãà\\uf0711nnB. \\n0\\n400\\n800\\n1200\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\\nFirst 5 Days\\nSecond 5 Days\\nData Center Chillers\\nT, a snippet of an energy \\nconsumption\\n2,0000 m/2m/2\\nQ, query of length m\\nNote that |D| = |T|-|Q|+1D, a distance profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='We measure the Euclidean distance between each pair \\nwithin a similarity join set and store the resultants into an \\nordered vector. We call the result vector the matrix profile. \\nDefinition 7:  A matrix profile (or just profile) PAB is a \\nvector of the Euclidean distances between each pair in JAB. \\nWe call this vector the matrix profile because one \\n(inefficient) way to compute it would be to compute the full \\ndistance matrix of all the subsequences in one time series with \\nall the subsequence in another time series and extract the \\nsmallest value in each row (the smallest non-diagonal value for \\nthe self-join case).  In Figure 3 we show the matrix profile of \\nour running example. \\n \\nFigure 3. A time series T, and its self-join matrix profile P.  \\nLike the distance profile, the matrix profile can be \\nconsidered a meta time series annotating the time series T if the \\nmatrix profile is generated by joining T with itself. The profile \\nhas a host of interesting and exploitable properties. For \\nexample, the highest point on the profile corresponds to the \\ntime series discord [5], th e ( tied) lowest points correspond to \\nthe locations of the best time series motif pair [18], and the \\nvariance can be seen as a measure of the T‚Äôs com plexity. \\nMoreover, the histogram of the values in the matrix profile is \\nthe exact answer to the time series density estimation [4]. \\nWe name this special case of the similarity join set \\n(Definition 6) as self-similarity join set, and the corresponding \\nprofile as self-similarity join profile. \\nDefinition 8:  A self-similarity join  set JAA is a result of \\nsimilarity join of the set  A with itself . We denote this \\nformally as JAA = A ‚ãà\\uf0711nn A. We denote the corresponding \\nmatrix profile or self-similarity join profile as PAA. \\nNote that we exclude trivial matches when self -similarity \\njoin is performed, i.e., if A[i] and A[j] are subsequences from \\nthe same all -subsequences set A, Œ∏1nn (A[i], B[j]) is ‚Äúfalse‚Äù \\nwhen A[i] and A[j] are a trivially matched pair. \\nThe ith element in the matrix profile tells us the Euclidean \\ndistance to the nearest neighbor of the subsequence of T, \\nstarting at i.  However, it does not tell us where that neighbor is \\nlocated. This information is recorded in matrix profile index. \\nDefinition 9: A matrix profile index IAB of a similarity join \\nset JAB is a vector of integers where IAB[i] = j if {A[i], B[j]} \\n‚àà JAB. \\nBy storing the neighboring information this way, we can \\nefficiently retrieve the nearest neighbor of A[i] by accessing the \\nith element in the matrix profile index. \\nNote that the function which computes the similarity join set \\nof two input time series  is not symmetric; therefore, JAB ‚â† JBA,  \\nPAB ‚â† PBA, and IAB ‚â† IBA. \\nFor ease of presentation, we have confined this work to the \\nsingle dimensional case; however, nothing intrinsically \\nprecludes generalizations to multidimensional data. \\nSummary of the Previous Section \\nThe previous section was rather dense, so before moving on \\nwe summarize the main takeaway points. We can create two \\nmeta time series, the matrix profile and the matrix profile index, \\nto annotate a time series TA with the distance and location of all \\nits subsequences nearest neighbors in itself or another time \\nseries TB. These two data objects explicitly or implicitly contain \\nthe answers to many time series data mining tasks. However, \\nthey appear to be too expensive to compute to be practica l. In \\nthe next section we will show an algorithm that can compute \\nthese efficiently. \\nIII. ALGORITHMS \\nWe are finally in a position to explain our algorithm s. We \\nbegin by stating the fundamental intuition, which stems from \\nthe relationship between distance profiles and the matrix \\nprofile. As Figure 2 and Figure 3 visually suggest, all distance \\nprofiles (excluding the trivial match region) are upper bound \\napproximations to the matrix profile. More critically, if we \\ncompute all the distance profiles, an d take the minimum value \\nat each location, the result is the matrix profile! \\nThis tells us that if we have a fast way to compute the \\ndistance profiles, then we also have a fast way to compute the \\nmatrix profile. As we shall show in the next section, we have an \\nultra-fast way to compute the distance profiles. \\nA. Mueen‚Äôs ultra-fast Algorithm for Similarity Search (MASS) \\nWe begin by introducing a novel Euclidean distance \\nsimilarity search algorithm for time series data. The algorithm \\ndoes not just find the nearest neighbor to a query and return its \\ndistance; it returns the distance to every subsequence. In \\nparticular, it computes the distance profile, as shown in Figure \\n2. The algorithm requires just O( nlogn) time by exploiting the \\nFFT to calculate  the dot products between the query and all \\nsubsequences of the time series.  \\nWe need to carefully qualify the claim of ‚Äúultra-fast‚Äù. There \\nare dozens of algorithms for time series similarity search that \\nutilize index structures to efficiently locate neighbors [8]. While \\nsuch algorithms can be faster in the  best case , all these \\nalgorithms degenerate to brute force search in the worst case 1 \\n(actually, much worse than brute force search due to the \\noverhead of the index). Likewise, there are index -free methods \\nthat achieve speed -up using various early abandoning tricks \\n[22], but they too degrade to brute force search in the worst \\ncase. In contrast, the performance of the algorithms outlined in \\nTABLE I and TABLE II is completely independent of the data. \\nTABLE I. CALCULATION OF SLIDING DOT PRODUCTS \\nProcedure SlidingDotProduct(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: The dot product between Q and all subsequences in T \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nn ‚Üê Length(T), m ‚Üê Length(Q) \\nTa ‚Üê Append T with n zeros   \\nQr ‚Üê Reverse(Q)  \\nQra ‚Üê Append Qr with 2n-m zeros \\nQraf ‚Üê FFT(Qra), Taf ‚Üê FFT(Ta) \\nQT ‚Üê InverseFFT(ElementwiseMultiplication(Qraf, Taf)) \\nreturn QT \\n \\n1 There are many such worse case scenarios, including high levels of noise blurring \\nthe distinction between closest and furthest neighbors, and rendering triangular -\\ninequality pruning and early abandoning worthless. \\n2,0000\\nP, a matrix profile\\nT, a snippet of an energy \\nconsumption\\nNote that |P| = |T|-|Q|+1'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Line 1 determines the length of both the time series T and \\nthe query Q. In line 2, we use that information to append T with \\nan equal number of zeros. In line 3, we obtain the mirror image \\nof the original query. This reversing ensures that a convolution \\n(i.e. ‚Äúcrisscrossed‚Äù multiplication) essentially produces in-order \\nalignment. Because we require both vectors to be the s ame \\nlength, in line 4 we append enough zeros to the (now reversed) \\nquery so that, like Ta, it is also of length 2 n. In line 5, the \\nalgorithm calculates Fourier transforms of the appended -\\nreversed query (Qra) and the appended time series Ta. Note that \\nwe use FFT algorithm which is an O(nlogn) algorithm. The Qraf \\nand the Taf produced in line 5 are vectors of complex numbers \\nrepresenting frequency components of the two time series. The \\nalgorithm calculates the element-wise multiplication of the two \\ncomplex vectors and performs inverse FFT on the product. \\nLines 5-6 are the class ic convolution operation on two vectors \\n[7]. Figure 4 shows a toy example of the sliding dot product \\nfunction in work. The algorithm time complexity does not \\ndepend on the length of the query (m). \\n \\nFigure 4. A toy example of convolution operation being used to calculate  \\nsliding dot products for time series data. Note the reverse and append \\noperation on T and q in the input. Fifteen dot products are calculated for every \\nslide. The cells m = 2 to n = 4 from left ( red/bold arrows) contain valid \\nproducts. TABLE II takes this subroutine and uses it to create a distance \\nprofile (see Definition 3). \\nIn line 1 of TABLE II, we invoke the dot products code \\noutlined in TABLE I. The formula to calculate the z-normalized \\nEuclidean distance D[i] between two time series subsequence Q \\nand Ti,m using their dot product, QT[i] is (see [24] for \\nderivation): \\nùê∑[ùëñ] = ‚àö2ùëö(1‚àíùëÑùëá[ùëñ]‚àíùëöùúáùëÑùëÄùëá[ùëñ]\\nùëöùúéùëÑùõ¥ùëá[ùëñ] ) \\nwhere m is the subsequence length, ŒºQ is the mean of Q, \\nMT[i] is the mean of Ti,m, œÉQ is the standard deviation of Q, and  \\nŒ£T[i] is the standard deviation of Ti,m. Normally, it takes O( m) \\ntime to calculate the mean and standard deviation for every \\nsubsequence of a long time series. However, here we exploit a \\ntechnique noted in [22] in a different context. We cache \\ncumulative sums of the values and square of the values in the \\ntime series. At any stage the two cumulative sum vectors are \\nsufficient to calculate the mean an d the standard deviation of \\nany subsequence of any length.  See [14] for an elaborate \\ndescription and variations of MASS. \\nTABLE II. MUEEN‚ÄôS ALGORITHM FOR SIMILARITY SEARCH (MASS) \\nProcedure MASS(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: A distance profile D of the query Q  \\n1 \\n2 \\n3 \\n4 \\nQT ‚Üê SlidingDotProducts(Q, T) \\nŒºQ, œÉQ, ŒúT, Œ£T  ‚Üê ComputeMeanStd(Q, T)    // see [22] \\nD ‚Üê CalculateDistanceProfile(Q, T, QT, ŒºQ, œÉQ, ŒúT, Œ£T) \\nreturn D \\nUnlike the dozens of time series KNN search algorithms in \\nthe literature [8], this algorithm calculates the distance to every \\nsubsequence, i.e. the distance profile  of time series T. \\nAlternatively, in join nomenclature, the algorithm produces one \\nfull row of the all -pair similarity matrix. Thus, as we show in \\nthe next section, our join algorithm is simply a loop that \\ncomputes each full row of the all -pair similarity matrix and \\nupdates the current ‚Äúbest-so-far‚Äù matrix profile when needed. \\nB. The STAMP Algorithm \\nWe call our join algorithm STAMP, Scalable Time series \\nAnytime Matrix Profile. The algorithm is outlined in TABLE \\nIII. In line 1, we extract the length of TB. In line 2, we allocate \\nmemory and initial matrix profile PAB and matrix profile index \\nIAB. From lines 3 to line 6, we calculate the distance profiles D \\nusing each subsequence B[idx] in the time series TB and the \\ntime series TA. Then, we perform pairwise minimum for each \\nelement in D with the paired element in PAB (i.e., min( D[i], \\nPAB[i]) for i = 0 to length(D) - 1.) We also update IAB[i] with idx \\nwhen D[i] ‚â§ PAB[i] as we perform the  pairwise minimum \\noperation. Finally, we return the result PAB and IAB in line 7.  \\nNote that the algorithm presented in TABLE III computes \\nthe matrix profile for the general similarity join. To modify the \\ncurrent algorithm to compute  the self -similarity join matrix \\nprofile of a time series TA, we simply replace TB in line 1 with \\nTA, replace B with A in line 4, and ignore trivial match in D \\nwhen performing ElementWiseMin in line 5. \\nTABLE III. THE STAMP ALGORITHM \\nProcedure STAMP(TA, TB, m) \\nInput: Two user provided time series, TA and TB, interested \\nsubsequence length m \\nOutput: A matrix profile PAB and associated matrix profile index IAB of \\nTA join TB, JAB = A‚ãà\\uf0711nnB \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nnB ‚Üê Length(TB) \\nPAB ‚Üê infs, IAB ‚Üê zeros, idxes ‚Üê 1:nB-m+1 \\nfor each idx in idxes    // In any order, but random for anytime algorithm \\n          D ‚Üê MASS(B[idx], TA) \\n          PAB, IAB ‚Üê ElementWiseMin(PAB, IAB, D, idx) \\nend for \\nreturn PAB, IAB \\nTo parallelize the STAMP algorithm for multicore \\nmachines, we simply distribute the indexes to secondary \\nprocess run in each core, and the secondary processes use the \\nindexes they received to update their own PAB and IAB. Once the \\nmain process returns from  all secondary processes, we use \\nElementWiseMin to merge the received PAB and IAB.  \\nC. An Anytime Algorithm for TSAPSS \\nWhile the exact algorithm introduced in the previous section \\nis extremely scalable, there will always be datasets for which \\ntime needed for an exact solution is untenable. We can mitigate \\nthis by computing the results in an anytime fashion, allowing \\nfast approximate solutions [30]. To a dd the anytime nature to \\nthe STAMP algorithm, we simply ensure a randomized search \\norder in line 2 of TABLE III.  \\nWe can compute a (post-hoc) measurement of the quality of \\nan anytime solution by measuring the Root -Mean-Squared-\\nError (RMSE) between the true matrix profile and the current \\nbest-so-far mat rix profile. As Figure 5 suggests, with an \\nexperiment on random walk data, the algorithm converges very \\nquickly. \\nQ2T1 0 00 00 00 00 0Q1T4Q2T2+Q1T1 Q2T3+Q1T2 Q2T4+Q1T3\\nOutput\\nInputT2T1 T4T3 00 00 Q1Q2 00 00 00'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Figure 5. main) The decre ase in RMSE as the STAMP algorithm updates \\nmatrix profile with the distance profile calculated at each iteration.  inset) The \\napproximate matrix profile at the 10% mark is visually indistinguishable from \\nthe final matrix profile. \\nZilberstein [30] gives a number of desirable properties of \\nanytime algorithms, including Low Overhead , Interruptibility, \\nMonotonicity, Recognizable Quality, Diminishing Returns and \\nPreemptability (these properties are mostly obvious from their \\nnames, but full definitions are at [30]). \\nBecause each subsequence‚Äôs distance profile is bounded \\nbelow by the exact matrix profile, updating an approximate \\nmatrix profile with a distance profile with pairwise minimum \\noperation either drives the approximate solution closer the exact \\nsolution or retains the current approximate solution. Thus , we \\nhave guaranteed Monotonicity. From Figure 5, the approximate \\nmatrix profile converges to the exact matrix profile \\nsuperlinearly; therefore, we have strong Diminishing Returns. \\nWe can easily achieve Interruptibility and Preemptability by \\nsimply inserting a few lines of code between lines 5 and 6 of \\nTABLE III that read: \\n5new \\n6new \\n7new \\nif CheckForUserInterrupt  = TRUE \\n          Report({PAB, IAB}, ‚ÄòHere is an approximate answer.‚Äô) \\nif GetUserChoice = ‚Äòfurther refine‚Äô, CONTINUE, else BREAK \\nThe space and time overhead for the anytime property is \\neffectively zero, thus we have Low Overhead. This leaves only \\nthe property of Recognizable Quality. Here we must resort to a \\nprobabilistic argument.  The convergence curve shown in  \\nFigure 5 is very typical, so we could use past convergence \\ncurves to predict the quality of solution when interrupted on \\nsimilar data.  \\nD. Time and Space Complexity \\nThe overall complexity of the proposed algorithm is \\nO(n2logn) where n is the length of the time series. However, our \\nexperiments (see Section 4.1) empirically suggest that the \\nruntime of  STAMP‚Äôs growth rate is roughly O( n2) instead of \\nO(n2logn). One possible e xplanation for this is that the nlogn \\nfactor comes from the FFT subroutine. Because FFT is so \\nimportant in many applications, it is extraordinarily well \\noptimized. Thus, the empirical runtime is very close to linear. \\nIn contrast to the above, the brute for ce nested loop \\napproach has a time complexity of O(n2m). Recall the industrial \\nexample in the introduction section. We have  m = 10,080, but \\nlog(n) = 5.7, so we would expect our approach to be about \\n1,768 times faster. In fact, we are empirically even faster. The \\ncomplexity analysis downplays the details of important constant \\nfactors. The nested loop algorithm must also z -normalize the \\nsubsequences. This either requires O( nm) time, but with an \\nuntenable O(nm) space overhead, or an O( n2m) time overhead. \\nAnd r ecall that this is before a single Euclidean distance \\ncalculation is performed.  \\nFinally, we mention one quirk of our algorithm which we \\ninherit from using the highly optimized FFT subroutine. Our \\nalgorithm is fastest when n is an integer power of two, slower \\nfor non -power of two but composite numbers, and slowest \\nwhen n is prime. The difference (for otherwise similar values of \\nn) can approach a factor of 1.6x. Thus, where possible, it is \\nworth contriving the best case by tru ncation or zero-padding to \\nthe nearest power of two. \\nE. Incrementally Maintaining TSAPSS \\nUp to this point we have discussed the batch version of \\nTSAPSS. By batch, we mean that the STAMP algorithm needs \\nto see the entire time series TA and TB (or just TA if we are \\ncalculating the self -similarity join matrix profile) before \\ncreating the matrix profile. However, it would be advantageous \\nif we could build the matrix profile incrementally. Given that \\nwe have performed a batch construction of matrix profile, i f \\nnew data arrives, it would clearly be preferable to incrementally \\nadjust the current profile, rather than start from scratch.  \\nBecause the matrix profile solves both the times series motif \\nand the time series discord problems, an incremental version of \\nSTAMP would automatically provide the first incremental \\nversions of both algorithms. We call such an algorithm the \\nSTAMPI (STAMP Incremental) algorithm. \\nWe will demonstrate our ability to incrementally maintain \\nthe matrix profile in this section. For simpli city and brevity \\nTABLE IV only shows the algorithm to maintain the self -\\nsimilarity join. The generalizations are obvious. \\nTABLE IV. THE STAMPI ALGORITHM \\nProcedure STAMPI(TA, t, PAA, IAA) \\nInput: The original time series TA, a new data point t following TA, the \\nmatrix profile PAA and its associated matrix profile index IAA of TA.  \\nOutput: The incrementally updated matrix profile PAA,new and its matrix \\nprofile index IAA,new of the current time series TA,new= TA, t. \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nTA,new = [TA, t] \\nS ‚Üê last subsequence in TA,new, idx ‚Üê index of S in TA,new \\nD ‚Üê MASS (S, TA) \\nPAA, IAA ‚Üê ElementWiseMin(PAA, IAA, D, idx) \\npAA,last, iAA,last ‚Üê FindMin(D) \\nPAA,new ‚Üê [PAA, pAA,last], IAA,new ‚Üê [IAA, iAA,last] \\nreturn PAA,new, IAA,new \\nFor clarity, we denote the updated time series as TA,new, the \\nupdated matrix profile as PAA,new and the associated matrix \\nprofile index as IAA,new. As each additional data point t arrives, \\nthe size of the time series TA increases by one, and a new \\nsubsequence S is generated at the end of TA,new. In line 3 we \\nobtain the distance profile of S with regard to TA. Then, as in the \\noriginal STAMP algorithm,  in line 4 we perform  a pairwise \\ncomparison between every element in D with the corresponding \\nelement in PAA to see if the corresponding element in PAA needs \\nto be updated. In line 5, we find the nearest neighbor of S and \\nthe associated index by evaluating the minimum value of D. \\nFinally, in line 6, we obtain the new matrix profile and \\nassociated matrix profile index by concatenating the results in \\nline 4 and line 5.  \\nThe time complexity of the STAMP I algorithm is O(nlogn) \\nwhere n is the length of size of th e current time series TA. Note \\nthat as we maintain the profile, each incremental call of \\nSTAMPI requires invoking the FFT subroutine with a slightly \\nlonger time series ( n becomes n+1 ). Thus it gets very slightly \\nslower at each time step. Therefore, the best way to measure the \\nperformance is to ask for the Maximum Time Horizon (MTH), \\nin essence the answer to this question: ‚Äú Given this arrival rate, \\nhow long can we maintain the profile before we can no longer \\nupdate fast enough?‚Äù \\nNote that the subsequence length m is not considered in the \\nMTH evaluation. Recall that overall time complexity of the \\n10,0000\\nRMSE\\niteration1,000\\napproximate matrix \\nprofile at 1,000 iterations.\\nexact matrix profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='algorithm is determined by the efficiency of the FFT \\nsubroutine, which is independent of m. We have computed the \\nMTH for two common scenarios of interest to the community. \\n\\uf0b7 House Electrical Demand  [19]: This dataset is updated \\nevery eight seconds. By iteratively calling the STAMP I \\nalgorithm, we can maintain the profile for 5.8 years.  \\n\\uf0b7 Oil Refinery :  Most telemetry in oil refineries is sampled \\nat once a minute [26]. The relatively low sampling rate \\nreflects the ‚Äúinertia‚Äù of massive boilers/condensers. Even if \\nwe maintain the profile for 40 years, the update time is only \\naround 6.78 seconds. Moreover, th e raw data, matrix \\nprofile and index would only require 0.5 gigabytes of main \\nmemory. Thus the MTH here is forty plus years. Given \\nprojected improvements in hardware, this effectively \\nmeans we can maintain the matrix profile forever. \\nAs impressive as these  numbers are, they are actually quite \\npessimistic. For simplicity we assume that every value in the \\nmatrix profile index will be updated at each time step.  \\nHowever, empirically, much less than 0.1% of them need to be \\nupdated. If it is possible to prove an upper bound on the number \\nof changes to the matrix profile index per update, then we could \\ngreatly extend the MTH  or handle much faster sampling rates. \\nWe leave such considerations for future work. \\nIV. EXPERIMENTAL EVALUATION \\nWe begin by stating our experimen tal philosophy. We have \\ndesigned all experiments such that they are easily reproducible. \\nTo this end, we have built a webpage [24] which contains al l \\ndatasets and code used in this work, together with spreadsheets \\nwhich contain the raw numbers and some supporting videos.  \\nGiven page limits, and the utility of our algorithm for a host \\nof existing (and several new) data mining tasks, we have chosen \\nto c onduct exceptionally broad  but shallow experiments. We \\nhave conducted such deep detailed experiments and placed \\nthem at [24]. Unless otherwise state d we measure wall clock \\ntime on an Intel i7@4GHz with 4 cores. \\nA. Scalability of Profile-Based Self-Join \\nBecause the time performance of STAMP is independent of \\nthe data quality or any user inputs (there are none except the \\nchoice of m, which does not affect the speed), our scalability \\nexperiments are unusually brief.  In TABLE V we show the \\ntime required for a self -join with m fixed to 256, for \\nincreasingly long time series.  \\nTABLE V.  TIME REQUIRED FOR A SELF-JOIN WITH M = 256, VARYING N \\nValue of n 217 218 219 220 221 \\nTime Required 15.1 min 70.4 min 5.4 hours 24.4 hours 4.2 days \\nIn TABLE VI, we show the time required for a self -join \\nwith n fixed to 2 17, for increasing long m. Again recall that \\nunlike virtually all other time series data mining algorithms in \\nthe literature whose performance degrades for longer \\nsubsequences [8][18], the running time of STAMP does not \\ndepend on m. \\nTABLE VI. TIME REQUIRED FOR A SELF-JOIN WITH N= 217, VARYING M \\nValue of m 64 128 256 512 1,024 \\nTime Required 15.1 min 15.1 min 15.1 min 15.0 min 14.5 min \\nFinally, we further exploit the simple parallelizability of the \\nalgorithm by using four 16 -core virtual machines on Microsoft \\nAzure to redo the two -million join ( n = 2 21 and m = 256) \\nexperiment. By scaling up the computational power, we have \\nreduced the running time from 4.2 days to just 14.1 hours. This \\nuse of cloud computing required writing just few dozen lines of \\nsimple additional code [24]. \\nIt is difficult to find good baselines to compare to. We \\nbelieve STAMP is the only algorithm that does full, exact joins \\non time series subsequences. Most algorithms do only threshold \\njoins and/or do approximate joins, and are not specialized for \\ntime series subsequences. However, we searched for the best \\nbaselines and made the following concessions to  the rival \\nalgorithms to allow comparisons. \\n\\uf0b7 TSFRDAA [11]: While STAMP returns all nearest \\nneighbors, we adjust the threshold (the selectively) of \\nTSFRDAA such that only needs to return the top 1% nearest \\nneighbors, a much easier task. \\n\\uf0b7 HDSJI-SAX [12]: This method  allows false negatives ; we \\nignore this. It needs time to build indexes ; we do not count \\nthis time, and as above, we allow it to return  the only the \\ntop 1% nearest neighbors ( as bef ore, not the 100% the \\nSTAMP returns, and therefore a much easier task.). \\n\\uf0b7 Optimized Nested Loop (ONL): Here we use a nested -loop \\njoin optimized in the following manner. We use a state -of-\\nthe-art DFT indexing technique to do the search in the \\ninner loop, and we do not count the time needed to build \\nthe indexes [8]. We carefully adjust  parameters for best \\nperformance.  \\nWe consider the first 2 18 data points of the ECG dataset \\nused in [22], with a query length of 256, about one heartbeat ; \\nTABLE VII shows the results.  \\nTABLE VII. TIME FOR A SELF-JOIN WITH M = 256, VARYING ALGORITHMS \\nAlgorithm TSFRDAA HDSJI-SAX ONL STAMP \\nTime Required 51.7 hours 19.6 min 28.1 hours 1.17 hours \\nAs these results show, even if we ignore the limitations of \\nthe baseline methods, STAMP is still significantly faster. \\nB. Profile-Based Self-Join \\nA recent paper notes that many fundamental problems in \\nseismology can be solved by joining seismometer telemetry \\n[28], including the disc overy of foreshocks, aftershocks, \\ntriggered earthquakes, volcanic activity and induced seismicity. \\nHowever, the paper notes a join with a query length of 200 on a \\ndata stream of length 604,781 requires 9.5 days. Their solution, \\na clever transformation of t he data to allow LSH based \\ntechniques, does achieve significant speedup, but at the cost of \\nfalse negatives and the need for significant parameter tuning.  \\nThe authors kindly shared their data, and, as we hint at in \\nFigure 6, confirmed that STAMP does not have false negatives. \\n \\nFigure 6. top) An excerpt of a seismic time series aligned with its matrix \\nprofile (bottom). The ground truth provided by the authors of [28] requires that \\nthe events occurring at time 4,050 and 7,800 match. \\nWe repeated the n = 604,781, m = 200 experime nt and \\nfound it took just 8.9 hours to finish. As impressive as this is, in \\nthe next section we show that we can do even better.   \\n4,000 5,000 6,000 7,000 8,000 9,000\\n0\\n5\\n10\\n15\\nSeismic time series (excerpt) \\nMatrix Profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='C. The Utility of Anytime STAMP \\nThe seismology dataset offers an excellent opportunity to \\ndemonstrate the utility of the anytime version of our algorithm. \\nThe authors of [28] revealed in their long -term ambition of \\nmining even larger datasets [3]. In Figure 7 we repeated the \\nexperiment with the snippet shown in Figure 6, this time \\nreporting the best-so-far matrix profile reported by the \\nalgorithm at various milestones. Even with just 0.25% of the \\ndistances computed (that is to say, 400 times faster) the correct \\nanswer has emerged. Thus, we can provide the correct answers \\nto the seismologists in just minutes, rather than the 9.5 days.  \\n \\nFigure 7. top) An excerpt of the seismic data that is also shown in Figure 6. \\ntop-to-bottom) The approximations of the matrix profile for increasing \\ninterrupt times. By the time we have computed just 0.25% of the calculations \\nrequired, the minimum of the matrix profile points to the ground truth. \\nTo show the generality of this anytime feature of STAMP, \\nwe consider a very different dataset. As shown in Figure \\n8.inset), it is possible to convert DNA to a time ser ies [22]. We \\nconverted the Y-chromosome of the Chimpanzee this way. The \\nresulting time series is  little over one -million in length. We \\nperformed a self-join with m = 60,000.  Figure 8.bottom shows \\nthe best motif is so well conserved (ignoring the first 20%), that \\nit must correspond to a recent (in evolutionary tim e) gene \\nduplication event. In fact, in a subsequent analysis we \\ndiscovered that ‚Äúmuch of the Y (Chimp chromosome) consists of \\nlengthy, highly similar repeat units, or ‚Äòamplicons‚Äô‚Äù [9]. \\n \\nFigure 8. top) The Y -chromosome of the Chimp in time series space with its \\nmatrix profile. bottom) A zoom -in of the top motif discovered using anytime \\nSTAMP, we believe it to be an amplicon [9]. \\nThis demanding join would take just over a day of CPU \\ntime (see TABLE V). However, using anytime STAMP we \\nhave the result shown above after doing just 0.021% of the \\ncomputations, in about 18 seconds. At [24] we have videos that \\nshow the rapid convergence of the anytime variant of STAMP. \\nD. Profile-Based Similarity Join Set \\nIn this section we show two uses of similarity join set. The \\nfirst use is more familiar to APSS users, quantifying what is \\nsimilar between two time series. The second example, \\nquantifying what is different between two time series, is novel  \\nand can only be supported by threshold -free algorithms that \\nreport the nearest neighbor for all objects. \\nTime Series Set Similarity \\nGiven two (or more) time series collected under different \\nconditions or treatments, a data analyst may wish to know what \\npatterns (if any) are conserved between the two time series. To \\ndemonstrate the utility of automating this, we consider a simple \\nbut in tuitive example. Figure 9 shows the raw audio of two \\npopular songs converted to Mel Frequency Cepstral \\nCoefficients (MFCCs). Specifically, the songs used in this \\nexample are ‚ÄúUnder Pressure‚Äù by Queen an d David Bowie and \\n‚ÄúIce Ice Baby ‚Äù by the American rapper Vanilla Ice. Normally, \\nthere are 13 MFCCs; here, we consider just one for simplicity. \\n \\n \\nFigure 9. Two songs represented by just the 2 nd MFCC at 100Hz. We \\nrecognize that it is difficult to see any structure in these times series; however, \\nthis difficulty is the motivation for this experiment. \\nEven for these two relatively short time series, visual \\ninspection does not offer immediate answers. The problem here \\nis compounded by the size reproduction, but is not significantly \\neasier with large-scale [24] or interactive graphic tools. We ran \\nJAB (Queen-Bowie, Vanilla Ice) on these datasets with m = 500 \\n(five seconds), the best match, corresponding the minimum \\nvalue of the matrix profile PAB, is shown in Figure 10. \\n \\nFigure 10. The result of JAB (Queen-Bowie (red/bold), Vanilla-Ice (green/fine)) \\nproduces a strongly conserved five second region. \\nReaders may know the cause for this highly conserved \\nsubsequence. It corresponds to the famous baseline of ‚ÄúUnder \\nPressure,‚Äù which was sampled (plagiarized) by Vanilla Ice in \\nhis song. The join took 21.9 seconds. The ability to find \\nconserved structure in apparently disparate time series could \\nopen many avenues of research in medicine and industry. \\nTime Series Difference \\nWe introduce the Time Series Diff (TSD) operator, which \\ninformally asks ‚ÄúWhat happens in time series T A, that does not \\nhappen in time series TB?‚Äù Here TA/TB could be an ECG before \\na drug is administered/after a drug is administered, telemetry \\nbefore a successful launch/before a catastrophic launch etc. The \\nTSD is simply the subsequence referred to by the maximum \\nvalue of the JAB join‚Äôs profile PAB. \\nWe begin with a simple intuitive example. The UK and US \\nversions of the Harry Potter audiobook series are performed by \\ndifferent narrators, and have a handful of differences in the text. \\nFor example, the UK version contains: \\nHarry was passionate about Quidditch. He had played as Seeker on the Gryffindor \\nhouse Quidditch team ever since his first year at Hogwarts and owned a Firebolt, one \\nof the best racing brooms in the world... \\nBut the corresponding USA version has: \\nHarry had been on the Gryffindor House Quidditch team ever since his first year at \\nHogwarts and owned one of the best racing brooms in the world, a Firebolt. \\nAs shown in Figure 11, we can convert the audio \\ncorresponding to these snippets into MFCCs and invoke a JAB \\njoin set to produce a matrix profile PAB that represents the \\ndifferences between them. As Figure 11.left shows, the low \\nvalues of this profile correspond to identical spoken phrases \\n(despite having two different narrators). However here we are \\n4,000 5,000 6,000 7,000 8,000 9,000\\ndata\\n0.25%\\n1%\\n100%\\n0 1,000,0000\\n100\\n200\\nPan troglodytes Y-chromosome \\n0 60,000\\n12,749,475 to 14,249,474 bp\\n622,725 to 2,122,724 bp\\nT1 = 0;\\nfor i = 1 to length(chromosome) \\nif chromosomei = A, then Ti+1 = Ti + 2 \\nif chromosomei = G, then Ti+1 = Ti + 1\\nif chromosomei = C, then Ti+1 = Ti - 1 \\nif chromosomei = T, then Ti+1 = Ti - 2 \\nend\\n0\\nQueen-Bowie\\n1,000 2,000\\n-10\\n0\\n10\\nVanilla Ice\\n0 250 500-3\\n-2\\n-1\\n0\\n1\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='interested in the differences, the maximum value of the profile. \\nAs we can see in Figure 11.right, here the profile corresponds \\nto a phrase unique to the USA edition.  \\n \\nFigure 11. The 2 nd MFCC of snippets from the USA ( pink/bold) and UK \\n(green/fine) Harry Potter audiobooks.  The JAB join of the two longer sections \\nin the main text produce s mostly small values in the profile correspond to the \\nsame phrase (left), the largest value in  the profile corresponds to a phrase \\nunique to the USA edition (right). \\nThe time required to do this is just 0.067 seconds, much \\nfaster than real time. While this demonstration is trivial, in [24] \\nwe show an example applied to ECG telemetry.  \\nE. Profile-Based Motif Discovery \\nSince their introduction in 2003, time series motifs have \\nbecome one of the most frequently used primitives in time  \\nseries data mining, with applications in dozens of domains [2]. \\nThere are several proposed definitions for time series motifs, \\nbut in [18] it is argued that if you can solve the most basic \\nvariant, the closest (non -trivial) pair of subsequences, then all \\nother variants only require some minor additional calculations. \\nNote that the locations of the two (tying) minimum values of \\nthe matrix profile are exactly the locations of the 1st motif pair. \\nThe fastest known exact algorithm for computing time \\nseries motifs is the MK algorithm [18]. Note, however, that this \\nalgorithm‚Äôs time performance depends on the time series itself. \\nIn contrast, the Profile -Based Motif Discovery (PBMD) takes \\ntime independent of the data. To see this, we compared the two \\napproaches on an electrocardiogram of length 65,536. In Figure \\n12.left we ask what happens as we search for lon ger and longer \\nmotifs. In Figure 12.right we ask what happens if the motif \\nlength is fixed to m = 512, but the data becomes increasing \\nnoisy.  \\n \\nFigure 12. The time required to find the top -motif pairs in a time series of \\nlength 216 for increasingly long motif lengths ( left), and for a length fixed to \\n512, but in the face of increasing noise levels (right). \\nThese results show that  even in the best case for MK, \\nPBMD is competitive, but as we have longer queries and/or \\nnoisier data, its advantage becomes unassailable. Moreover, \\nPBMD inherits STAMP‚Äôs anytime and incremental \\ncomputability, and is easily parallelizable.  \\nF. Profile-Based Discord Discovery \\nA time series discord  is the subsequence that has the \\nmaximum distance to its nearest neighbor. While this is a \\nsimple definition, time series discords are known to be very \\ncompetitive as novelty/anomaly detectors  [5]. Note that as \\nshown in Figure 13, the time series discord is encoded as the \\nmaximum value in a matrix profile. \\n \\nFigure 13. top) An excerpt from an ECG incorporating a premature ventricular \\ncontraction (red/bold). bottom) The time series profile peaks exactly at the \\nbeginning of the PVC. \\nThe time taken to compute the discord  is obviously just the \\ntime needed to compute the matrix profile ( here, 0.9 seconds). \\nThere are a few dozen discord discovery algorithms in the \\nliterature. Some of them may be competitive in the best case, \\nbut just like motif -discovery algorithms they all degenerate to \\nbrute force search in the worst case, and none allow the anytime \\nproperties that we inherit from using STAMP.  \\nG. Incrementally Maintaining Motifs and Discords \\nWe have demonstrated the ability to detect time series \\nmotifs and discords using the matrix profile in the previous two \\nsections. However, we assumed that the entire time series was \\navailable beforehand. Here we remove this assumption and \\nshow how STAMP I allows us to incrementa lly maintain time \\nseries motifs/ discords in an online fashion. There are other \\nattempts at one [20][2] or both [25] of these tasks, but they are \\nall approximate and allow false dismissals.   \\nIn Section III.E, we introduced the STAMPI algorithm. The \\nability to incrementally maintain the matrix profile implies the \\nability to exactly maintain the time series motif [18] and/or time \\nseries discord [5] in streaming data. We simply need to keep \\ntrack of the extreme values of the incrementally-growing matrix \\nprofile, report a new pair of motifs when a new minimum value \\nis detected, and report a new discord when we see a new \\nmaximum value. \\nWe demonstrate the utility of these ideas on the AMPds \\ndataset [13]. Here the kitchen fridge and the heat pump are both \\nplugged into a single metered power supply. For the first week, \\nonly the refrigerator is running. At the end of the week, the \\nweather gets cold and the heat pump is turned on. The sampling \\nrate is one sample/m inute, and th e subsequence length is 100. \\nWe apply the STAMP algorithm to the first three days of data, \\nthen invoke STAMP I to handle newly arriving data , report an \\nevent when we detect a new extreme value. \\nOur first event occurs at the 9,864 th minute (6 da y 20 hour \\n24 minute). As shown in Figure 14, a new minimum value is \\ndetected, which indicates a new time series motif. The just -\\narrived 100 -minute-long pattern looks v ery similar to another \\npattern that occurred five hours earlier. While there is a lot of \\nregularity in the fridge data in general, the exceptional \\nsimilarity observed here suggested some underlying physical \\nmechanism that caused such a perfectly -conserved pattern, \\nperhaps a mechanical ice-making ‚Äúsubroutine.‚Äù \\n \\nFigure 14. top) The matrix profile of the first 9,864 minutes of data. bottom) \\nThe minimum value of the matrix profile corresponds to a pair of time series \\nmotifs in the power usage data. right) The time series motif detected. \\nOur second event occurs at the 10,473 th minute (7 day 6 \\nhour 33 minute). As shown in Figure 15, a new maximum value \\n‚Ä¶indor house Quidditch team ever since his first ye‚Ä¶\\nHarry had been on the Gryffindor House Quidditch te..\\nsince his first year at Hogwarts and owned a Fire..\\nsince his first year at Hogwarts and owned on..\\nED = 2.9 \\nClosest Match \\n0 100(1.6 seconds) 0 100\\nED = 10.4\\n(1.6 seconds)\\nFurthest Match (Time Series Difference) \\n256 512 1,024 2,048 4,096\\n0\\n40\\n80\\n120\\n160\\n200\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nSubsequence Length  (m)\\nTime Taken (min)\\n80 40 0\\n0\\n70\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nNoise Added (dB)  \\n0 1000 2000 3000\\nECG qtdb\\nSel102\\n(excerpt)\\nPremature Ventricular \\nContraction\\nTime Series Profile\\n0 5,000 10,000\\n0 100\\nnew minimum value\\nnew motifMatrix Profile\\nPower Usage Data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='is detected, which indicates a new time series discord. The time \\nseries discord corresponds to the first occurrence of a heat \\npump pattern in the power usage data.  \\n \\nFigure 15. top) The matrix profile for the first 10,473 minutes. bottom) The \\nmaximum value of the matrix profile corresponds to a time series discord. \\nright) The time series discord detected is the first  heat pump pattern \\noccurrence in the dataset. \\nThe maximum time needed to process a single data point \\nwith STAMP I in this dataset is 0.005 second s, which is less \\nthan 0.01% of the data sampling rate. Thus, on this dataset we \\ncould continue monitoring with the STAMP I algorithm for \\nseveral decades before running out of time or memory. \\nH. Profile-Based Shapelet Discovery \\nShapelets are time series subsequences which are in some \\nsense the maximally representative of a class [23][27]. \\nShapelets can be used to classify time series (essentially, the \\nnearest shapelet algorithm), offering the benefits of speed, \\nintuitiveness and at least on some domains, significantly \\nimproved accuracy [23]. However, these advantages come at \\nthe cost of a very expensive training phase, with O( n2m4) time \\ncomplexity, where m is the length of the longest time series \\nobject in the dataset, and n is the number of objects in the \\ntraining set. In order to mitigate this high ti me complexity, \\nresearchers have proposed various distance pruning techniques \\nand candidate ranking approaches for both the admissible [27] \\nand appro ximate [23] shapelet discovery. Nevertheless, \\nscalability remains the bottleneck.  \\nBecause shapelets are essentially supervised motifs, and we \\nhave shown that STAMP can find motifs very quickly, it is \\nnatural to ask if STAMP has implications for shapelet \\ndiscovery. While space limitations prohibit a detailed \\nconsideration of this question, we briefly sketch out and test \\nthis possibility as follows. \\nAs shown in Figure 16, we can use matrix profile to \\nheuristically ‚Äúsuggest‚Äù candidate shapelets. We consider two \\ntime series TA (green/bold) and TB (pink/light) with class 1 and \\nclass 0 being their corresponding class label, and we take  JAB, \\nJAA, JBA and JBB. Our claim is the differences in the heights of \\nPAB, PAA (or PBA, PBB) are strong indicators of good candidate \\nshapelets. The intuition is that if a discriminative pattern is \\npresent in say, class 1, but not in class 0, then we expect to see a \\n‚Äúbump‚Äù in the PAB (the intuition holds if the order is reversed). \\nA significant difference (quantified by a threshold shown in \\ndashed line) between the heights of PAA and PAB curves \\ntherefore indicates the occurrence of good candidate shapelets, \\npatterns that only occur in one of the two classes. \\n \\nFigure 16. top.left) Two time series TA and TB formed by concatenating \\ninstances of class 1 and 0 respectively of ArrowHead [6]. bottom) The height \\ndifference between PAB (or PBA) and PAA (or PBB) are suggestive of  good \\nshapelets. top.right) An example of good shapelet extracted from class 1. \\nThe time taken to compute  all four matrix profiles is 1.0 \\nseconds and the time to further evaluate the two twelve \\ncandidates selected takes 2.7 seconds. On the same machine, \\nthe brute force shapelet classifier takes 4.2 minutes with 2,364 \\ncandidates. Note that, in this toy demonst ration, the speedup is \\n68X, however, for larger datasets, the speedup is greater [24]. \\nI. Profile-Based Semantic Segmentation \\nThe goal of time series semantic segmentation is to partition \\na dataset containing multiple activities/regimes into atomic \\nbehaviors or conditions. For example, for human activity data \\nthe regimes might later be mapped to { eating, working, \\ncommuting,...}[29]. As this example suggests, most work in this \\narea is highly domain -dependent. In contrast, here we show \\nhow the matrix profile index can be emp loyed for domain \\nagnostic time series segmentation.  \\nThe intuition of our approach is as follows. Within a single \\nregime we might expect that most subsequences will have a \\nnearest neighbor close by (in time). For example, consider the \\ntoy problem shown in Figure 17 which shows two obvious \\nregimes. We would expect that the nearest neighbor to the first \\nrun gait cycle is the second or third or fourth run cycle, but it \\nwill almost certainly not be one of the walk cycles. In general, \\nthis tendency for nearest neighbor pointers not to cross the \\nboundaries corresponding to regime changes may be sufficient \\nto discover these boundaries, and of course, this is precisely the \\ninformation that is encoded in the matrix profile index. \\nThus, for each point we count how many ‚Äúarcs‚Äù connecting \\ntwo nearest neighbors cross it if we connect each subsequence \\nto its nearest neighbor as shown in Figure 17. \\n \\nFigure 17. top) A (toy) time series ( red) and nearest neighbor locations for \\neach subsequence. bottom) The number of arcs crossing above each \\nsubsequence. \\nWe applied the procedure described above to a heavily \\nstudied activity segmentation problem [29], which was derived \\nfrom the CMU Motion Capture data base [16]. The recordings \\nare represented as multi -dimensional time series, and most \\nresearch efforts carefully select the best subset for the \\nsegmentation task. For example, [29] states that ‚Äú we only \\nconsider the 14 most informative joints out of 29 .‚Äù In contrast, \\nwe attempt this with a single dimension. The only parameter we \\nneed to set is sliding window size . In Figure 18 we show the \\nsegmenting results obtained using our approach , with  \\nannotations taken from [29] for context. \\nMuch of the evaluation in this community lacks formal \\nmetrics, preferring instead visual sanity tests like the one in \\nFigure 18. Given this, we can say that our approach is very \\ncompetitive on this dataset, in spite of the fact that we \\nhandicapped ourselves to only consider one dimension.  \\n0 100\\n0 10,0005,000\\nMatrix Profile\\nPower Usage Data\\nnew maximum value\\nnew discord\\n0\\n7\\n0 1,400\\nPABPAA\\n0 1,600\\nTA\\nTB\\n0 300\\nAn example of good shapelet\\n0 1,400\\nPBA\\nPBB\\n8\\n0\\n0 1,400\\nDiff(PAB ,PAA) Threshold\\n-1\\n4\\n-2\\n4\\n0 1,400\\nDiff(PBA ,PBB) Threshold\\n0\\n1\\n2\\n1 2 0\\nwalking slow    walking slow     run       run run run\\nNumber of arcs intersecting each point'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content=\"Figure 18. top) A matrix profile ( blue) obtained for the time series ( red) and \\nnumber of arcs crossing each point ( green). Low values of this green curve \\ncoorespond to candidate split points. bottom) Human annotations of the \\nactivities: w ‚Äì walk, s ‚Äì stretch, p ‚Äì punch, c ‚Äì chop, t ‚Äì turn, d ‚Äì drink. \\nV. CONCLUSION \\nWe have introduced a scalable algorithm for creating time \\nseries subsequences joins. Our algorithm is simple, fast, \\nparallelizable and parameter -free, and can be in crementally \\nupdated for moderately fast data arrival rates. We have shown \\nthat our algorithm has implications for many existing tasks, \\nsuch as motif discovery, discord discovery, shapelet discovery \\nand semantic segmentation, and may open up new avenues for  \\nresearch, including computing various definitions of time series \\nset difference. Our code is freely available for the community to \\nconfirm, extend and exploit our ideas. \\nREFERENCES \\n[1] R. J. Bayardo, Y. Ma and R. Srikant, ‚ÄúScaling up all pairs similarity \\nsearch,‚Äù WWW 2007, pp 131-140. \\n[2] N. Begum and E. Keogh, ‚ÄúRare time series motif discovery from \\nunbounded streams,‚Äù PVLDB 8(2): 149-160, 2014. \\n[3] G. Beroza, ‚ÄúPersonal Correspondence,‚Äù Jan 21th, 2016. \\n[4] T. Bouezmarni and J. Rombouts, ‚ÄúNonparametric density estimation for \\npositive time series,‚Äù CSDA, 54, 245-261, 2010.  \\n[5] V. Chandola, D. Cheboli and V. Kumar, ‚ÄúDetecting anomalies in a time \\nseries database,‚Äù UMN TR09-004. \\n[6] T. Chen  et al ., ‚ÄúThe UCR time series classification archive,‚Äù \\nhttp://www.cs.ucr.edu/~eamonn/time_series_data/. \\n[7] ‚ÄúConvolution - Wikipedia, the free encyclopedia,‚Äù  \\nhttps://en.wikipedia.org/wiki/Convolution, Accessed: 2016-01-19. \\n[8] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang and E. J. Keogh, \\n‚ÄúQuerying and mining of time series data: experimental comparison of \\nrepresentations and distance measures,‚Äù PVLDB 1(2): 1542-1552. 2008. \\n[9] J. Hughes et al. , ‚ÄúChimpanzee and human Y chromosomes are \\nremarkably divergent in structure‚Äù Nature 463, (2010). \\n[10] H. Lee, R. Ng and K. Shim, ‚ÄúSimilarity join size estimation using \\nLocality sensitive hashing,‚Äù PVLDB, 4(6):338‚Äì349, 2011. \\n[11] W. Luo, H. Tan, H. Mao  and L. M.  Ni, ‚ÄúEfficient similarity joins on \\nmassive high-dimensional datasets using mapreduce,‚Äù In MDM'12, IEEE \\nComputer Society, pp. 1-10. \\n[12] Y. Ma, X. Meng and S. Wang, ‚ÄúParallel similarity joins on massive high-\\ndimensional data using MapReduce ,‚Äù Concurrency and Computation , \\nVolume 28, Issue 1 Jan 2016. Pages 166‚Äì183. \\n[13] S. V. Makonin, ‚ÄúAMPds: a public dataset for load disaggregation and \\neco-feedback research,‚Äù EPEC 2013, pp 1-6. \\n[14] MASS: http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html \\n[15] G. D. F. Morales and A. Gionis, ‚Äústreaming similarity self-join,‚Äù Proc. \\nVLDB Endow, 2016. \\n[16] Motion Capture Database, http://mocap.cs.cmu.edu/ \\n[17] A. Mueen, H. Hamooni and T. Estrada, ‚ÄúTime series join on subsequence \\ncorrelation,‚Äù IEEE ICDM 2014, pp. 450-459. \\n[18] A. Mueen, E. Keogh, Q. Zhu, S. Cash and B. Westover, ‚ÄúExact discovery \\nof time series motif,‚Äù SDM 2009. \\n[19] D. Murray et al. , ‚ÄúA data management platform for personalised real-\\ntime energy feedback,‚Äù In EEDAL 2015. \\n[20] V. Niennattrakul et al, ‚ÄúData editing techniques to allow the application \\nof distance-based outlier detection to streams,‚Äù ICDM 2010: 947-952. \\n[21] D. Patnaik, et al, ‚ÄúSustainable operation and management of data center \\nchillers using temporal data mining,‚Äù KDD 2009. \\n[22] T. Rakthanmanon et al., ‚ÄúSearching and Mining Trillions of Time Series \\nSubsequences under Dynamic Time Warping,‚Äù In KDD 2012, 262-270. \\n[23] T. Rakthanmanon and E. Keogh, ‚ÄúFast shapelets: a scalable algorithm for \\ndiscovering time series shapelets,‚Äù SDM, 2013. \\n[24] Supporting page. http://www.cs.ucr.edu/~eamonn/MatrixProfile.html \\n[25] C. D. Truong and D. T.  Anh, ‚ÄúAn Efficient Method for Motif and \\nAnomaly Detection in Time Series‚Äù IJBIDM, Vol. 10, No. 4, 2015. \\n[26] A. Tucker and X. Liu, ‚ÄúA Bayesian  Network Approach to Explaining \\nTime Series with Changing Structure,‚Äù Intell Data Anal, 8 (5) (2004). \\n[27] L. Ye and E. Keogh, ‚ÄúTime series shapelets: a new primitive for data \\nmining,‚Äù ACM SIGKDD, 2009, pp 947-56. \\n[28] C. Yoon, O. O‚ÄôReilly, K. Bergen and G. Beroza, ‚ÄúEarthquake detection \\nthrough computationally efficient similarity search,‚Äù Sci. Adv. 2015. \\n[29] F. Zhou, F. Torre and J.  Hodgins ‚Äú Aligned Cluster Analysis for \\nTemporal Segmentation of Human Motion,‚Äù IEEE FG'2008. \\n[30] S. Zilberstein and S. Russell, ‚ÄúApproximate Reasoning Using Anytime \\nAlgorithms,‚Äù In Imprecise and Approximate Computation , Kluwer \\nAcademic Publishers, 1995. \\n \\n0 5,000 10,000\\nMatrix  Profile\\nSplit Points   \\nPrediction\\nOne-dimension of multi-d time series: Subject 86, recording 4, dimension 30\\nW S P N      W C T           D            P         W\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Matrix Profile I: All Pairs Similarity Joins for Time Series: \\nA Unifying View that Includes Motifs, Discords and Shapelets \\nChin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,  \\nHoang Anh Dau, ‚Ä†Diego Furtado Silva, ‚Ä°Abdullah Mueen, and Eamonn Keogh \\nUniversity of California, Riverside, ‚Ä†Universidade de S√£o Paulo, ‚Ä°University of New Mexico \\n{myeh003, yzhu015, lulan001, nbegu001, yding007, hdau001}@ucr.edu, diegofsilva@icmc.usp.br, mueen@unm.edu, eamonn@cs.ucr.edu \\n \\nAbstract‚Äî The all-pairs-similarity-search (or similarity join) \\nproblem has been extensively studied for text and a handful of \\nother datatypes. However, surprisingly little progress has been  \\nmade on similarity joins for time series subsequences. The lack of  \\nprogress probably stems from the daunting nature of the \\nproblem. For even modest sized data sets the obvious nested -loop \\nalgorithm can take months, and the typical speed -up techniques \\nin this domain (i.e., indexing, lower -bounding, triangular -\\ninequality pruning and early abandoning) at best produce one or \\ntwo orders of magnitude speedup. In this work we introduce a \\nnovel scalable algorithm for time series subsequence all -pairs-\\nsimilarity-search. For exceptionally large datasets, the algorithm \\ncan be trivially cast as an anytime algorithm and produce high -\\nquality approximate solutions in reasonable  time. The exact \\nsimilarity join algorithm computes the answer to the time series \\nmotif and time series discord  problem as a side -effect, and our \\nalgorithm incidentally provides the fastest known algorithm for \\nboth these  extensively-studied problems. We de monstrate the \\nutility of our ideas for many time series data mining problems, \\nincluding motif discovery, novelty discovery,  shapelet discovery,  \\nsemantic segmentation, density estimation, and contrast set \\nmining.    \\nKeywords‚ÄîTime Series; Similarity Joins; Motif Discovery \\nI. INTRODUCTION \\nThe all-pairs-similarity-search (also known as similarity \\njoin) problem comes in several variants. The basic task is this: \\nGiven a collection of data objects, retrieve the nearest neighbor \\nfor each object . In the text domain the  algorithm has \\napplications in a host of problems, including community \\ndiscovery, duplicate detection, collaborative filtering, \\nclustering, and query refinement [1]. While virtually all text \\nprocessing algorithms have analogues in time series data \\nmining, there has been surprisingly little progress on Time \\nSeries subsequences All-Pairs-Similarity-Search (TSAPSS). \\nWe believe that this lack of progress stems not from a lack \\nof interest in this useful primitive, but from the daunting nature \\nof the problem. Consider the following example that reflects the \\nneeds of an industrial collaborator. A boiler at a che mical \\nrefinery reports pressure once a minute. After a year, we have a \\ntime series of length 525,600. A plant manager may wish to do \\na similarity self-join on this data with week -long subsequences \\n(10,080) to discover operating regimes (summer vs. winter o r \\nlight distillate vs. heavy distillate etc.) The obvious nested loop \\nalgorithm requires 132,880,692,960 Euclidean distance \\ncomputations. If we assume each one takes 0.0001 seconds, \\nthen the join will take 153.8 days. The core contribution of this \\nwork is to show that we can reduce this time to 6.3 hours, using \\nan off-the-shelf desktop computer. Moreover, we show that this \\njoin can be computed and/or updated incrementally. Thus we \\ncould maintain this join essentially forever on a standard \\ndesktop, even if the data arrival frequency was much faster than \\nonce a minute. \\nOur algorithm uses an ultra -fast similarity search algorithm \\nunder z -normalized Euclidean distance as a subroutine, \\nexploiting the overlap between subsequences using the classic \\nFast Fourier Transform (FFT) algorithm.  \\nOur method has the following advantages/features: \\n\\uf0b7 It is exact, providing no false positives or false dismissals. \\n\\uf0b7 It is simple and parameter -free. In contrast, the more \\ngeneral metric space APSS algorithms require building and \\ntuning spatial access methods and/or hash functions.  \\n\\uf0b7 Our algorithm requires an inconsequential space overhead, \\njust O(n) with a small constant factor. \\n\\uf0b7 While our exact algorithm is extremely scalable, for \\nextremely large datasets we can compute the results in an \\nanytime fashion, allowing ultra-fast approximate solutions.  \\n\\uf0b7 Having computed the similarity join for a dataset, we can \\nincrementally upd ate it very efficiently. In many domains \\nthis means we can effectively maintain exact joins on \\nstreaming data forever.  \\n\\uf0b7 Our method provides full joins, eliminating the need to \\nspecify a similarity threshold, which as we will show, is a \\nnear impossible task in this domain.  \\n\\uf0b7 Our algorithm is embarrassingly parallelizable, both on \\nmulticore processors and in distributed systems. \\nGiven all these features, our algorithm has implications for \\nmany time series data mining tasks [5][18][28].  \\nThe rest of the paper is organized as follows. Section II \\nreviews related work an d introduces the necessary background \\nmaterials and definitions. In Section III we introduce our \\nalgorithm and its anytime and incremental variants. Section IV \\nsees a detailed empirical evaluation of our algorithm and shows \\nits implications for many data mining tasks. Finally, in Section \\nV we offer conclusions and directions for future work. \\nII. RELATED WORK AND BACKGROUND \\nThe basic variant of  similarity join problem we are \\ninterested in is as follows : Given a collection of data objects, \\nretrieve the nearest neighbor for every object.   \\nOther common variants include retrieving the top-K nearest \\nneighbors or the nearest neighbor for each object if that \\nneighbor is within a user-supplied threshold, œÑ. (Such variations \\nare trivial generalizations of our proposed algorithm, so we \\nomit them from further discussion). The latter variant results in \\na much easier problem, provided that the threshold is small. For \\nexample, [1] notes that virtually all research efforts ‚Äú exploit a \\nsimilarity threshold more aggressively in order to limit the set'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='of candidate pairs that are considered..  [or] ... to reduce the \\namount of information indexed in the first place.‚Äù \\nThis critical dependence on œÑ is a major issue for text joins, \\nas it is known that ‚Äú join size can change dramatically \\ndepending on the input similarity threshold ‚Äù [10]. However, \\nthis issue is even more critical for time series for two reasons. \\nFirst, unlike similarity (which is bounded between zero and \\none), the Euclidean distance is effectively unbounded, and \\ngenerally not intuitive. For example, if two heartbeats have a \\nEuclidean distance of 17.1, are they similar? Even for a domain \\nexpert that knows the sampling rate and the noise level of the \\ndata, this is not obvious. Second, a single threshold can produce \\nradically different output sizes, even for datasets that are very \\nsimilar.  Consider Figure 1 which shows the output size vs. \\nthreshold setting for the first and s econd halves of a ten -day \\nperiod monitoring data center chillers [21]. For the first five \\ndays a threshold of 0.6 would return zero items, but for the \\nsecond five days the same setting would return 108 items.  This \\nshows the difficulty in selecting  an appropriate threshold. Our \\nsolution is to have no threshold, and do a full join. \\n \\nFigure 1.  Output size vs. threshold for data center chillers [21]. Values beyond \\n2.0 are truncated for clarity (but archived at [24]).  \\nA handful of efforts have considered joins on time series, \\nachieving speedup by (in addition to the use of MapReduce) \\nconverting the data to lower -dimensional representations such \\nas PAA [11] or SAX [12] and exploiting lower bounds and/o r \\nLocality Sensitive Hashing (LSH) to prune some calculations. \\nHowever, the methods are very complex, with many (10 -plus) \\nparameters to adjust. As [11] acknowledges with admirable \\ncandor, ‚ÄúReasoning about the optimal settings is not trivial.‚Äù In \\ncontrast, our proposed algorithm has zero parameters to set. \\nA very recent research effort [28] has tackled the scalability \\nissue by converting the real -valued time series into discrete \\n‚Äúfingerprints‚Äù before using a LSH approach, much like the text \\nretrieval community [1]. They produced impressive speedup, \\nbut they also experienced false negatives. Moreover, the \\napproach has several parameters that need to be set; for \\nexample, they need to set the threshold to a very precise 0.818.  \\nAs we shall show, our algorithm allows both anytime and \\nincremental (i.e. streaming) versions. While a streaming join \\nalgorithm for text was recently introduced [15], we are not \\naware of any such algorithms for time series data or general \\nmetric spaces. More generally, there is a large amount of \\nliterature on joins for text processing [1]. Such work is \\ninteresting, but of little utility given our constraints, data type \\nand problem setting. We require full joins, not threshold joins, \\nand we are unwilling to allow the possibility of false negatives. \\nA. Definitions and Notation \\nWe begin by defining the data type of interest, time series: \\nDefinition 1:  A time series T is a sequence of real -valued \\nnumbers ti: T = t1, t2, ..., tn where n is the length of T. \\nWe are not interested in the global properties of time series, \\nbut in the similarity between local subsequences:  \\nDefinition 2:  A subsequence Ti,m of a T is a continuous \\nsubset of the values from T of length m starting from \\nposition i.   Ti,m = ti, ti+1,‚Ä¶, ti+m-1, where 1 ‚â§  i ‚â§  n-m+1.  \\nWe can take any subsequence from a time series and \\ncompute its distance to all sequences. We call an ordered vector \\nof such distances a distance profile: \\nDefinition 3:  A distance profile D is a vector of the \\nEuclidean distances between a given query and each \\nsubsequence in an all-subsequences set (see Definition 4).  \\nNote that we are assuming that the distance is measured \\nusing the Euclidean distance between the z -normalized \\nsubsequences [8]. The distance profile can be considered a meta \\ntime series that annotates the time series T that was used to \\ngenerate it. The first three definitions are illustrated in Figure 2. \\n \\nFigure 2. A subsequence Q extracted from a time series T is used as a query to \\nevery subsequence in T. The vector of all distances is a distance profile. \\nNote that if the query and all-subsequences set belong to the \\nsame time series, the distance profile must be zero at the \\nlocation of the query, and close to zero just before and just \\nafter. Such matches are c alled trivial matches in the literature \\n[18], and are avoided by ignoring an exclusion zone (shown as \\na gray region) of m/2 before and after the location of the query. \\nWe are interested in similarity join of all subsequences of a \\ngiven time series. We define an all-subsequences set of a given \\ntime series as a set that contains all possible subsequences from \\nthe time series. The notion of all-subsequences set is purely for \\nnotational purposes. In our implementation, we do not actually \\nextract the subsequences in this form as it would require \\nsignificant time and space overhead. \\nDefinition 4: An all-subsequences set A of a time series T \\nis an ordered set of all possible subsequences of T obtained \\nby sliding a window of length m across T: A ={T1,m,, \\nT2,m,‚Ä¶, Tn-m+1,m}, where m is a user -defined subsequence \\nlength. We use A[i] to denote Ti,m. \\nWe are interested in the nearest neighbor (i.e., 1NN) relation \\nbetween subsequences; therefore, we define a 1NN-join \\nfunction which indicates the nearest neighbor relation between \\nthe two input subsequences. \\nDefinition 5:  1NN-join function :  given two all -\\nsubsequences sets A and B and two subsequences A[i] and \\nB[j], a 1NN-join function  Œ∏1nn (A[i], B[j]) is a Boolean \\nfunction which returns ‚Äútrue‚Äù only if B[j] is the nearest \\nneighbor of A[i] in the set B. \\nWith the defined join function, a similarity join set  can be \\ngenerated by applying the similarity join operator on two input \\nall-subsequences sets. \\nDefinition 6: Similarity join set : given all -subsequences \\nsets A and B, a similarity join set  JAB of A and B is a set \\ncontaining pairs of each subsequence in A with its nearest \\nneighbor in B: JAB={‚å© A[i], B[j] ‚å™ |Œ∏1nn (A[i], B[j])}.  We \\ndenote this formally as JAB = A‚ãà\\uf0711nnB. \\n0\\n400\\n800\\n1200\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\\nFirst 5 Days\\nSecond 5 Days\\nData Center Chillers\\nT, a snippet of an energy \\nconsumption\\n2,0000 m/2m/2\\nQ, query of length m\\nNote that |D| = |T|-|Q|+1D, a distance profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='We measure the Euclidean distance between each pair \\nwithin a similarity join set and store the resultants into an \\nordered vector. We call the result vector the matrix profile. \\nDefinition 7:  A matrix profile (or just profile) PAB is a \\nvector of the Euclidean distances between each pair in JAB. \\nWe call this vector the matrix profile because one \\n(inefficient) way to compute it would be to compute the full \\ndistance matrix of all the subsequences in one time series with \\nall the subsequence in another time series and extract the \\nsmallest value in each row (the smallest non-diagonal value for \\nthe self-join case).  In Figure 3 we show the matrix profile of \\nour running example. \\n \\nFigure 3. A time series T, and its self-join matrix profile P.  \\nLike the distance profile, the matrix profile can be \\nconsidered a meta time series annotating the time series T if the \\nmatrix profile is generated by joining T with itself. The profile \\nhas a host of interesting and exploitable properties. For \\nexample, the highest point on the profile corresponds to the \\ntime series discord [5], th e ( tied) lowest points correspond to \\nthe locations of the best time series motif pair [18], and the \\nvariance can be seen as a measure of the T‚Äôs com plexity. \\nMoreover, the histogram of the values in the matrix profile is \\nthe exact answer to the time series density estimation [4]. \\nWe name this special case of the similarity join set \\n(Definition 6) as self-similarity join set, and the corresponding \\nprofile as self-similarity join profile. \\nDefinition 8:  A self-similarity join  set JAA is a result of \\nsimilarity join of the set  A with itself . We denote this \\nformally as JAA = A ‚ãà\\uf0711nn A. We denote the corresponding \\nmatrix profile or self-similarity join profile as PAA. \\nNote that we exclude trivial matches when self -similarity \\njoin is performed, i.e., if A[i] and A[j] are subsequences from \\nthe same all -subsequences set A, Œ∏1nn (A[i], B[j]) is ‚Äúfalse‚Äù \\nwhen A[i] and A[j] are a trivially matched pair. \\nThe ith element in the matrix profile tells us the Euclidean \\ndistance to the nearest neighbor of the subsequence of T, \\nstarting at i.  However, it does not tell us where that neighbor is \\nlocated. This information is recorded in matrix profile index. \\nDefinition 9: A matrix profile index IAB of a similarity join \\nset JAB is a vector of integers where IAB[i] = j if {A[i], B[j]} \\n‚àà JAB. \\nBy storing the neighboring information this way, we can \\nefficiently retrieve the nearest neighbor of A[i] by accessing the \\nith element in the matrix profile index. \\nNote that the function which computes the similarity join set \\nof two input time series  is not symmetric; therefore, JAB ‚â† JBA,  \\nPAB ‚â† PBA, and IAB ‚â† IBA. \\nFor ease of presentation, we have confined this work to the \\nsingle dimensional case; however, nothing intrinsically \\nprecludes generalizations to multidimensional data. \\nSummary of the Previous Section \\nThe previous section was rather dense, so before moving on \\nwe summarize the main takeaway points. We can create two \\nmeta time series, the matrix profile and the matrix profile index, \\nto annotate a time series TA with the distance and location of all \\nits subsequences nearest neighbors in itself or another time \\nseries TB. These two data objects explicitly or implicitly contain \\nthe answers to many time series data mining tasks. However, \\nthey appear to be too expensive to compute to be practica l. In \\nthe next section we will show an algorithm that can compute \\nthese efficiently. \\nIII. ALGORITHMS \\nWe are finally in a position to explain our algorithm s. We \\nbegin by stating the fundamental intuition, which stems from \\nthe relationship between distance profiles and the matrix \\nprofile. As Figure 2 and Figure 3 visually suggest, all distance \\nprofiles (excluding the trivial match region) are upper bound \\napproximations to the matrix profile. More critically, if we \\ncompute all the distance profiles, an d take the minimum value \\nat each location, the result is the matrix profile! \\nThis tells us that if we have a fast way to compute the \\ndistance profiles, then we also have a fast way to compute the \\nmatrix profile. As we shall show in the next section, we have an \\nultra-fast way to compute the distance profiles. \\nA. Mueen‚Äôs ultra-fast Algorithm for Similarity Search (MASS) \\nWe begin by introducing a novel Euclidean distance \\nsimilarity search algorithm for time series data. The algorithm \\ndoes not just find the nearest neighbor to a query and return its \\ndistance; it returns the distance to every subsequence. In \\nparticular, it computes the distance profile, as shown in Figure \\n2. The algorithm requires just O( nlogn) time by exploiting the \\nFFT to calculate  the dot products between the query and all \\nsubsequences of the time series.  \\nWe need to carefully qualify the claim of ‚Äúultra-fast‚Äù. There \\nare dozens of algorithms for time series similarity search that \\nutilize index structures to efficiently locate neighbors [8]. While \\nsuch algorithms can be faster in the  best case , all these \\nalgorithms degenerate to brute force search in the worst case 1 \\n(actually, much worse than brute force search due to the \\noverhead of the index). Likewise, there are index -free methods \\nthat achieve speed -up using various early abandoning tricks \\n[22], but they too degrade to brute force search in the worst \\ncase. In contrast, the performance of the algorithms outlined in \\nTABLE I and TABLE II is completely independent of the data. \\nTABLE I. CALCULATION OF SLIDING DOT PRODUCTS \\nProcedure SlidingDotProduct(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: The dot product between Q and all subsequences in T \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nn ‚Üê Length(T), m ‚Üê Length(Q) \\nTa ‚Üê Append T with n zeros   \\nQr ‚Üê Reverse(Q)  \\nQra ‚Üê Append Qr with 2n-m zeros \\nQraf ‚Üê FFT(Qra), Taf ‚Üê FFT(Ta) \\nQT ‚Üê InverseFFT(ElementwiseMultiplication(Qraf, Taf)) \\nreturn QT \\n \\n1 There are many such worse case scenarios, including high levels of noise blurring \\nthe distinction between closest and furthest neighbors, and rendering triangular -\\ninequality pruning and early abandoning worthless. \\n2,0000\\nP, a matrix profile\\nT, a snippet of an energy \\nconsumption\\nNote that |P| = |T|-|Q|+1'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Line 1 determines the length of both the time series T and \\nthe query Q. In line 2, we use that information to append T with \\nan equal number of zeros. In line 3, we obtain the mirror image \\nof the original query. This reversing ensures that a convolution \\n(i.e. ‚Äúcrisscrossed‚Äù multiplication) essentially produces in-order \\nalignment. Because we require both vectors to be the s ame \\nlength, in line 4 we append enough zeros to the (now reversed) \\nquery so that, like Ta, it is also of length 2 n. In line 5, the \\nalgorithm calculates Fourier transforms of the appended -\\nreversed query (Qra) and the appended time series Ta. Note that \\nwe use FFT algorithm which is an O(nlogn) algorithm. The Qraf \\nand the Taf produced in line 5 are vectors of complex numbers \\nrepresenting frequency components of the two time series. The \\nalgorithm calculates the element-wise multiplication of the two \\ncomplex vectors and performs inverse FFT on the product. \\nLines 5-6 are the class ic convolution operation on two vectors \\n[7]. Figure 4 shows a toy example of the sliding dot product \\nfunction in work. The algorithm time complexity does not \\ndepend on the length of the query (m). \\n \\nFigure 4. A toy example of convolution operation being used to calculate  \\nsliding dot products for time series data. Note the reverse and append \\noperation on T and q in the input. Fifteen dot products are calculated for every \\nslide. The cells m = 2 to n = 4 from left ( red/bold arrows) contain valid \\nproducts. TABLE II takes this subroutine and uses it to create a distance \\nprofile (see Definition 3). \\nIn line 1 of TABLE II, we invoke the dot products code \\noutlined in TABLE I. The formula to calculate the z-normalized \\nEuclidean distance D[i] between two time series subsequence Q \\nand Ti,m using their dot product, QT[i] is (see [24] for \\nderivation): \\nùê∑[ùëñ] = ‚àö2ùëö(1‚àíùëÑùëá[ùëñ]‚àíùëöùúáùëÑùëÄùëá[ùëñ]\\nùëöùúéùëÑùõ¥ùëá[ùëñ] ) \\nwhere m is the subsequence length, ŒºQ is the mean of Q, \\nMT[i] is the mean of Ti,m, œÉQ is the standard deviation of Q, and  \\nŒ£T[i] is the standard deviation of Ti,m. Normally, it takes O( m) \\ntime to calculate the mean and standard deviation for every \\nsubsequence of a long time series. However, here we exploit a \\ntechnique noted in [22] in a different context. We cache \\ncumulative sums of the values and square of the values in the \\ntime series. At any stage the two cumulative sum vectors are \\nsufficient to calculate the mean an d the standard deviation of \\nany subsequence of any length.  See [14] for an elaborate \\ndescription and variations of MASS. \\nTABLE II. MUEEN‚ÄôS ALGORITHM FOR SIMILARITY SEARCH (MASS) \\nProcedure MASS(Q, T) \\nInput: A query Q, and a user provided time series T \\nOutput: A distance profile D of the query Q  \\n1 \\n2 \\n3 \\n4 \\nQT ‚Üê SlidingDotProducts(Q, T) \\nŒºQ, œÉQ, ŒúT, Œ£T  ‚Üê ComputeMeanStd(Q, T)    // see [22] \\nD ‚Üê CalculateDistanceProfile(Q, T, QT, ŒºQ, œÉQ, ŒúT, Œ£T) \\nreturn D \\nUnlike the dozens of time series KNN search algorithms in \\nthe literature [8], this algorithm calculates the distance to every \\nsubsequence, i.e. the distance profile  of time series T. \\nAlternatively, in join nomenclature, the algorithm produces one \\nfull row of the all -pair similarity matrix. Thus, as we show in \\nthe next section, our join algorithm is simply a loop that \\ncomputes each full row of the all -pair similarity matrix and \\nupdates the current ‚Äúbest-so-far‚Äù matrix profile when needed. \\nB. The STAMP Algorithm \\nWe call our join algorithm STAMP, Scalable Time series \\nAnytime Matrix Profile. The algorithm is outlined in TABLE \\nIII. In line 1, we extract the length of TB. In line 2, we allocate \\nmemory and initial matrix profile PAB and matrix profile index \\nIAB. From lines 3 to line 6, we calculate the distance profiles D \\nusing each subsequence B[idx] in the time series TB and the \\ntime series TA. Then, we perform pairwise minimum for each \\nelement in D with the paired element in PAB (i.e., min( D[i], \\nPAB[i]) for i = 0 to length(D) - 1.) We also update IAB[i] with idx \\nwhen D[i] ‚â§ PAB[i] as we perform the  pairwise minimum \\noperation. Finally, we return the result PAB and IAB in line 7.  \\nNote that the algorithm presented in TABLE III computes \\nthe matrix profile for the general similarity join. To modify the \\ncurrent algorithm to compute  the self -similarity join matrix \\nprofile of a time series TA, we simply replace TB in line 1 with \\nTA, replace B with A in line 4, and ignore trivial match in D \\nwhen performing ElementWiseMin in line 5. \\nTABLE III. THE STAMP ALGORITHM \\nProcedure STAMP(TA, TB, m) \\nInput: Two user provided time series, TA and TB, interested \\nsubsequence length m \\nOutput: A matrix profile PAB and associated matrix profile index IAB of \\nTA join TB, JAB = A‚ãà\\uf0711nnB \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nnB ‚Üê Length(TB) \\nPAB ‚Üê infs, IAB ‚Üê zeros, idxes ‚Üê 1:nB-m+1 \\nfor each idx in idxes    // In any order, but random for anytime algorithm \\n          D ‚Üê MASS(B[idx], TA) \\n          PAB, IAB ‚Üê ElementWiseMin(PAB, IAB, D, idx) \\nend for \\nreturn PAB, IAB \\nTo parallelize the STAMP algorithm for multicore \\nmachines, we simply distribute the indexes to secondary \\nprocess run in each core, and the secondary processes use the \\nindexes they received to update their own PAB and IAB. Once the \\nmain process returns from  all secondary processes, we use \\nElementWiseMin to merge the received PAB and IAB.  \\nC. An Anytime Algorithm for TSAPSS \\nWhile the exact algorithm introduced in the previous section \\nis extremely scalable, there will always be datasets for which \\ntime needed for an exact solution is untenable. We can mitigate \\nthis by computing the results in an anytime fashion, allowing \\nfast approximate solutions [30]. To a dd the anytime nature to \\nthe STAMP algorithm, we simply ensure a randomized search \\norder in line 2 of TABLE III.  \\nWe can compute a (post-hoc) measurement of the quality of \\nan anytime solution by measuring the Root -Mean-Squared-\\nError (RMSE) between the true matrix profile and the current \\nbest-so-far mat rix profile. As Figure 5 suggests, with an \\nexperiment on random walk data, the algorithm converges very \\nquickly. \\nQ2T1 0 00 00 00 00 0Q1T4Q2T2+Q1T1 Q2T3+Q1T2 Q2T4+Q1T3\\nOutput\\nInputT2T1 T4T3 00 00 Q1Q2 00 00 00'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='Figure 5. main) The decre ase in RMSE as the STAMP algorithm updates \\nmatrix profile with the distance profile calculated at each iteration.  inset) The \\napproximate matrix profile at the 10% mark is visually indistinguishable from \\nthe final matrix profile. \\nZilberstein [30] gives a number of desirable properties of \\nanytime algorithms, including Low Overhead , Interruptibility, \\nMonotonicity, Recognizable Quality, Diminishing Returns and \\nPreemptability (these properties are mostly obvious from their \\nnames, but full definitions are at [30]). \\nBecause each subsequence‚Äôs distance profile is bounded \\nbelow by the exact matrix profile, updating an approximate \\nmatrix profile with a distance profile with pairwise minimum \\noperation either drives the approximate solution closer the exact \\nsolution or retains the current approximate solution. Thus , we \\nhave guaranteed Monotonicity. From Figure 5, the approximate \\nmatrix profile converges to the exact matrix profile \\nsuperlinearly; therefore, we have strong Diminishing Returns. \\nWe can easily achieve Interruptibility and Preemptability by \\nsimply inserting a few lines of code between lines 5 and 6 of \\nTABLE III that read: \\n5new \\n6new \\n7new \\nif CheckForUserInterrupt  = TRUE \\n          Report({PAB, IAB}, ‚ÄòHere is an approximate answer.‚Äô) \\nif GetUserChoice = ‚Äòfurther refine‚Äô, CONTINUE, else BREAK \\nThe space and time overhead for the anytime property is \\neffectively zero, thus we have Low Overhead. This leaves only \\nthe property of Recognizable Quality. Here we must resort to a \\nprobabilistic argument.  The convergence curve shown in  \\nFigure 5 is very typical, so we could use past convergence \\ncurves to predict the quality of solution when interrupted on \\nsimilar data.  \\nD. Time and Space Complexity \\nThe overall complexity of the proposed algorithm is \\nO(n2logn) where n is the length of the time series. However, our \\nexperiments (see Section 4.1) empirically suggest that the \\nruntime of  STAMP‚Äôs growth rate is roughly O( n2) instead of \\nO(n2logn). One possible e xplanation for this is that the nlogn \\nfactor comes from the FFT subroutine. Because FFT is so \\nimportant in many applications, it is extraordinarily well \\noptimized. Thus, the empirical runtime is very close to linear. \\nIn contrast to the above, the brute for ce nested loop \\napproach has a time complexity of O(n2m). Recall the industrial \\nexample in the introduction section. We have  m = 10,080, but \\nlog(n) = 5.7, so we would expect our approach to be about \\n1,768 times faster. In fact, we are empirically even faster. The \\ncomplexity analysis downplays the details of important constant \\nfactors. The nested loop algorithm must also z -normalize the \\nsubsequences. This either requires O( nm) time, but with an \\nuntenable O(nm) space overhead, or an O( n2m) time overhead. \\nAnd r ecall that this is before a single Euclidean distance \\ncalculation is performed.  \\nFinally, we mention one quirk of our algorithm which we \\ninherit from using the highly optimized FFT subroutine. Our \\nalgorithm is fastest when n is an integer power of two, slower \\nfor non -power of two but composite numbers, and slowest \\nwhen n is prime. The difference (for otherwise similar values of \\nn) can approach a factor of 1.6x. Thus, where possible, it is \\nworth contriving the best case by tru ncation or zero-padding to \\nthe nearest power of two. \\nE. Incrementally Maintaining TSAPSS \\nUp to this point we have discussed the batch version of \\nTSAPSS. By batch, we mean that the STAMP algorithm needs \\nto see the entire time series TA and TB (or just TA if we are \\ncalculating the self -similarity join matrix profile) before \\ncreating the matrix profile. However, it would be advantageous \\nif we could build the matrix profile incrementally. Given that \\nwe have performed a batch construction of matrix profile, i f \\nnew data arrives, it would clearly be preferable to incrementally \\nadjust the current profile, rather than start from scratch.  \\nBecause the matrix profile solves both the times series motif \\nand the time series discord problems, an incremental version of \\nSTAMP would automatically provide the first incremental \\nversions of both algorithms. We call such an algorithm the \\nSTAMPI (STAMP Incremental) algorithm. \\nWe will demonstrate our ability to incrementally maintain \\nthe matrix profile in this section. For simpli city and brevity \\nTABLE IV only shows the algorithm to maintain the self -\\nsimilarity join. The generalizations are obvious. \\nTABLE IV. THE STAMPI ALGORITHM \\nProcedure STAMPI(TA, t, PAA, IAA) \\nInput: The original time series TA, a new data point t following TA, the \\nmatrix profile PAA and its associated matrix profile index IAA of TA.  \\nOutput: The incrementally updated matrix profile PAA,new and its matrix \\nprofile index IAA,new of the current time series TA,new= TA, t. \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\nTA,new = [TA, t] \\nS ‚Üê last subsequence in TA,new, idx ‚Üê index of S in TA,new \\nD ‚Üê MASS (S, TA) \\nPAA, IAA ‚Üê ElementWiseMin(PAA, IAA, D, idx) \\npAA,last, iAA,last ‚Üê FindMin(D) \\nPAA,new ‚Üê [PAA, pAA,last], IAA,new ‚Üê [IAA, iAA,last] \\nreturn PAA,new, IAA,new \\nFor clarity, we denote the updated time series as TA,new, the \\nupdated matrix profile as PAA,new and the associated matrix \\nprofile index as IAA,new. As each additional data point t arrives, \\nthe size of the time series TA increases by one, and a new \\nsubsequence S is generated at the end of TA,new. In line 3 we \\nobtain the distance profile of S with regard to TA. Then, as in the \\noriginal STAMP algorithm,  in line 4 we perform  a pairwise \\ncomparison between every element in D with the corresponding \\nelement in PAA to see if the corresponding element in PAA needs \\nto be updated. In line 5, we find the nearest neighbor of S and \\nthe associated index by evaluating the minimum value of D. \\nFinally, in line 6, we obtain the new matrix profile and \\nassociated matrix profile index by concatenating the results in \\nline 4 and line 5.  \\nThe time complexity of the STAMP I algorithm is O(nlogn) \\nwhere n is the length of size of th e current time series TA. Note \\nthat as we maintain the profile, each incremental call of \\nSTAMPI requires invoking the FFT subroutine with a slightly \\nlonger time series ( n becomes n+1 ). Thus it gets very slightly \\nslower at each time step. Therefore, the best way to measure the \\nperformance is to ask for the Maximum Time Horizon (MTH), \\nin essence the answer to this question: ‚Äú Given this arrival rate, \\nhow long can we maintain the profile before we can no longer \\nupdate fast enough?‚Äù \\nNote that the subsequence length m is not considered in the \\nMTH evaluation. Recall that overall time complexity of the \\n10,0000\\nRMSE\\niteration1,000\\napproximate matrix \\nprofile at 1,000 iterations.\\nexact matrix profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='algorithm is determined by the efficiency of the FFT \\nsubroutine, which is independent of m. We have computed the \\nMTH for two common scenarios of interest to the community. \\n\\uf0b7 House Electrical Demand  [19]: This dataset is updated \\nevery eight seconds. By iteratively calling the STAMP I \\nalgorithm, we can maintain the profile for 5.8 years.  \\n\\uf0b7 Oil Refinery :  Most telemetry in oil refineries is sampled \\nat once a minute [26]. The relatively low sampling rate \\nreflects the ‚Äúinertia‚Äù of massive boilers/condensers. Even if \\nwe maintain the profile for 40 years, the update time is only \\naround 6.78 seconds. Moreover, th e raw data, matrix \\nprofile and index would only require 0.5 gigabytes of main \\nmemory. Thus the MTH here is forty plus years. Given \\nprojected improvements in hardware, this effectively \\nmeans we can maintain the matrix profile forever. \\nAs impressive as these  numbers are, they are actually quite \\npessimistic. For simplicity we assume that every value in the \\nmatrix profile index will be updated at each time step.  \\nHowever, empirically, much less than 0.1% of them need to be \\nupdated. If it is possible to prove an upper bound on the number \\nof changes to the matrix profile index per update, then we could \\ngreatly extend the MTH  or handle much faster sampling rates. \\nWe leave such considerations for future work. \\nIV. EXPERIMENTAL EVALUATION \\nWe begin by stating our experimen tal philosophy. We have \\ndesigned all experiments such that they are easily reproducible. \\nTo this end, we have built a webpage [24] which contains al l \\ndatasets and code used in this work, together with spreadsheets \\nwhich contain the raw numbers and some supporting videos.  \\nGiven page limits, and the utility of our algorithm for a host \\nof existing (and several new) data mining tasks, we have chosen \\nto c onduct exceptionally broad  but shallow experiments. We \\nhave conducted such deep detailed experiments and placed \\nthem at [24]. Unless otherwise state d we measure wall clock \\ntime on an Intel i7@4GHz with 4 cores. \\nA. Scalability of Profile-Based Self-Join \\nBecause the time performance of STAMP is independent of \\nthe data quality or any user inputs (there are none except the \\nchoice of m, which does not affect the speed), our scalability \\nexperiments are unusually brief.  In TABLE V we show the \\ntime required for a self -join with m fixed to 256, for \\nincreasingly long time series.  \\nTABLE V.  TIME REQUIRED FOR A SELF-JOIN WITH M = 256, VARYING N \\nValue of n 217 218 219 220 221 \\nTime Required 15.1 min 70.4 min 5.4 hours 24.4 hours 4.2 days \\nIn TABLE VI, we show the time required for a self -join \\nwith n fixed to 2 17, for increasing long m. Again recall that \\nunlike virtually all other time series data mining algorithms in \\nthe literature whose performance degrades for longer \\nsubsequences [8][18], the running time of STAMP does not \\ndepend on m. \\nTABLE VI. TIME REQUIRED FOR A SELF-JOIN WITH N= 217, VARYING M \\nValue of m 64 128 256 512 1,024 \\nTime Required 15.1 min 15.1 min 15.1 min 15.0 min 14.5 min \\nFinally, we further exploit the simple parallelizability of the \\nalgorithm by using four 16 -core virtual machines on Microsoft \\nAzure to redo the two -million join ( n = 2 21 and m = 256) \\nexperiment. By scaling up the computational power, we have \\nreduced the running time from 4.2 days to just 14.1 hours. This \\nuse of cloud computing required writing just few dozen lines of \\nsimple additional code [24]. \\nIt is difficult to find good baselines to compare to. We \\nbelieve STAMP is the only algorithm that does full, exact joins \\non time series subsequences. Most algorithms do only threshold \\njoins and/or do approximate joins, and are not specialized for \\ntime series subsequences. However, we searched for the best \\nbaselines and made the following concessions to  the rival \\nalgorithms to allow comparisons. \\n\\uf0b7 TSFRDAA [11]: While STAMP returns all nearest \\nneighbors, we adjust the threshold (the selectively) of \\nTSFRDAA such that only needs to return the top 1% nearest \\nneighbors, a much easier task. \\n\\uf0b7 HDSJI-SAX [12]: This method  allows false negatives ; we \\nignore this. It needs time to build indexes ; we do not count \\nthis time, and as above, we allow it to return  the only the \\ntop 1% nearest neighbors ( as bef ore, not the 100% the \\nSTAMP returns, and therefore a much easier task.). \\n\\uf0b7 Optimized Nested Loop (ONL): Here we use a nested -loop \\njoin optimized in the following manner. We use a state -of-\\nthe-art DFT indexing technique to do the search in the \\ninner loop, and we do not count the time needed to build \\nthe indexes [8]. We carefully adjust  parameters for best \\nperformance.  \\nWe consider the first 2 18 data points of the ECG dataset \\nused in [22], with a query length of 256, about one heartbeat ; \\nTABLE VII shows the results.  \\nTABLE VII. TIME FOR A SELF-JOIN WITH M = 256, VARYING ALGORITHMS \\nAlgorithm TSFRDAA HDSJI-SAX ONL STAMP \\nTime Required 51.7 hours 19.6 min 28.1 hours 1.17 hours \\nAs these results show, even if we ignore the limitations of \\nthe baseline methods, STAMP is still significantly faster. \\nB. Profile-Based Self-Join \\nA recent paper notes that many fundamental problems in \\nseismology can be solved by joining seismometer telemetry \\n[28], including the disc overy of foreshocks, aftershocks, \\ntriggered earthquakes, volcanic activity and induced seismicity. \\nHowever, the paper notes a join with a query length of 200 on a \\ndata stream of length 604,781 requires 9.5 days. Their solution, \\na clever transformation of t he data to allow LSH based \\ntechniques, does achieve significant speedup, but at the cost of \\nfalse negatives and the need for significant parameter tuning.  \\nThe authors kindly shared their data, and, as we hint at in \\nFigure 6, confirmed that STAMP does not have false negatives. \\n \\nFigure 6. top) An excerpt of a seismic time series aligned with its matrix \\nprofile (bottom). The ground truth provided by the authors of [28] requires that \\nthe events occurring at time 4,050 and 7,800 match. \\nWe repeated the n = 604,781, m = 200 experime nt and \\nfound it took just 8.9 hours to finish. As impressive as this is, in \\nthe next section we show that we can do even better.   \\n4,000 5,000 6,000 7,000 8,000 9,000\\n0\\n5\\n10\\n15\\nSeismic time series (excerpt) \\nMatrix Profile'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='C. The Utility of Anytime STAMP \\nThe seismology dataset offers an excellent opportunity to \\ndemonstrate the utility of the anytime version of our algorithm. \\nThe authors of [28] revealed in their long -term ambition of \\nmining even larger datasets [3]. In Figure 7 we repeated the \\nexperiment with the snippet shown in Figure 6, this time \\nreporting the best-so-far matrix profile reported by the \\nalgorithm at various milestones. Even with just 0.25% of the \\ndistances computed (that is to say, 400 times faster) the correct \\nanswer has emerged. Thus, we can provide the correct answers \\nto the seismologists in just minutes, rather than the 9.5 days.  \\n \\nFigure 7. top) An excerpt of the seismic data that is also shown in Figure 6. \\ntop-to-bottom) The approximations of the matrix profile for increasing \\ninterrupt times. By the time we have computed just 0.25% of the calculations \\nrequired, the minimum of the matrix profile points to the ground truth. \\nTo show the generality of this anytime feature of STAMP, \\nwe consider a very different dataset. As shown in Figure \\n8.inset), it is possible to convert DNA to a time ser ies [22]. We \\nconverted the Y-chromosome of the Chimpanzee this way. The \\nresulting time series is  little over one -million in length. We \\nperformed a self-join with m = 60,000.  Figure 8.bottom shows \\nthe best motif is so well conserved (ignoring the first 20%), that \\nit must correspond to a recent (in evolutionary tim e) gene \\nduplication event. In fact, in a subsequent analysis we \\ndiscovered that ‚Äúmuch of the Y (Chimp chromosome) consists of \\nlengthy, highly similar repeat units, or ‚Äòamplicons‚Äô‚Äù [9]. \\n \\nFigure 8. top) The Y -chromosome of the Chimp in time series space with its \\nmatrix profile. bottom) A zoom -in of the top motif discovered using anytime \\nSTAMP, we believe it to be an amplicon [9]. \\nThis demanding join would take just over a day of CPU \\ntime (see TABLE V). However, using anytime STAMP we \\nhave the result shown above after doing just 0.021% of the \\ncomputations, in about 18 seconds. At [24] we have videos that \\nshow the rapid convergence of the anytime variant of STAMP. \\nD. Profile-Based Similarity Join Set \\nIn this section we show two uses of similarity join set. The \\nfirst use is more familiar to APSS users, quantifying what is \\nsimilar between two time series. The second example, \\nquantifying what is different between two time series, is novel  \\nand can only be supported by threshold -free algorithms that \\nreport the nearest neighbor for all objects. \\nTime Series Set Similarity \\nGiven two (or more) time series collected under different \\nconditions or treatments, a data analyst may wish to know what \\npatterns (if any) are conserved between the two time series. To \\ndemonstrate the utility of automating this, we consider a simple \\nbut in tuitive example. Figure 9 shows the raw audio of two \\npopular songs converted to Mel Frequency Cepstral \\nCoefficients (MFCCs). Specifically, the songs used in this \\nexample are ‚ÄúUnder Pressure‚Äù by Queen an d David Bowie and \\n‚ÄúIce Ice Baby ‚Äù by the American rapper Vanilla Ice. Normally, \\nthere are 13 MFCCs; here, we consider just one for simplicity. \\n \\n \\nFigure 9. Two songs represented by just the 2 nd MFCC at 100Hz. We \\nrecognize that it is difficult to see any structure in these times series; however, \\nthis difficulty is the motivation for this experiment. \\nEven for these two relatively short time series, visual \\ninspection does not offer immediate answers. The problem here \\nis compounded by the size reproduction, but is not significantly \\neasier with large-scale [24] or interactive graphic tools. We ran \\nJAB (Queen-Bowie, Vanilla Ice) on these datasets with m = 500 \\n(five seconds), the best match, corresponding the minimum \\nvalue of the matrix profile PAB, is shown in Figure 10. \\n \\nFigure 10. The result of JAB (Queen-Bowie (red/bold), Vanilla-Ice (green/fine)) \\nproduces a strongly conserved five second region. \\nReaders may know the cause for this highly conserved \\nsubsequence. It corresponds to the famous baseline of ‚ÄúUnder \\nPressure,‚Äù which was sampled (plagiarized) by Vanilla Ice in \\nhis song. The join took 21.9 seconds. The ability to find \\nconserved structure in apparently disparate time series could \\nopen many avenues of research in medicine and industry. \\nTime Series Difference \\nWe introduce the Time Series Diff (TSD) operator, which \\ninformally asks ‚ÄúWhat happens in time series T A, that does not \\nhappen in time series TB?‚Äù Here TA/TB could be an ECG before \\na drug is administered/after a drug is administered, telemetry \\nbefore a successful launch/before a catastrophic launch etc. The \\nTSD is simply the subsequence referred to by the maximum \\nvalue of the JAB join‚Äôs profile PAB. \\nWe begin with a simple intuitive example. The UK and US \\nversions of the Harry Potter audiobook series are performed by \\ndifferent narrators, and have a handful of differences in the text. \\nFor example, the UK version contains: \\nHarry was passionate about Quidditch. He had played as Seeker on the Gryffindor \\nhouse Quidditch team ever since his first year at Hogwarts and owned a Firebolt, one \\nof the best racing brooms in the world... \\nBut the corresponding USA version has: \\nHarry had been on the Gryffindor House Quidditch team ever since his first year at \\nHogwarts and owned one of the best racing brooms in the world, a Firebolt. \\nAs shown in Figure 11, we can convert the audio \\ncorresponding to these snippets into MFCCs and invoke a JAB \\njoin set to produce a matrix profile PAB that represents the \\ndifferences between them. As Figure 11.left shows, the low \\nvalues of this profile correspond to identical spoken phrases \\n(despite having two different narrators). However here we are \\n4,000 5,000 6,000 7,000 8,000 9,000\\ndata\\n0.25%\\n1%\\n100%\\n0 1,000,0000\\n100\\n200\\nPan troglodytes Y-chromosome \\n0 60,000\\n12,749,475 to 14,249,474 bp\\n622,725 to 2,122,724 bp\\nT1 = 0;\\nfor i = 1 to length(chromosome) \\nif chromosomei = A, then Ti+1 = Ti + 2 \\nif chromosomei = G, then Ti+1 = Ti + 1\\nif chromosomei = C, then Ti+1 = Ti - 1 \\nif chromosomei = T, then Ti+1 = Ti - 2 \\nend\\n0\\nQueen-Bowie\\n1,000 2,000\\n-10\\n0\\n10\\nVanilla Ice\\n0 250 500-3\\n-2\\n-1\\n0\\n1\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='interested in the differences, the maximum value of the profile. \\nAs we can see in Figure 11.right, here the profile corresponds \\nto a phrase unique to the USA edition.  \\n \\nFigure 11. The 2 nd MFCC of snippets from the USA ( pink/bold) and UK \\n(green/fine) Harry Potter audiobooks.  The JAB join of the two longer sections \\nin the main text produce s mostly small values in the profile correspond to the \\nsame phrase (left), the largest value in  the profile corresponds to a phrase \\nunique to the USA edition (right). \\nThe time required to do this is just 0.067 seconds, much \\nfaster than real time. While this demonstration is trivial, in [24] \\nwe show an example applied to ECG telemetry.  \\nE. Profile-Based Motif Discovery \\nSince their introduction in 2003, time series motifs have \\nbecome one of the most frequently used primitives in time  \\nseries data mining, with applications in dozens of domains [2]. \\nThere are several proposed definitions for time series motifs, \\nbut in [18] it is argued that if you can solve the most basic \\nvariant, the closest (non -trivial) pair of subsequences, then all \\nother variants only require some minor additional calculations. \\nNote that the locations of the two (tying) minimum values of \\nthe matrix profile are exactly the locations of the 1st motif pair. \\nThe fastest known exact algorithm for computing time \\nseries motifs is the MK algorithm [18]. Note, however, that this \\nalgorithm‚Äôs time performance depends on the time series itself. \\nIn contrast, the Profile -Based Motif Discovery (PBMD) takes \\ntime independent of the data. To see this, we compared the two \\napproaches on an electrocardiogram of length 65,536. In Figure \\n12.left we ask what happens as we search for lon ger and longer \\nmotifs. In Figure 12.right we ask what happens if the motif \\nlength is fixed to m = 512, but the data becomes increasing \\nnoisy.  \\n \\nFigure 12. The time required to find the top -motif pairs in a time series of \\nlength 216 for increasingly long motif lengths ( left), and for a length fixed to \\n512, but in the face of increasing noise levels (right). \\nThese results show that  even in the best case for MK, \\nPBMD is competitive, but as we have longer queries and/or \\nnoisier data, its advantage becomes unassailable. Moreover, \\nPBMD inherits STAMP‚Äôs anytime and incremental \\ncomputability, and is easily parallelizable.  \\nF. Profile-Based Discord Discovery \\nA time series discord  is the subsequence that has the \\nmaximum distance to its nearest neighbor. While this is a \\nsimple definition, time series discords are known to be very \\ncompetitive as novelty/anomaly detectors  [5]. Note that as \\nshown in Figure 13, the time series discord is encoded as the \\nmaximum value in a matrix profile. \\n \\nFigure 13. top) An excerpt from an ECG incorporating a premature ventricular \\ncontraction (red/bold). bottom) The time series profile peaks exactly at the \\nbeginning of the PVC. \\nThe time taken to compute the discord  is obviously just the \\ntime needed to compute the matrix profile ( here, 0.9 seconds). \\nThere are a few dozen discord discovery algorithms in the \\nliterature. Some of them may be competitive in the best case, \\nbut just like motif -discovery algorithms they all degenerate to \\nbrute force search in the worst case, and none allow the anytime \\nproperties that we inherit from using STAMP.  \\nG. Incrementally Maintaining Motifs and Discords \\nWe have demonstrated the ability to detect time series \\nmotifs and discords using the matrix profile in the previous two \\nsections. However, we assumed that the entire time series was \\navailable beforehand. Here we remove this assumption and \\nshow how STAMP I allows us to incrementa lly maintain time \\nseries motifs/ discords in an online fashion. There are other \\nattempts at one [20][2] or both [25] of these tasks, but they are \\nall approximate and allow false dismissals.   \\nIn Section III.E, we introduced the STAMPI algorithm. The \\nability to incrementally maintain the matrix profile implies the \\nability to exactly maintain the time series motif [18] and/or time \\nseries discord [5] in streaming data. We simply need to keep \\ntrack of the extreme values of the incrementally-growing matrix \\nprofile, report a new pair of motifs when a new minimum value \\nis detected, and report a new discord when we see a new \\nmaximum value. \\nWe demonstrate the utility of these ideas on the AMPds \\ndataset [13]. Here the kitchen fridge and the heat pump are both \\nplugged into a single metered power supply. For the first week, \\nonly the refrigerator is running. At the end of the week, the \\nweather gets cold and the heat pump is turned on. The sampling \\nrate is one sample/m inute, and th e subsequence length is 100. \\nWe apply the STAMP algorithm to the first three days of data, \\nthen invoke STAMP I to handle newly arriving data , report an \\nevent when we detect a new extreme value. \\nOur first event occurs at the 9,864 th minute (6 da y 20 hour \\n24 minute). As shown in Figure 14, a new minimum value is \\ndetected, which indicates a new time series motif. The just -\\narrived 100 -minute-long pattern looks v ery similar to another \\npattern that occurred five hours earlier. While there is a lot of \\nregularity in the fridge data in general, the exceptional \\nsimilarity observed here suggested some underlying physical \\nmechanism that caused such a perfectly -conserved pattern, \\nperhaps a mechanical ice-making ‚Äúsubroutine.‚Äù \\n \\nFigure 14. top) The matrix profile of the first 9,864 minutes of data. bottom) \\nThe minimum value of the matrix profile corresponds to a pair of time series \\nmotifs in the power usage data. right) The time series motif detected. \\nOur second event occurs at the 10,473 th minute (7 day 6 \\nhour 33 minute). As shown in Figure 15, a new maximum value \\n‚Ä¶indor house Quidditch team ever since his first ye‚Ä¶\\nHarry had been on the Gryffindor House Quidditch te..\\nsince his first year at Hogwarts and owned a Fire..\\nsince his first year at Hogwarts and owned on..\\nED = 2.9 \\nClosest Match \\n0 100(1.6 seconds) 0 100\\nED = 10.4\\n(1.6 seconds)\\nFurthest Match (Time Series Difference) \\n256 512 1,024 2,048 4,096\\n0\\n40\\n80\\n120\\n160\\n200\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nSubsequence Length  (m)\\nTime Taken (min)\\n80 40 0\\n0\\n70\\nMK Algorithm\\nProfile-Based Motif \\nDiscovery\\nNoise Added (dB)  \\n0 1000 2000 3000\\nECG qtdb\\nSel102\\n(excerpt)\\nPremature Ventricular \\nContraction\\nTime Series Profile\\n0 5,000 10,000\\n0 100\\nnew minimum value\\nnew motifMatrix Profile\\nPower Usage Data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content='is detected, which indicates a new time series discord. The time \\nseries discord corresponds to the first occurrence of a heat \\npump pattern in the power usage data.  \\n \\nFigure 15. top) The matrix profile for the first 10,473 minutes. bottom) The \\nmaximum value of the matrix profile corresponds to a time series discord. \\nright) The time series discord detected is the first  heat pump pattern \\noccurrence in the dataset. \\nThe maximum time needed to process a single data point \\nwith STAMP I in this dataset is 0.005 second s, which is less \\nthan 0.01% of the data sampling rate. Thus, on this dataset we \\ncould continue monitoring with the STAMP I algorithm for \\nseveral decades before running out of time or memory. \\nH. Profile-Based Shapelet Discovery \\nShapelets are time series subsequences which are in some \\nsense the maximally representative of a class [23][27]. \\nShapelets can be used to classify time series (essentially, the \\nnearest shapelet algorithm), offering the benefits of speed, \\nintuitiveness and at least on some domains, significantly \\nimproved accuracy [23]. However, these advantages come at \\nthe cost of a very expensive training phase, with O( n2m4) time \\ncomplexity, where m is the length of the longest time series \\nobject in the dataset, and n is the number of objects in the \\ntraining set. In order to mitigate this high ti me complexity, \\nresearchers have proposed various distance pruning techniques \\nand candidate ranking approaches for both the admissible [27] \\nand appro ximate [23] shapelet discovery. Nevertheless, \\nscalability remains the bottleneck.  \\nBecause shapelets are essentially supervised motifs, and we \\nhave shown that STAMP can find motifs very quickly, it is \\nnatural to ask if STAMP has implications for shapelet \\ndiscovery. While space limitations prohibit a detailed \\nconsideration of this question, we briefly sketch out and test \\nthis possibility as follows. \\nAs shown in Figure 16, we can use matrix profile to \\nheuristically ‚Äúsuggest‚Äù candidate shapelets. We consider two \\ntime series TA (green/bold) and TB (pink/light) with class 1 and \\nclass 0 being their corresponding class label, and we take  JAB, \\nJAA, JBA and JBB. Our claim is the differences in the heights of \\nPAB, PAA (or PBA, PBB) are strong indicators of good candidate \\nshapelets. The intuition is that if a discriminative pattern is \\npresent in say, class 1, but not in class 0, then we expect to see a \\n‚Äúbump‚Äù in the PAB (the intuition holds if the order is reversed). \\nA significant difference (quantified by a threshold shown in \\ndashed line) between the heights of PAA and PAB curves \\ntherefore indicates the occurrence of good candidate shapelets, \\npatterns that only occur in one of the two classes. \\n \\nFigure 16. top.left) Two time series TA and TB formed by concatenating \\ninstances of class 1 and 0 respectively of ArrowHead [6]. bottom) The height \\ndifference between PAB (or PBA) and PAA (or PBB) are suggestive of  good \\nshapelets. top.right) An example of good shapelet extracted from class 1. \\nThe time taken to compute  all four matrix profiles is 1.0 \\nseconds and the time to further evaluate the two twelve \\ncandidates selected takes 2.7 seconds. On the same machine, \\nthe brute force shapelet classifier takes 4.2 minutes with 2,364 \\ncandidates. Note that, in this toy demonst ration, the speedup is \\n68X, however, for larger datasets, the speedup is greater [24]. \\nI. Profile-Based Semantic Segmentation \\nThe goal of time series semantic segmentation is to partition \\na dataset containing multiple activities/regimes into atomic \\nbehaviors or conditions. For example, for human activity data \\nthe regimes might later be mapped to { eating, working, \\ncommuting,...}[29]. As this example suggests, most work in this \\narea is highly domain -dependent. In contrast, here we show \\nhow the matrix profile index can be emp loyed for domain \\nagnostic time series segmentation.  \\nThe intuition of our approach is as follows. Within a single \\nregime we might expect that most subsequences will have a \\nnearest neighbor close by (in time). For example, consider the \\ntoy problem shown in Figure 17 which shows two obvious \\nregimes. We would expect that the nearest neighbor to the first \\nrun gait cycle is the second or third or fourth run cycle, but it \\nwill almost certainly not be one of the walk cycles. In general, \\nthis tendency for nearest neighbor pointers not to cross the \\nboundaries corresponding to regime changes may be sufficient \\nto discover these boundaries, and of course, this is precisely the \\ninformation that is encoded in the matrix profile index. \\nThus, for each point we count how many ‚Äúarcs‚Äù connecting \\ntwo nearest neighbors cross it if we connect each subsequence \\nto its nearest neighbor as shown in Figure 17. \\n \\nFigure 17. top) A (toy) time series ( red) and nearest neighbor locations for \\neach subsequence. bottom) The number of arcs crossing above each \\nsubsequence. \\nWe applied the procedure described above to a heavily \\nstudied activity segmentation problem [29], which was derived \\nfrom the CMU Motion Capture data base [16]. The recordings \\nare represented as multi -dimensional time series, and most \\nresearch efforts carefully select the best subset for the \\nsegmentation task. For example, [29] states that ‚Äú we only \\nconsider the 14 most informative joints out of 29 .‚Äù In contrast, \\nwe attempt this with a single dimension. The only parameter we \\nneed to set is sliding window size . In Figure 18 we show the \\nsegmenting results obtained using our approach , with  \\nannotations taken from [29] for context. \\nMuch of the evaluation in this community lacks formal \\nmetrics, preferring instead visual sanity tests like the one in \\nFigure 18. Given this, we can say that our approach is very \\ncompetitive on this dataset, in spite of the fact that we \\nhandicapped ourselves to only consider one dimension.  \\n0 100\\n0 10,0005,000\\nMatrix Profile\\nPower Usage Data\\nnew maximum value\\nnew discord\\n0\\n7\\n0 1,400\\nPABPAA\\n0 1,600\\nTA\\nTB\\n0 300\\nAn example of good shapelet\\n0 1,400\\nPBA\\nPBB\\n8\\n0\\n0 1,400\\nDiff(PAB ,PAA) Threshold\\n-1\\n4\\n-2\\n4\\n0 1,400\\nDiff(PBA ,PBB) Threshold\\n0\\n1\\n2\\n1 2 0\\nwalking slow    walking slow     run       run run run\\nNumber of arcs intersecting each point'),\n",
       " Document(metadata={'producer': 'macOS Version 15.7.1 (Build 24G231) Quartz PDFContext, AppendMode 1.1', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': \"D:20160929004132Z00'00'\", 'author': 'IEEE', 'moddate': \"D:20251217092537Z00'00'\", 'title': 'Paper Title (use style: paper title)', 'source': '../data/pdf/Matrix Profile_I.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'Matrix Profile_I.pdf', 'file_type': 'pdf'}, page_content=\"Figure 18. top) A matrix profile ( blue) obtained for the time series ( red) and \\nnumber of arcs crossing each point ( green). Low values of this green curve \\ncoorespond to candidate split points. bottom) Human annotations of the \\nactivities: w ‚Äì walk, s ‚Äì stretch, p ‚Äì punch, c ‚Äì chop, t ‚Äì turn, d ‚Äì drink. \\nV. CONCLUSION \\nWe have introduced a scalable algorithm for creating time \\nseries subsequences joins. Our algorithm is simple, fast, \\nparallelizable and parameter -free, and can be in crementally \\nupdated for moderately fast data arrival rates. We have shown \\nthat our algorithm has implications for many existing tasks, \\nsuch as motif discovery, discord discovery, shapelet discovery \\nand semantic segmentation, and may open up new avenues for  \\nresearch, including computing various definitions of time series \\nset difference. Our code is freely available for the community to \\nconfirm, extend and exploit our ideas. \\nREFERENCES \\n[1] R. J. Bayardo, Y. Ma and R. Srikant, ‚ÄúScaling up all pairs similarity \\nsearch,‚Äù WWW 2007, pp 131-140. \\n[2] N. Begum and E. Keogh, ‚ÄúRare time series motif discovery from \\nunbounded streams,‚Äù PVLDB 8(2): 149-160, 2014. \\n[3] G. Beroza, ‚ÄúPersonal Correspondence,‚Äù Jan 21th, 2016. \\n[4] T. Bouezmarni and J. Rombouts, ‚ÄúNonparametric density estimation for \\npositive time series,‚Äù CSDA, 54, 245-261, 2010.  \\n[5] V. Chandola, D. Cheboli and V. Kumar, ‚ÄúDetecting anomalies in a time \\nseries database,‚Äù UMN TR09-004. \\n[6] T. Chen  et al ., ‚ÄúThe UCR time series classification archive,‚Äù \\nhttp://www.cs.ucr.edu/~eamonn/time_series_data/. \\n[7] ‚ÄúConvolution - Wikipedia, the free encyclopedia,‚Äù  \\nhttps://en.wikipedia.org/wiki/Convolution, Accessed: 2016-01-19. \\n[8] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang and E. J. Keogh, \\n‚ÄúQuerying and mining of time series data: experimental comparison of \\nrepresentations and distance measures,‚Äù PVLDB 1(2): 1542-1552. 2008. \\n[9] J. Hughes et al. , ‚ÄúChimpanzee and human Y chromosomes are \\nremarkably divergent in structure‚Äù Nature 463, (2010). \\n[10] H. Lee, R. Ng and K. Shim, ‚ÄúSimilarity join size estimation using \\nLocality sensitive hashing,‚Äù PVLDB, 4(6):338‚Äì349, 2011. \\n[11] W. Luo, H. Tan, H. Mao  and L. M.  Ni, ‚ÄúEfficient similarity joins on \\nmassive high-dimensional datasets using mapreduce,‚Äù In MDM'12, IEEE \\nComputer Society, pp. 1-10. \\n[12] Y. Ma, X. Meng and S. Wang, ‚ÄúParallel similarity joins on massive high-\\ndimensional data using MapReduce ,‚Äù Concurrency and Computation , \\nVolume 28, Issue 1 Jan 2016. Pages 166‚Äì183. \\n[13] S. V. Makonin, ‚ÄúAMPds: a public dataset for load disaggregation and \\neco-feedback research,‚Äù EPEC 2013, pp 1-6. \\n[14] MASS: http://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html \\n[15] G. D. F. Morales and A. Gionis, ‚Äústreaming similarity self-join,‚Äù Proc. \\nVLDB Endow, 2016. \\n[16] Motion Capture Database, http://mocap.cs.cmu.edu/ \\n[17] A. Mueen, H. Hamooni and T. Estrada, ‚ÄúTime series join on subsequence \\ncorrelation,‚Äù IEEE ICDM 2014, pp. 450-459. \\n[18] A. Mueen, E. Keogh, Q. Zhu, S. Cash and B. Westover, ‚ÄúExact discovery \\nof time series motif,‚Äù SDM 2009. \\n[19] D. Murray et al. , ‚ÄúA data management platform for personalised real-\\ntime energy feedback,‚Äù In EEDAL 2015. \\n[20] V. Niennattrakul et al, ‚ÄúData editing techniques to allow the application \\nof distance-based outlier detection to streams,‚Äù ICDM 2010: 947-952. \\n[21] D. Patnaik, et al, ‚ÄúSustainable operation and management of data center \\nchillers using temporal data mining,‚Äù KDD 2009. \\n[22] T. Rakthanmanon et al., ‚ÄúSearching and Mining Trillions of Time Series \\nSubsequences under Dynamic Time Warping,‚Äù In KDD 2012, 262-270. \\n[23] T. Rakthanmanon and E. Keogh, ‚ÄúFast shapelets: a scalable algorithm for \\ndiscovering time series shapelets,‚Äù SDM, 2013. \\n[24] Supporting page. http://www.cs.ucr.edu/~eamonn/MatrixProfile.html \\n[25] C. D. Truong and D. T.  Anh, ‚ÄúAn Efficient Method for Motif and \\nAnomaly Detection in Time Series‚Äù IJBIDM, Vol. 10, No. 4, 2015. \\n[26] A. Tucker and X. Liu, ‚ÄúA Bayesian  Network Approach to Explaining \\nTime Series with Changing Structure,‚Äù Intell Data Anal, 8 (5) (2004). \\n[27] L. Ye and E. Keogh, ‚ÄúTime series shapelets: a new primitive for data \\nmining,‚Äù ACM SIGKDD, 2009, pp 947-56. \\n[28] C. Yoon, O. O‚ÄôReilly, K. Bergen and G. Beroza, ‚ÄúEarthquake detection \\nthrough computationally efficient similarity search,‚Äù Sci. Adv. 2015. \\n[29] F. Zhou, F. Torre and J.  Hodgins ‚Äú Aligned Cluster Analysis for \\nTemporal Segmentation of Human Motion,‚Äù IEEE FG'2008. \\n[30] S. Zilberstein and S. Russell, ‚ÄúApproximate Reasoning Using Anytime \\nAlgorithms,‚Äù In Imprecise and Approximate Computation , Kluwer \\nAcademic Publishers, 1995. \\n \\n0 5,000 10,000\\nMatrix  Profile\\nSplit Points   \\nPrediction\\nOne-dimension of multi-d time series: Subject 86, recording 4, dimension 30\\nW S P N      W C T           D            P         W\")]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b02620",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents, chunk_size= 2000, chunk_overlap=200):\n",
    "    \"\"\"splitting the documents into smaller chunk for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap= chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators=[\" \", \"\", ]\n",
    "    ) \n",
    "    split_doc = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_doc)} chunks\")\n",
    "\n",
    "    # show example chunk \n",
    "    if split_doc:\n",
    "        print(\"example chunk\")\n",
    "        print(f\"Content: {split_doc[0].page_content[:200]}\")\n",
    "        print(f\"metadata: {split_doc[0].metadata}\")\n",
    "    return split_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_documents(all_pdf_files)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b58d2d",
   "metadata": {},
   "source": [
    "### Embedding and vectorstoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a9a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embeddingManager:\n",
    "    \"\"\"Handling document gerneration using using sentence transformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name:str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence Transformer\"\"\"\n",
    "        try: \n",
    "            print(f\"Loading the embedding model:  {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"generate embeddings for a list of text\"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"model not loaded\")\n",
    "        print(f\"generating embeddings for the {len(texts)} texts.....\")\n",
    "        embeddings = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape : {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "## initializing the embedding manager \n",
    "\n",
    "embedding_manager = embeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802015fd",
   "metadata": {},
   "source": [
    "### Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbedb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a chromadb vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory : str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Intializes the chromadb client and collection\"\"\"\n",
    "        try:\n",
    "            # create persistent Chromadb\n",
    "            os.makedirs(self.persist_directory, exist_ok = True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(name=self.collection_name,\n",
    "                                                                   metadata={\"description\": \"PDF document embeddings for RAG\"})\n",
    "            \n",
    "            print(f\"Vector store initalized Collection: {self.collection_name}\")\n",
    "            print(f\"Vector store initalized Collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing the vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents : List[any], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and embeddings to the vector store\"\"\"\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number documents must match number embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # prepare the data for chromadb\n",
    "        ids= []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embedding_list = []\n",
    "\n",
    "        for i ,(doc, embedding) in enumerate(zip(documents,embeddings)):\n",
    "            # generate unique id \n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # embeddings \n",
    "            embedding_list.append(embedding.tolist())\n",
    "\n",
    "            # Add to collection\n",
    "            try:\n",
    "                self.collection.add(\n",
    "                    ids=ids,\n",
    "                    embeddings= embedding_list,\n",
    "                    metadatas=metadatas,\n",
    "                    documents=documents_text\n",
    "                )\n",
    "                print(f\"successfully added {len(documents)} documents to vector store\")\n",
    "                print(f\"total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding the documents to vector store {e}\")\n",
    "                raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ae765",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the text to embeddings \n",
    "texts = [doc.page_content for doc in chunks]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d46aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### passing texts to embeddings manager \n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "### store in the vectordb\n",
    "\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede1497",
   "metadata": {},
   "source": [
    "### RAG retriever pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query based retrieval from the vector store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: embeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retriever(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieves relevant documents for the query\"\"\"\n",
    "        print(f\"retrieving documents for query : '{query}'\")\n",
    "        print(f\"top k : {top_k}, score threshold: {score_threshold}\")\n",
    "\n",
    "        # generate the query into embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()], n_results=top_k\n",
    "            )\n",
    "\n",
    "            # process results\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results.get('documents') and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(\n",
    "                    zip(ids, documents, metadatas, distances)\n",
    "                ):\n",
    "                    # convert distance to similarity score (for chroma db cosine distance is used)\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying vector store: {e}\")\n",
    "            return []\n",
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)\n",
    "rag_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d8387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever.retriever(\"similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c1dd89",
   "metadata": {},
   "source": [
    "### Integration of vectordb context pipelinewith LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f46a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### simple RAG pipeline with groq LLM\n",
    "from multiprocessing import context\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### initialize the groq LLM along with the API_KEY\n",
    "groq_api_key = \"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\"\n",
    "llm = ChatGroq(api_key=groq_api_key, model=\"groq/compound-mini\", temperature=0.1, max_tokens= 512)\n",
    "\n",
    "## simple rag  function retrieval and generation\n",
    "\n",
    "def rag_simple(query, retriever, llm , top_k = 5):\n",
    "\n",
    "    ## retrieve the context\n",
    "    results = retriever.retriever(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant documents found.\"\n",
    "    \n",
    "   ## generate the answer using the GROQ LLM\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question.\n",
    "       Context: {context}\n",
    "       Question: {query}\n",
    "       Answer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b914d958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for query : 'What is similarity search?'\n",
      "top k : 5, score threshold: 0.0\n",
      "generating embeddings for the 1 texts.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape : (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n",
      "**Similarity search** is the computational task of finding, for a given query object, the data objects that are most alike to it according to some similarity (or distance) measure. In practice it usually means retrieving the nearest‚Äëneighbor(s) of each object in a collection‚Äîe.g., the items whose Euclidean (or z‚Äënormalized Euclidean) distance to the query is smallest. This operation underlies many applications such as clustering, duplicate detection, motif discovery, and, in the time‚Äëseries domain, all‚Äëpairs‚Äësimilarity joins.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"What is similarity search?\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d8d14",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4d247eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for query : ' what is similarity search'\n",
      "top k : 5, score threshold: 0.1\n",
      "generating embeddings for the 1 texts.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape : (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Answer: No relevant context found.\n",
      "Sources: []\n",
      "Confidence: 0.0\n",
      "Context Preview: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retriever(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\" what is similarity search\", rag_retriever, llm, top_k=5, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b1f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018dadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94ee8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02efe4cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
